


\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}

\title{RLHF + DPO math}
\author{Stanislav Fedotov, Practical Generative AI course}
\date{January 2024}

\begin{document}

\maketitle

\section{tl;dr}

RLHF and its variations (including DPO) are the tools of directly introducing human preferences into LLMs in contrast to Supervised Fine Tuning that only trains a model to produce likely texts. Note, however, that if an SFT dataset is very aligned and safe, the resulting model may also inherit these qualities to some degree even without the RLHF stage (\href{https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/}{Phi-2} model by Microsoft claims to enjoy this).

\bigskip

\textbf{Important notation}:
\begin{itemize}
\item $\pi_{\theta}(y|x)$ is the LLM with (trainable) weights $\theta$ which takes a prompt $x$ and produces a completion $y$ or, rather, probabilities of all possible completions given $x$.
\item $\pi_{\mathrm{ref}}(y|x)$ is the frozen reference model. It is usually $\pi_{\mathrm{SFT}}(y|x)$, the LLM after Supervised Fine Tuning and before RLHF.
\end{itemize}

\subsection{What is RL in the context of LLMs}

\begin{itemize}
\item In the usual setup we train an LLM on (prompt, completion) data. Thus, a model learns to produce likely completions.

In the RL (Reinforcement Learning) setup we have a separate \textbf{reward model} $r(x, y)$ showing how much a completion $y$ is appropriate a the prompt $x$. During training, we sample $x$ from data, then let the LLM generate $y = \pi_{\theta}(y|x)$, and we train the LLM for maximizing the reward $r(x, y)$ for $y\sim\pi_{\theta}(y|x)$.

\item In RL fine tuning $r(x, y)$ is usually a differentiable function. So, we don't even need the vast RL tool set developed to cope with "win/lose"-like discrete reward (which is good). 

\item The reward is $r(x, y)$ for $y\sim\pi_{\theta}(y|x)$, and we need additional hacks to differentiate through the sampling process. This can be done, for example, with log-derivative trick (see \ref{sec:log-derivative}).

\item If we simply maximize the reward, we can harm LLM's generation abilities. You can see it as ``If we make an LLM very polite, it can get very boring and unhelpful''. So, we don't want the LLM drift far away from $\pi_{\mathrm{ref}}(y|x)$ (LLM before RL). To achieve this, we apply regularization.

\item The most popular regularizer is the KL-divergence between output probabilities. So, the loss becomes
$$r(x, \pi_{\theta}(y|x)) - \beta\cdot\mathrm{KL}(\pi_{\theta}(y|x)\| \pi_{\mathrm{ref}}(y|x))$$
There are other possible regularizations (see below in the main text).

\end{itemize}

\subsection{The reward model}
\begin{itemize}
\item A reward model $r(x, y)$ measures how much a completion $y$ is appropriate for a prompt $x$. We introduce the notion of ``appropriateness'' with data. How --- see below.
\item The reward is a \textit{ranking model}. Its meaning is: if $r(x, y) > r(x, y')$, then $y$ is a more appropriate completion than $y'$.

It is \textbf{not} a classification model. It doesn't say ``this is good, that is bad'' (not in a typical situation, at least). Even for two good completions, it allows to say which is better.

\item Training strategy depends on what kind of data we are able to collect. As in any ranking task, there are three main types of data:
\begin{itemize}
\item Pairwise. That is, we fine tune on triplets $(x, y_a, y_r)$, where where $y_a$ is more appropriate than $y_r$. In other words, the human feedback is ``Completion $y_a$ is more appropriate than Completion $y_r$ for the prompt $x$''. Typical RLHF/DPO fine tuning consumes this very type of data, and the other two types are way more niche. The reward model is trained by minimizing
$$\mathcal{L}_{RM} = -\mathbb{E}_{(x, y_a, y_r)\sim\mathcal{D}}\log\sigma(r(x, y_a) - r(x, y_r)).$$

\item Pointwise. It uses data like ``thumbs up/down for this completion'' of ``Five stars for this completion''. A reward model for this could be trained for ordinary classification or regression task, or rather you can use an intrinsic reward function. Pointwise reward is a rare choice, but you can check the \href{https://arxiv.org/pdf/2402.01306}{KTO (Kahneman-Tversky optimization) paper} for an example.%, see \hyperref[sec:pointwise]{this subsection}.

\item Listwise. The data is like ``Completion $y_1$ is better than completion $y_2$ which is better than completion $y_3$ and so on''. It is possible to collect such data with ChatGPT or other powerful LLMs. This kind of data isn't very popular either; for an example, check \href{Starling-7B}{https://starling.cs.berkeley.edu/}.%see \hyperref[sec:listwise]{this subsection}

\end{itemize}

\end{itemize}

\subsection{Intrinsic reward model}

\begin{itemize}
\item As it was shown in the DPO (Direct Preference Optimization) paper, we don't need to train an external reward model, at least not in pairwise setup. Instead, we can use
$$r(x, y) = \beta\log\frac{\pi_{\theta}(y|x)}{\pi_{\mathrm{ref}}(y|x)}$$
as an intrinsic reward model. See \hyperref[sec:DPO]{DPO section} for details.

\item Note the difference between RLHF and DPO fine tuning strategies:
\begin{itemize}
\item In RLHF you train the reward model on triplets $(x, y_a, y_r)$ ($y_a$ is more appropriate than $y_r$) and on RL stage you use only $x$ pairs and sample $y = \pi_{\theta}(y|x)$.
\item In DPO you fine tune LLM on triplets $(x, y_a, y_r)$ with the following loss:
\begin{align*}
\sigma\left(\beta\log\frac{\pi_{\theta}(y_a|x)}{\pi_{\mathrm{ref}}(y_a|x)} - \beta\frac{\pi_{\theta}(y_r|x)}{\pi_{\mathrm{ref}}(y_r|x)}\right)
\end{align*}
\end{itemize}

\item Intrinsic reward models allows using RL fine tuning even outside alignment training, to extract more information from supervised fine tuning data when the potential of SFT itself is already depleted. See \hyperref[sec:self-play]{Self-Play section} for details.
\end{itemize}

\newpage
\section{RLHF}

\subsection{Bradley-Terry model}

The Bradley-Terry model was created as a ranking model. Imagine, for example, that several teams compete in a championship and we want to make a total ranking of the teams. We can do it by assigning to the $i$-th team a numerical measure $\beta_i$ of its strength. Ideally, the outcome of a competition between teams $i$ and $j$ should be determined by $\beta_i - \beta_j$.

The Bradley-Terry model treats the outcome of a game between teams $(i, j)$ as a Bernoulli random variable with probability of $i$ winning equal to

$$p^*(team_i\succ team_j) = \sigma(\beta_i - \beta_j) = \frac{1}{1 + e^{-(\beta_i - \beta_j)}} = $$
$$=\frac{e^{\beta_i - \beta_j}}{1 + e^{\beta_i - \beta_j}} = \frac{e^{\beta_i}}{e^{\beta_i} + e^{\beta_j}}$$

Simply put, it's our usual way of making $\beta_i - \beta_j$ into probabilities of winning, such that going $team_i\succ team_j\longleftrightarrow team_i\succ team_j$ is made by sign change $\beta_i - \beta_j \longleftrightarrow \beta_j - \beta_i$:

$$p^*(team_j\succ team_j) = 1 - \sigma(\beta_i - \beta_j) = \sigma(\beta_j - \beta_j)$$

%\begin{center}
%\includegraphics[width=14cm]{state-space-mind-map.png}
%\end{center}
\subsection{Reward model}

During LLM training we work with user preferences in the form ``for a prompt $x$ the completion $y_a$ is better than $y_r$'' (\textit{a} and \textit{r} stand for ``accepted'' and ``rejected''). 

We model the strength of a completion $y$ of a prompt $x$ by the reward model value $r^*(x, y)$. The formulas above become:

$$p^*(y_a\succ y_r | x) = \sigma(r^*(x, y_a) - r^*(x, y_r))$$

The reward model is trained by negative loglikelihood optimization:
$$\mathcal{L}_{RM} = -\mathbb{E}_{(x, y_a, y_r)\sim\mathcal{D}}\log\sigma(r^*(x, y_a) - r^*(x, y_r)),$$
where $(x, y_a, y_r)\sim\mathcal{D}$ stands for sampling from the dataset that we were lucky to collect. So, $\mathbb{E}_{(x, y_a, y_r)\sim\mathcal{D}}$ stands in practice for ``sum over all $(x, y_a, y_r)$ from the dataset $\mathcal{D}$''.

\bigskip

\subsection{RLHF}

RLHF allows to update an LLM given a reward model $r(x, y)$. 

The idea is quite simple:
\begin{itemize}
\item We start to denote our LLM by $\pi_{\theta}(y|x)$ and call it \textbf{policy}. But don't be afraid: it's just our good old LLM with parameters $\theta$ that predicts completion $y$ given a prompt $x$ or, more generally, probabilities of completions $y$ given $x$.
\item We take the same dataset $\mathcal{D} = \{(x, y_a, y_r)\}$ that we used for training the reward model;
\item We sample $(x, y_a, y_r)$ from $\mathcal{D}$, throw away $y_a, y_r$ and generate a new completion $y$ from $\pi_{\theta}(y|x)$. We denote this sampling process $x\sim\mathcal{D}, y\sim\pi_{\theta}(y|x)$.
\item More accurately, you usually generate $y$ token by token and each new token gives a new summand to the optimization objective.
\item We optimize reward $r(x, y)$ of generated $y$ in pursuit of making the LLM give us more rewarding completions:
$$\mathbb{E}_{x\sim\mathcal{D}, y\sim\pi_{\theta}(y|x)}r(x, y)\longrightarrow\max\limits_{\theta}.$$
We would like to say that $y = \pi_{\theta}(y|x)$ and our maximization task is just
$$\sum_{x\in\mathcal{D}}r(x, LLM_{\theta}(x))\longrightarrow\max\limits_{\theta}.$$
But this is not so simple. Below, we'll see why.
\end{itemize}

\bigskip

Now, let's discuss the loss function in more details, because we'll need to regularize it.

\begin{itemize}
\item If you get too carried away with maximizing reward (human preferences), you can ruin the quality. An absurd, but illustrative example: a model that politely refuses to answer any question is perfectly non-toxic, although completely useless. So, we don't want the LLM to drift too far away during RLHF. 

There could be different ways of enforcing it. For example, we could add $||\theta_{new} - \theta_{old}||^2$ regularization on weights. But practice shows that it's better to control outputs, i.e. to keep $\pi_{\theta}(y|x)$ close to the initial distribution $\pi_{\mathrm{SFT}}(y|x)$ (the latter is called ``reference policy'').

The most common tool for measuring distance between distributions is KL-divergence. So, the regularized version of the training objective is:
$$\mathcal{L}_{\mathrm{RLHF}} = \mathbb{E}_{x\sim\mathcal{D}, y\sim\pi_{\theta}(y|x)}\left[r(x, y)\right] - \beta\mathbb{D}_{\mathrm{KL}}\left[\pi_{\theta}(y|x)||\pi_{\mathrm{SFT}}(y|x)\right]\eqno{(1)}$$

\textbf{Note 1}: This objective is being \textit{maximized}.

\textbf{Note 2}: KL-divergence is outside $\mathbb{E}_{x\sim\mathcal{D}, y\sim\pi_{\theta}(y|x)}$. If we want to drag it inside, we will use the definition of KL-divergence $\mathbb{D}_{\mathrm{KL}}(p||q) = \sum_i p_i\log\frac{p_i}{q_i} = \mathbb{E}_p\log\frac{p}{q}$ and write instead
$$\mathcal{L}_{\mathrm{RLHF}} = \mathbb{E}_{x\sim\mathcal{D}, y\sim\pi_{\theta}(y|x)}\left[r(x, y) - \beta\log\frac{\pi_{\theta}(y|x)}{\pi_{\mathrm{SFT}}(y|x)}\right]\eqno{(2)}$$

\textbf{Note 3} If you're into RL, you can recognize TRPO (Trust Region Policy Optimization) in the idea of KL-regularization.

\item The original \href{https://arxiv.org/pdf/2203.02155.pdf}{InstructGPT paper} also suggesting adding one more summand to the objective to directly control performance on the pretraining dataset:
\begin{align*}
\mathcal{L}_{\mathrm{RLHF}} = &\mathbb{E}_{x\sim\mathcal{D}, y\sim\pi_{\theta}(y|x)}\left[r(x, y) - \beta\log\frac{\pi_{\theta}(y|x)}{\pi_{\mathrm{SFT}}(y|x)}\right] + \\
&+ \gamma\mathbb{E}_{x\sim\mathbb{D}_{pretrain}}\log\pi_{\theta}(x) \tag{3}
\end{align*}

\item KL-divergence in not the only possible mechanism to control the drift of $\pi_{\theta}(y|x)$ from $\pi_{\mathrm{SFT}}(y|x)$. Another one is \textbf{Clipped Surrogate Objective}. The corresponding loss looks like this:
\begin{align*}
\mathcal{L}_{\mathrm{RLHF-clip}} = &\mathbb{E}_{x\sim\mathcal{D}, y\sim\pi_{\theta}(y|x)}\min\left[\frac{\pi_{\theta}(y|x)}{\pi_{\mathrm{SFT}}(y|x)}r(x, y); \mathrm{clip}\left(\frac{\pi_{\theta}(y|x)}{\pi_{\mathrm{SFT}}(y|x)}, 1 - \epsilon, 1 + \epsilon\right)r(x, y) \right],
\end{align*}

The ratio $\frac{\pi_{\theta}(y|x)}{\pi_{\mathrm{SFT}}(y|x)}$ comes from the original PPO where it plays the role of importance sampling. Clipping allows to avoid extreme values of $\frac{\pi_{\theta}(y|x)}{\pi_{\mathrm{SFT}}(y|x)}$ and, as a consequence, extreme changes of the policy.

\end{itemize}

\subsection{Ok, so RLHF is simply a fine tuning process? [Beware: math!]}
\label{sec:log-derivative}

Well, there is one peculiar thing to it. Let's look again at the most basic formulation of RLHF as an optimization problem:

$$\mathbb{E}_{x\sim\mathcal{D}, y\sim\pi_{\theta}(y|x)}r(x, y)\longrightarrow\max\limits_{\theta}.$$

Looks harmless. However, the function that we are optimizing works as follows:
\begin{align*}
x&\longrightarrow x, \pi_{\theta}(y|x)\qquad\mbox{(calculating logits of LLM)}\\
&\longrightarrow x, y\qquad\mbox{\textbf{(sampling $y$ from $\pi_{\theta}(y|x)$})}\\
&\longrightarrow r(x, y)\qquad\mbox{(calculating the reward)}
\end{align*}
Probably, you've already understood why sampling is in bold. In the best case (greedy sampling) it is done with $\mathrm{argmax}$ which is non-differentiable. In the worst case, it's just random sampling from a softmax of logits, and we can't differentiate through a sampling procedure. Just imagine how it works for a batch of data. You take $x_1,\ldots, x_B$, then you sample $y_1,\ldots, y_n$ and then your loss becomes
$$\frac1B\sum_{i=1}^Br(x_i, y_i)$$
which doesn't depend on $\theta$ (the model weights). Seems like we're in a bit of a pickle.

\bigskip

To deal with it, let's rewrite the expectation from the point of view of mathematics using that $\mathbb{E}_{a\sim q(a)}f(a) = \sum_aq(a)f(a)$ (this sum becomes $\int_aq(a)f(a)da$ when $a$ is continuous). So, we have
$$\mathbb{E}_{x\sim\mathcal{D}, y\sim\pi_{\theta}(y|x)}r(x, y)=\mathbb{E}_{x\sim\mathcal{D}}\mathbb{E}_{y\sim\pi_{\theta}(y|x)}r(x, y) = $$
$$=\mathbb{E}_{x\sim\mathcal{D}}\sum_y\pi_{\theta}(y|x)r(x, y).$$
Note that $y$ are the possible completions, so it's valid for us to write a sum (and not an integral). Anyway, we need to find the gradient of this sum. You would ask: why is it difficult? Well, let's compare two situations:

\medskip

\textbf{Situation 1}. During pre-training (or SFT), the loss function is
$$\mathbb{E}_{(x, y)\sim\mathcal{D}}\ell(y, \pi_{\theta}(y|x)) = \sum_{(x, y)}q(x, y)\ell(y, \pi_{\theta}(y|x)),$$
where $q(x, y)$ is the probability of the particular prompt+completion $(x, y)$ in our data and $\ell$ is actually negative log-likelihood. If we take a gradient with respect to $\theta$, we have:
$$\nabla_{\theta}\sum_{(x, y)}q(x, y)\ell(y, \pi_{\theta}(y|x)) = \sum_{(x, y)}q(x, y)\nabla_{\theta}\ell(y, \pi_{\theta}(y|x))=$$
$$=\mathbb{E}_{(x, y)\sim\mathcal{D}}\nabla_{\theta}\ell(y, \pi_{\theta}(y|x))$$
because $q(x, y)$ doesn't depend on $\theta$. So, this is again a mathematical expectation and we can estimate it with Monte-Carlo:
$$\mathbb{E}_{(x, y)\sim\mathcal{D}}\nabla_{\theta}\ell(y, \pi_{\theta}(y|x))\approx\frac1B\sum_{
\begin{smallmatrix}
(x_i, y_i)\sim\mathcal{D},\\
i=1,\ldots,B
\end{smallmatrix}
}\nabla_{\theta}\ell(y_i, \pi_{\theta}(y_i|x_i))$$
which is the familiar stochastic gradient descent.

\medskip

Why could we do it? \textbf{Because $q(x,y)$ didn't depend on $\theta$!}

\medskip

\textbf{Situation 2}. Now, let's differentiate the RLHF loss. We can carry $\nabla_{\theta}$ inside $\mathbb{E}_{x\sim\mathcal{D}}$, as in Situation 1, but then we encounter problems:
$$\nabla_{\theta}\mathbb{E}_{x\sim\mathcal{D}}\sum_y\pi_{\theta}(y|x)r(x, y) =$$
$$=\mathbb{E}_{x\sim\mathcal{D}}\sum_y\left[\color{red}{\nabla_{\theta}\pi_{\theta}(y|x)\cdot r(x, y)} + \color{blue}{\pi_{\theta}(y|x)\cdot \nabla_{\theta}r(x, y)}\right]$$
Note that the internal sum is over all $y$, each with its own probability $\pi_{\theta}(y|x)$.

Now, the blue summand is zero, because $r(x, y)$ formally doesn't depend on $\theta$. We're left with the red one:
$$=\mathbb{E}_{x\sim\mathcal{D}}\sum_y\color{red}{\nabla_{\theta}\pi_{\theta}(y|x)\cdot r(x, y)}$$
To estimate this sum with Monte Carlo (=on a batch of data), we need it to be a mathematical expectation. But an expectation is
$$\sum_y\mathrm{density(y|x)}\cdot\mathrm{function(x)},$$
and we don't spot a density here. No, $\nabla_{\theta}\pi_{\theta}(y|x)$ is not a density.

\medskip

There are several ways of coping with it. When it comes to VAE (variational autoencoder), we'll use \textbf{reparametrization trick}. And now, I'll show you how to do it with \textbf{log-derivative trick}. Let's leverage the fact that
$$\frac{d}{dt}\log{f(t)} = \frac1{f(t)}\frac{d}{dt}f(t).$$
In our case,
$$\nabla_{\theta}\log\pi_{\theta}(y|x) = \frac1{\pi_{\theta}(y|x)}\nabla_{\theta}\pi_{\theta}(y|x),$$
or, equivalently,
$$\nabla_{\theta}\pi_{\theta}(y|x) = \pi_{\theta}(y|x)\nabla_{\theta}\log\pi_{\theta}(y|x)$$
Let's rewrite the gradient:
$$\nabla_{\theta}\mathbb{E}_{x\sim\mathcal{D}}\sum_y\pi_{\theta}(y|x)r(x, y) = \ldots = \mathbb{E}_{x\sim\mathcal{D}}\sum_y\nabla_{\theta}\pi_{\theta}(y|x)\cdot r(x, y)=$$
$$=\mathbb{E}_{x\sim\mathcal{D}}\sum_y{\color{magenta}{\pi_{\theta}(y|x)}}\nabla_{\theta}\log\pi_{\theta}(y|x)\cdot r(x, y).$$
What I've painted in magenta is a density! Now, we can write
$$\ldots=\mathbb{E}_{x\sim\mathcal{D}}\,\mathbb{E}_{y\sim\color{magenta}{\pi_{\theta}(y|x)}}\nabla_{\theta}\log\pi_{\theta}(y|x)\cdot r(x, y).$$
This thing we can approximate with Monte-Carlo sampling:
$$\ldots\approx\frac1B\sum_{
\begin{smallmatrix}
x_i\sim\mathcal{D},\\
i=1,\ldots,B
\end{smallmatrix}
}\frac1C\sum_{
\begin{smallmatrix}
y_j\sim\pi_{\theta}(y_j|x_i),\\
j=1,\ldots,C
\end{smallmatrix}
}\nabla_{\theta}\log\pi_{\theta}(y_j|x_i)\cdot r(x_i, y_j)$$
What does it mean in practice?
\begin{itemize}
\item We take a batch of $B$ prompts $x_i$,
\item For each of them we sample $C$ (usually, $C=1$) completions $y_j$ from our LLM $\pi_{\theta}(y|x_i)$,
\item For each pair $(x_i, y_j)$ we calculate $\nabla_{\theta}\log\pi_{\theta}(y_j|x_i)\cdot r(x_i, y_j)$
\item We sum all of them and divide by $BC$,
\item We used what we've obtained as the gradient update!
\end{itemize}

\subsection{PPO and why we don't cover it}

Reinforcement Learning is a very tricky thing. Simple algorithms don't converge well, and more advanced ones are quite involved. Giving a fitting RL intro here is hardly possible (the long read is already too long :( ). So, we will just hint for you the next steps. If you want to fully understand RLHF, you will need the following components:
\begin{itemize}
    \item Advantage Actor-Critic (A2C),
    \item Proximal policy optimization (PPO).
\end{itemize}

\section{Direct preference optimization (DPO)}
\label{sec:DPO}

Reinforcement learning in general is notorious for instability, so researchers sought for ways of removing RL from RLHF.

One of the most successful options so far is \textbf{Direct preference optimization} suggested in the NeurIPS 2023 best paper award winning \href{https://arxiv.org/pdf/2305.18290.pdf}{Your Language Model is Secretly a Reward Model} paper. Its main idea is that we don't need to train an external reward model and we can find it inside our LLM. Let's see how it's done.

\subsection{Analytical solution for RLHF objective maximization}

The authors showed that maximum of $\mathcal{L}_{\mathrm{RLHF}}$ can be found analytically. Indeed,
\begin{align*}
\mathcal{L}_{\mathrm{RLHF}} &= \mathbb{E}_{x\sim\mathcal{D}, y\sim\pi_{\theta}(y|x)}\left[r(x, y) - \beta\log\frac{\pi_{\theta}(y|x)}{\pi_{\mathrm{SFT}}(y|x)}\right]\\
&=-\mathbb{E}_{x\sim\mathcal{D}, y\sim\pi_{\theta}(y|x)}\left[\log\frac{\pi_{\theta}(y|x)}{\pi_{\mathrm{SFT}}(y|x)\exp\left(\frac1{\beta}r(x, y)\right)}\right]\\
\end{align*}

The expression $-\mathbb{E}_{x\sim\mathcal{D}, y\sim\pi_{\theta}(y|x)}\log\frac{\pi_{\theta}(y|x)}{something}$ resembles $-\mathbb{D}_{\mathrm{KL}}(\pi_{\theta}(y|x)||something)$. If it were so, its maximum, i.e. the minimum of KL-divergence would be when $something$ coincides with $\pi_{\theta}(y|x)$.

The only problem is that $\pi_{\mathrm{SFT}}(y|x)\exp\left(\frac1{\beta}r(x, y)\right)$ is not guaranteed to be a probabilistic distribution, that is it probably doesn't sum to $1$. Let's denote

$$Z(x) = \sum_y\pi_{\mathrm{SFT}}(y|x)\exp\left(\frac1{\beta}r(x, y)\right)$$

$$\pi^*(y|x) = \frac1{Z(x)}\pi_{\mathrm{SFT}}(y|x)\exp\left(\frac1{\beta}r(x, y)\right)$$

and rewrite

\begin{align*}
\mathcal{L}_{\mathrm{RLHF}} &=\mathbb{E}_{x\sim\mathcal{D}, y\sim\pi_{\theta}
(y|x)}\left[\log{Z}(x) - \log\frac{\pi_{\theta}(y|x)}{\frac1{Z(x)}\pi_{\mathrm{SFT}}(y|x)\exp\left(\frac1{\beta}r(x, y)\right)}\right]
\end{align*}

Now, $Z(x)$ doesn't depend on $\theta$, so it has no effect on optimization, so

\begin{align*}
\max\limits_{\theta}\mathcal{L}_{\mathrm{RLHF}} &=\min\limits_{\theta}\mathbb{E}_{x\sim\mathcal{D}, y\sim\pi_{\theta}
(y|x)}\frac1{Z(x)}\pi_{\mathrm{SFT}}(y|x)\exp\left(\frac1{\beta}r(x, y)\right)
\end{align*}

and it is 

$$\pi_{\theta}(y|x) = \pi^*(y|x) = \frac1{Z(x)}\pi_{\mathrm{SFT}}(y|x)\exp\left(\frac1{\beta}r(x, y)\right).$$

\subsection{Intrinsic reward model and the DPO loss}

Above we've expressed the optimal policy in terms of reward model. Now let's do the other way around:
$$r(x, y) = \beta\log\frac{\pi^*(y|x)}{\pi_{\mathrm{SFT}}(y|x)} + \beta\log{Z(x)}$$

From this we can write the DPO loss function. It will differ depending on a ranking model we use. We know only Bradley-Terry model, so we'll cling to it:

$$p(y_a\succ y_r|x) = \sigma(r(x, y_a) - r(x, y_r))$$

Now, let's recall that we want to train the LLM to favor $y_a | x$ over $y_r | x$ for all $(x, y_a, y_r)\in\mathcal{D}$. That is, to maximise all $p(y_a\succ y_r|x)$. So, the loss that we need is:
\begin{align*}
\mathcal{L}_{\mathrm{DPO}} &= \mathbb{E}_{(x, y_a, y_r)\sim\mathcal{D}}p_{\theta}(y_a\succ y_r|x)
\end{align*}
where
\begin{align*}
p_{\theta}(y_a\succ y_r|x)&= \sigma\left(\left[\beta\log\frac{\pi_{\theta}(y_a|x)}{\pi_{\mathrm{SFT}}(y_a|x)} + \beta\log{Z(x)}\right] -
\left[\beta\log\frac{\pi^*(y_r|x)}{\pi_{\mathrm{SFT}}(y_r|x)} + \beta\log{Z(x)}\right]\right)\\
&=\sigma\left(\beta\log\frac{\pi_{\theta}(y_a|x)}{\pi_{\mathrm{SFT}}(y_a|x)} - \beta\log\frac{\pi_{\theta}(y_r|x)}{\pi_{\mathrm{SFT}}(y_r|x)}\right)
\end{align*}
A nice thing: $Z(x)$ gets canceled!

\bigskip

Note that we don't need RL for this. Optimizing $\mathcal{L}_{\mathrm{DPO}}$ is just fine tuning.

\bigskip

\textbf{Note}. Other, more complicated DPO objectives have also been suggested. You can, for example, add KL-divergence as a regularizer. If you want to see more, you can check the links  \href{https://huggingface.co/docs/trl/main/en/dpo_trainer#Loss}{here}.

\subsection{Gradient update analysis}

The DPO paper suggests nice analysis of the DPO objective gradient:

\begin{align*}
&\nabla\mathcal{L}_{\mathrm{DPO}} = \\
&= -\beta\mathbb{E}\left[
\underbrace{\sigma(\widehat{r}_{\theta}(x, y_a) - \widehat{r}_{\theta}(x, y_r))}_{
\begin{smallmatrix}
\mbox{higher weight}\\
\mbox{when reward estimate}\\
\mbox{is wrong}\\
\end{smallmatrix}
}
\left[
\underbrace{\nabla_{\theta}\log\pi_{\theta}(y_a|x)}_{
\uparrow\mbox{ likelihood of }y_a
}
-\underbrace{\nabla_{\theta}\log\pi_{\theta}(y_r|x)}_{
\downarrow\mbox{ likelihood of }y_a
}
\right]
\right],
\end{align*}
where $\widehat{r}_{\theta}(x, y) = \beta\frac{\pi_{\theta(y|x)}}{\pi_{\theta}(y|x)}$ is also a surrogate reward function of sorts.

\newpage
\section{Self-play}
\label{sec:self-play}

RL-based strategies can also be used to improve the results of Supervised Fine Tuning (SFT) without collecting new data.

The authors of the \href{https://arxiv.org/pdf/2401.01335.pdf}{SPIN (Self-Play Fine-Tuning)} paper took zephyr-7b-sft-full, a fine-tuned LLM based on Mistral-7B, and proved that:

\begin{itemize}
\item If we try to fine tune the LLM further on its own SFT dataset, we don’t improve it much and can even diminish the evaluation scores.
\item Still, LLM’s completions underperform in comparison to the ground truth completions from the dataset, so it seems that we could probably squeeze some more information from this data.
\end{itemize}

The algorithm SPIN that they suggest resembles a GAN and involves alternating training of two ``players''. On each step $t$:
\begin{enumerate}
\item The first player is the function $f_t$ (which is represented by a neural network, of course) which tries to determine whether a completion $y$ of a prompt $x$ is natural or generated. So, $f_t$ could maximise the following objective:
$$f_{t} = \mathrm{argmax}_{f\in\mathcal{F}_t}\mathbb{E}_{x\sim\mathcal{D}, y\sim\pi_{\mathrm{true}}(y|x), y'\sim\pi_{\theta_{t-1}}(y'|x)}(f(x, y) - f(x, y'))$$
In human language it means that $x$ is a prompt taken from STF data, $y$ is its actual completion and $y'$ is its completion generated by the model from the $(t-1)$-th step of SPIN. The function $f(x, y)$ plays the role of ``confidence'' that $(x, y)$ is a natural prompt+completion pair, and the loss says: ``a truly natural completion should receive greater confidence than a generated completion''.

However, this maximization task is not good: values of $f$ are not bounded and optimization can eventually lead to $f(x, y')\rightarrow-\infty$. So, it can be beneficial to consider instead the objective
$$f_{t} = \mathrm{argmin}_{f\in\mathcal{F}_t}\mathbb{E}_{x\sim\mathcal{D}, y\sim\pi_{\mathrm{true}}(y|x), y'\sim\pi_{\theta_{t-1}}(y'|x)}\ell(f(x, y) - f(x, y')),$$
where $\ell$ is monotonically \textbf{decreasing} (note the $\mathrm{arg}\mathbf{min}$!) and smooth. The authors suggest taking the logistic loss function $\ell(t) = \log(1 + \exp(-t))$.

This objective is minimized inside a function class $\mathcal{F}_t$ which we are free to choose. We'll use this freedom soon to find a neat formula for $f_{t}$.

\item The second player is the LLM $\pi_{\theta_{t}}(y|x)$ itself which tries to ``fool'' $f_t$ by maximizing the familiar RLHF objective:
$$\mathcal{L}_t = \mathbb{E}_{x\sim\mathcal{D}, y\sim\pi_{\theta}(y|x)}\left[f_t(x, y)\right] - \beta\mathbb{D}_{\mathrm{KL}}\left[\pi_{\theta}(y|x)||\pi_{\mathrm{SFT}}(y|x)\right]$$
\end{enumerate}

Now, let's understand in what class $\mathcal{F}_t$ should we search for $f_t$. You probably remember that $\mathcal{L}_t$ is minimized by
$$\pi^*(y|x) = \frac1{Z(x)}\pi_{\mathrm{SFT}}(y|x)\exp\left(\frac1{\beta}f_t(x, y)\right).$$
For an arbitrary $f_t$ we can't guarantee that there is a set of weights $\theta$ such that $\pi_{\theta}(y|x) = \pi^*(y|x)$. So, let's choose $\mathcal{F}_t$ in a way that such $\theta$ exists!

Solving
$$\pi_{\theta}(y|x) = \frac1{Z(x)}\pi_{\mathrm{SFT}}(y|x)\exp\left(\frac1{\beta}f_t(x, y)\right).$$
in $f_t$, we get
$$f_t = \beta\log\frac{\pi_{\theta}(y|x)Z(x)}{\pi_{\mathrm{SFT}}(y|x)} = \beta\log\frac{\pi_{\theta}(y|x)}{\pi_{\mathrm{SFT}}(y|x)} + \beta Z(x)$$
The second term doesn't depend on $y$ and is of no interest for us. So, we set
$$\mathcal{F}_t = \left\{\beta\log\frac{\pi_{\theta}(y|x)}{\pi_{\mathrm{SFT}}(y|x)},\mbox{ for different $\theta$}\right\}.$$
And we can write:
$$f_{t} = \mathrm{argmin}_{\theta}\mathbb{E}_{x\sim\mathcal{D}, y\sim\pi_{\mathrm{true}}(y|x), y'\sim\pi_{\theta_{t-1}}(y'|x)}\left[\ell\left(\beta\log\frac{\pi_{\theta}(y|x)}{\pi_{\mathrm{SFT}}(y|x)} - \beta\log\frac{\pi_{\theta}(y'|x)}{\pi_{\mathrm{SFT}}(y'|x)}\right)\right],$$

That's the final version of Player 1 move: finding optimal $\theta$ for this minimization task.

Now, let's return to the Player 2. As we saw earlier, its optimal strategy is becoming:
$$\pi^*(y|x) = \frac1{Z(x)}\pi_{\mathrm{SFT}}(y|x)\exp\left(\frac1{\beta}f_t(x, y)\right).$$
If we substitute
$$f_t = \beta\log\frac{\pi_{\theta_{\mathrm{optimal}}}(y|x)}{\pi_{\mathrm{SFT}}(y|x)} + \beta Z(x)$$
here, we get
$$\pi^*(y|x) = \pi_{\theta_{\mathrm{optimal}}}(y|x).$$
Wow! So, Player 1 just uses the weights $\theta_{\mathrm{optimal}}$ that we used during the Player 1 move!

\bigskip

\textbf{Final algorithm}. We update weights $\theta$ of the LLM $\pi_{\theta}(y|x)$ iteratively. On step $t$ we have $\theta_{t-1}$, and we obtain $\theta_t$ by minimizing the following objective with respect to $\theta$:
$$\mathcal{L}_{\mathrm{final}} = \mathbb{E}_{x\sim\mathcal{D}, y\sim\pi_{\mathrm{true}}(y|x), y'\sim\pi_{\theta_{t-1}}(y'|x)}\left[\ell\left(\beta\log\frac{\pi_{\theta}(y|x)}{\pi_{\mathrm{SFT}}(y|x)} - \beta\log\frac{\pi_{\theta}(y'|x)}{\pi_{\mathrm{SFT}}(y'|x)}\right)\right].$$

\newpage
Experiments show that SPIN can significantly improve the LLM’s performance across a variety of benchmarks and even outperform models trained through DPO on a dataset with accepted/rejected labels given by GPT-4.

\begin{center}
\includegraphics[width=10cm]{self-play.png}
\end{center}

Note however, that SPIN is not a replacement for RLHF/DPO, because it's goal is completely different: while SPIN makes an LLM produce more ``likely'' completions, it doesn't introduce any kind of human preferences, so another RLHF/DPO step can still be needed after it.

\section{RL instead of SFT, DeepSeek R1, and GRPO}

In early 2025, we saw several attempts at training open source LLMs for long, non-linear reasoning. The utmost challenge with that is the lack of training data. You can't just go and collect a dataset of high-quality o1-style reasoning. So, a number of workarounds emerged, and one of the most radical ones was suggested by the authors of \href{https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek\_R1.pdf}{DeepSeek R1}. During their first attempt, the model DeepSeek R1-Zero was trained without SFT, only using RL! Let's briefly look at how this was done.

First of all, since RL replaces SFT, the reward function is not about human preference; instead, two reward groups are used: 
\begin{itemize}
\item \textbf{Accuracy rewards}: it checks whether the answer is correct. Just this! The solution isn't checked. Moreover, the authors choose tasks, where the answer can be checked automatically. For example, in math problems it should be inside \texttt{\\boxed\{\}}; for coding, the resulting code is checked with test cases.
\item \textbf{Format rewards}: it enforces the model to put its thinking process between \texttt{"\textless think\textgreater"} and \texttt{"\textless/think\textgreater"}
tags.
\end{itemize}

As an RL algorithm, the authors use \textbf{Group
Relative Policy Optimization (GRPO)}. This algorithm is quite intricate, but we'll try our best to explain it. At training, the following things happen:

\begin{itemize}
\item For a prompt $q$, several answers $o_1,\ldots,o_G$ are generated.
\item For each $o_i$, a reward $r_i$ is calculated.
\item Then, for each of them the relative \textbf{advantage} is computed
$$A_i = \frac{r_i - \textrm{mean}(r_1,\ldots,r_G)}{\textrm{std}(r_1,\ldots,r_G)}$$
\item Now, the objective is what we've already seem as \textbf{Clipped surrogate objective} with KL-regulatization.
$$\mathcal{L}(q) = \frac1G\sum_{i=1}^G\left[\min\left(\frac{\pi_{\theta}(o_i\mid q)}{\pi_{\mathrm{rdf}}(o_i\mid q)}A_i,
\mathrm{clip}\left(\frac{\pi_{\theta}(o_i\mid q)}{\pi_{\mathrm{rdf}}(o_i\mid q)}; 1 - \varepsilon, 1 + \varepsilon\right)A_i\right) - \beta\mathbb{D}_{\mathrm{KL}}(\pi_{\theta}\|\pi_{\mathrm{ref}})\right]$$
\item The final loss is
$$\mathbb{E}_{q\sim\mathcal{D},\,o_i\sim\pi_{\theta}(o\mid q)}\mathcal{L}$$
That is, $q$ is sampled from the data and $o_i$ from the policy (LLM) $\pi_{\theta}$ that we are training.

As an instructor, I'm a bit abashed that solutions are not checked, but somehow the model really not only learns to produce valid solutions, but also starts to exhibit human-like reasoning patterns, like the ``aha moment'':

\begin{center}
\includegraphics[width=12cm]{deepseek-aha.png}
\end{center}

There is a drawback though. Because the model isn't explicitly trained to reason like a human, its thoughts are poorly readable and exhibit language mixing. So, in the training of the final \textbf{DeepSeek-R1} the authors introduce some preliminary SFT on high-quality data (but partially generated by DeepSeek-R1Zero).

\end{itemize}

\end{document}

\section{RLHF/DPO for data which is not based on binary comparisons}

This section is completely optional, but if you're curious, read on!

\subsection{When data is thumbs-up/thumbs-down}
\label{sec:pointwise}

The authors of \href{https://arxiv.org/pdf/2402.01306.pdf}{KTO} argue that binary comparison data like ``Completion A is more appropriate than Completion B'' is hard to collect, while ``This completion is appropriate/non-appropriate'' is much more abundant.

To work with absolute (not relative) preferences, the authors suggest using an approach from prospect theory (see the paper by \href{http://cemi.ehess.fr/docannexe/file/2780/tversjy_kahneman_advances.pdf}{Kahneman and Tversky} on which HALO relies heavily). First of all, they adopt several principles governing human perception of reward/loss:
\begin{itemize}
\item The utility of an outcome is always relative to some reference point (some default or guaranteed amount of gain or loss).
\item Human utility is not linear in relative gain or loss: the rate of change of utility diminishes the further you move from the reference point.
\end{itemize}
That's what it looks like (see the Kahneman-Tversky curve):
\begin{center}
\includegraphics[width=8cm]{tversky-kaneman.png}
\end{center}

Inspired by the work by Tversky and Kahneman, the authors suggest to start from the \textit{human value function} like this:
$$h(z, z_{\mathrm{ref}}) = \sigma(z - z_{\mathrm{ref}}),$$
where $z$ is the reward/loss value, or, more generally,
$$
h(z, z_{\mathrm{ref}}) = 
\begin{cases}
\lambda_{+}\sigma(z - z_{\mathrm{ref}}),\mbox{ if $z \geq 0$ (it is a reward)},\\
\lambda_{-}\sigma(z - z_{\mathrm{ref}}),\mbox{ if $z < 0$ (it is a loss)}.
\end{cases}
$$
Now, what is $z$ for LLM fine tuning task? 

The authors start with the reward model introduced in the DPO paper:
$$r(x, y) = \beta\log\frac{\pi_{\theta}(y|x)}{\pi_{\mathrm{SFT}}(y|x)} + \beta\log{Z(x)}$$
As a reference reward, they suggest taking
$$\mathbb{E}_{x'\sim\mathcal{D},\ y'\sim\pi_{\theta}(y|x)}r(x, y'),$$
where the expectation is taken not only over completions of the current $x$, but over all the possible pairs (prompt, completion). If $Z(x)$ is the same for all $x$, we get
$$r(x, y) - \mathbb{E}_{x'\sim\mathcal{D},\ y'\sim\pi_{\theta}(y|x)}r(x, y') = $$
$$= \beta\log\frac{\pi_{\theta}(y|x)}{\pi_{\mathrm{SFT}}(y|x)} - \beta\mathbb{E}_{x'\sim\mathcal{D},\ y'\sim\pi_{\theta}(y|x)}\log\frac{\pi_{\theta}(y'|x')}{\pi_{\mathrm{SFT}}(y'|x')} = $$
$$= \beta\log\frac{\pi_{\theta}(y|x)}{\pi_{\mathrm{SFT}}(y|x)} - \beta\mathbb{E}_{x'\sim\mathcal{D}}\mathrm{KL}(\pi_{\theta}(y|x) \| \pi_{SFT}(y|x)).$$
Now, we set 
$$r_{KTO}(x,y) = \beta\log\frac{\pi_{\theta}(y|x)}{\pi_{\mathrm{SFT}}(y|x)},$$
$$r_{\mathrm{ref}} = \beta\mathbb{E}_{x'\sim\mathcal{D}}\mathrm{KL}(\pi_{\theta}(y|x) \| \pi_{SFT}(y|x))$$
But it's not $z$ yet, this doesn't take into account whether $y$ is a desirable completion for $x$. The authors suggest a simple fix for that:
$$
z(x, y) = \begin{cases}
\lambda_D\sigma(r_{KTO}(x,y) - r_{\mathrm{ref}}),\mbox{ if $(x,y)$ is desirable},\\
\lambda_U\sigma(r_{\mathrm{ref}} - r_{KTO}(x,y)),\mbox{ if $(x,y)$ is undesirable}.
\end{cases}
$$
The final loss that will be minimized during fine tuning is
$$\mathcal{L}_{KTO} = \mathbb{E}_{x\sim\mathcal{D}, y\sim\pi_{\theta}(y|x)}w(y) - z(x, y)$$
where
$$
w(y) = \begin{cases}
\lambda_D,\mbox{ if $(x,y)$ is desirable},\\
\lambda_U,\mbox{ if $(x,y)$ is undesirable}.
\end{cases}
$$

\subsection{When we have listwise comparison data}
\label{sec:listwise}

Imagine that our data is like $(y_1\succ y_2\succ \ldots \succ y_K\mid x)$ that is ``Completion $y_1$ is better than completion $y_2$ which is better than completion $y_3$ and so on'' with some $K > 2$. Collecting such data from real people sounds like an impossible feat, but it can be collected using LLMs. For example, the authors of \href{https://starling.cs.berkeley.edu/}{Starling-7B} generate this kind of data for the RLAIF training stage.

Terry-Bradley model doesn't work for $K>2$. One of the options is to use its generalization, the \textbf{Plackett-Luce ranking model}. Let's try to understand it.

\bigskip

%Imagine that we have $N$ judges and each ranked $K$ objects, with $n$-th judge saying that
%$$y_1^{(n)}\succ y_2^{(n)}\succ \ldots \succ y_K^{(n)}$$
%Please note that the objects $y^{(n)}_i$ may differ between judges. 

As the Bradley-Terry model does, Luke-Placett assigns a positive ``measure of worth'' $s(y)$ to each object. Now, imagine that we have several objects $y_1,\ldots,y_K$. We want to know
$$p(y_{w_1}\succ y_{w_2}\succ \ldots \succ y_{w_K})$$
for a permutation $(w_1, \ldots, w_K)$ of the set $1,\ldots,K$. We model it using the following process:
\begin{itemize}
\item First, we choose the highest ranking with probabilities
$$p\left(\mbox{$y_{w_1}$ is ranked as the first}\right) = \frac{s\left(y_{w_1}\right)}{s\left(y_{w_1}\right) + s\left(y_{w_2}\right) + \ldots + s\left(y_{w_K}\right)}.$$
\item Now, as the first is fixed, we choose the second one with probabilities
$$p\left(\mbox{$y_{w_1}$ is ranked as the second}\right) = \frac{s\left(y_{w_2}\right)}{s\left(y_{w_2}\right) + \ldots + s\left(y_{w_K}\right)}.$$
\item And so on.
\item Finally, when only two objects are left, we rank them using the familiar Bradley-Terry model:
$$p\left(\mbox{$y_{w_{K-1}}$ is ranked as the $(K-1)$-st}\right) = \frac{s\left(y_{w_{K-1}}\right)}{s\left(y_{w_{K-1}}\right) + s\left(y_{w_K}\right)}.$$
\end{itemize}

All the choices were independent, so we can multiply the probabilities:
$$p(y_{w_1}\succ y_{w_2}\succ \ldots \succ y_{w_K}) = \prod_{k=1}^K\frac{s\left(y_{w_k}\right)}{s\left(y_{w_k}\right)  + \ldots + s\left(y_{w_K}\right)}$$

Here is a scheme illustrating our model:

\begin{center}
\includegraphics[width=12cm]{placett-luce-graph.png}
\end{center}

An important note: it's a model, a way of describing how it could work and not the absolute truth. We could choose other models if we had wished so.

\bigskip

Ok, so now we understand the Plackett-Luce model. How to use it with LLMs? Let's recall that in this setup our data is like $(y_1\succ y_2\succ \ldots \succ y_K\mid x)$ that is ``Completion $y_1$ is better than completion $y_2$ which is better than completion $y_3$ and so on''. We want to train a reward model $r(x, y)$, and for that, we set
$$s(y) = \exp(r(x, y))$$
Then, much like in Terry-Bradley model, we get:
$$p(y_{1}\succ y_{2}\succ \ldots \succ y_{K}|x) = \prod_{k=1}^K\frac{\exp(r(y_k, x))}{\exp(r(y_k, x))  + \ldots + \exp(r(y_K, x))}$$
The logarithm of this expression will be the loss for training the reward model.

\end{document}


In this subsection I'll try to explain what is PPO and how it is used in RLHF.

\textbf{RL setup in RLHF:}

\begin{itemize}
\item Current \textbf{state} is a currently generated prefix $x$.
\item Next \textbf{action} is the choice (generation) of a next token $w_i$. For a prompt, a sequence of actions (a trajectory)
$$(x, w_1, \ldots, w_t)\mapsto(x, w_1, \ldots, w_t, w_{t+1})$$
results in a final pair $(x, y)$ of prompt and its completion.
\item \textbf{Agent} is the LLM $\pi_{\theta}(y|x)$. It chooses the next action on each step.
\item \textbf{Reward} on step $t$ is $r_t = r(x, w_1\ldots w_t)$.
\end{itemize}

Now, we want the reward to be high, but what do we actually optimize at a step $(x, s)$, $s=w_1\ldots w_t$? There can be several options, for example, a weighted average
$$\widehat{R}(x, s) = r(x, s) + \gamma r(x, sw_{t+1}) + \gamma^2 r(x, sw_{t+1}w_{t+2}) + \ldots$$
that shows what rewards we'll get all along the future trajectory.




The thing is that we don't want to just optimize a momentary reward $r(x, sw_{t+1})$, $s=w_1\ldots w_t$ on each step. Indeed, even if every step is good, we can't be sure that the resulting trajectory $(x, y)$ is optimal. 

So, on each step we want to optimize the expected reward 

with some $\gamma\in[0,1]$. So, this is something like a weighted average reward over all the actions starting from $(x, s)$. More accurately, we would like to optimize its expectation
$$V^{\pi}(x, s) = \mathbb{E}\left[r(x, s) + \gamma r(x, sw_{t+1}) + \gamma^2 r(x, sw_{t+1}w_{t+2}) + \ldots\right],$$
where the expectation is taken over the sampling as each step of generation.

The problem is that we can't just peek into the future and calculate $V^{\pi}(x, s)$. So, we need to estimate it. For example, with 



PPO comes from the \textbf{advantage actor-critic approach}. What are the main ideas of it:
\begin{itemize}
\item It's good to optimize $r(x,y)$, but it can be noisy
\end{itemize}

\begin{align*}
\mathcal{L}_{\mathrm{RLHF-clip}} = &\mathbb{E}_{x\sim\mathcal{D}, y\sim\pi_{\theta}(y|x)}\min\left[\frac{\pi_{\theta}(y|x)}{\pi_{\mathrm{SFT}}(y|x)}r(x, y); \mathrm{clip}\left(\frac{\pi_{\theta}(y|x)}{\pi_{\mathrm{SFT}}(y|x)}, 1 - \epsilon, 1 + \epsilon\right)r(x, y) \right],
\end{align*}

%\subsection{$r(x, y)$ or $r(y)$?}

%Preferable format of the reward model training data is (prompt + accepted completion, prompt + rejected completion). For example: "How to steal a car?" + "I don't know and even if I knew I wouldn't tell you!" vs "How to steal a car?" + "Well, first of all you should ensure that no one is watching you. Then..."

%In some cases, as in the RLHF demo notebook, we only have accepted and rejected sentences, so technically prompts are empty. Although it's not the best data option, 
