---
layout: post
title: "Paperwatch 08.09.2025"
categories: blog
permalink: /paperwatch-08-09-2025/
---

**Paperwatch 08.09.2025 by Stanislav Fedotov (Nebius Academy)**

# Writing-Zero: Bridge the Gap Between Non-verifiable Tasks and Verifiable Rewards

I love trying LLMs for creative tasks, even if most of the time they get me disappointed. I actually think that RLHF is partially to blame. It's known to restrict the output distribution, and in pursue of human alignment it leads to partial mode collapsing, creating things like:

* The annoying GPT syle
* Favouritism towards certain items or words. For example, GPT-5 has already made me hate "surgical fixes" in coding and "resonance" in creative writing. GPT-4o's love for "shadow weaving" (whatever this could be) was also notorious.
* Shift from beauty to utility in image generation. Maybe GPT-5 responds to "serious" prompts better, but just never produces the beautiful and crazy stuff DALL-E 3 does!

Sorry, I've got a bit carried away, but now you probably feel more like boosting LLM creativity is important. 

To tell the truth, the authors of this paper don't exactly address the problem of mode collapsing in creative writing. They ponder whether RLVR (RL with Verifyable Rewards) can be used for tasks where quality is a subjective thing - such as creative writing. Usual verifyable rewards rely on some objective ground truth: is the answer correct, does the code pass the tests, does an LLM agent successfully perform its task etc. What to do when truth itn't objective?

Actually, we've long known the answer, and it is - train a reward model on human preferences! Just like it was done in the original RLHF.

Of course, there are some differences:
* In the original RLHF, we had scalar reward function, which means that it predicted only one number - the reward score.
* The authors of this paper fine tuned **Qwen3-32B-Base** into **GenRM** - a reasoning model for the **pairwise scoring** task, and they prompted it to reason in several stages:

    * First, it formulates numerical scoring criteria
    * Then, it assigns scores to each of the two inputs according to these criteria
    * Finally, it outputs numerical scores in `\boxed{}`

The examples in the paper are in Chinese, but I've translated one for you with Google translate. I think this example makes the authors' idea quite clear:

```
# Assessment Assistant Answers
## 1. Unique Assessment Criteria
Based on user needs, this is a Creative Writing question, requiring you to write an argumentative essay based on a specific story. To address this need, I have developed the following specific evaluation criteria:
- **Comprehension of the Theme (1-10 points)**
- Deep Comprehension (9-10 points): Accurately grasps the core idea of ​​the ant story and deeply analyzes its moral.
- Thorough Comprehension (6-8 points): Basic understanding of the story theme, but insufficient exploration.
- Superficial Comprehension (3-5 points): A superficial understanding of the story theme.
- Misunderstanding (1-2 points): A serious misunderstanding of the story theme.
- **Argumentation (1-10 points)**
- Lofty Aspiration (9-10 points): Profound argument, clear thinking, rigorous reasoning, and novel perspective.
- Clear Viewpoint (6-8 points): Clear argument, general reasoning, and some degree of persuasiveness.
- Vague Viewpoint (3-5 points): Unclear argument, disorganized reasoning.
- No Clear Viewpoint (1-2 points): Almost no central argument or a disorganized viewpoint.
- **Rich Argumentation (1-10 points)**
- Strong Argumentation (9-10 points)** - Substantial Argument (6-8 points): The argument is relatively sound, with appropriate and typical examples, and the reasoning is thorough. - Thin Argument (3-5 points): The argument is insufficient, and the examples are not typical. - Lack of Argument (1-2 points): There is little evidence to support the thesis. - **Language Expression (1-10 points)**
- Elegant and Fluent (9-10 points): The wording is precise, the narrative is vivid, the expression is fluent, and the rhetoric is rich. - Clear and Coherent (6-8 points): The language is fluent, the narrative is smooth, and there is a certain linguistic charm. - Basically Fluent (3-5 points): The text is basically fluent, but lacks elegant expression. - Confused Expression (1-2 points): The language is stiff and the expression is not fluent. - **Meets the Question Requirements (1-10 points)**
- Fully Meets (9-10 points): It fully meets all the requirements of the question (self-determined theme and title, no less than 800 words). - Basically Meets (6-8 points): The text is fluent, but lacks elegant expression. - Partially Meets (3-5 points): Clearly fails to meet certain requirements - Seriously Failed (1-2 points): Almost fails to meet the basic requirements of the topic
- **Originality (1-10 points)**
- Highly Original (9-10 points): Unique perspective, not bound by conventional wisdom, and demonstrates personal depth of thought - Moderately Original (6-8 points): Some independent thinking, but most ideas fall within conventional wisdom - Moderately Original (3-5 points): General thinking, not high originality - Mostly Unoriginal (1-2 points): Almost entirely repetitive clichés, lacking personal thought
## 2. Reflection and Answer
None
## 3. Analysis
### General Assessment Criteria Analysis
** Following Instructions**:
- Assistant 1: Completely followed all instructions, developed an appropriate title for the essay, clearly defined the main idea, and met the required word count. Rating: 10 points
- Helper 2: The article fails to fully follow the instructions. The article makes no reference to the story of the two ants and fails to provoke reflection and insight based on the story. Rating: 2 points
** Usefulness**:
- Helper 1: The article provides in-depth reflection on the dialectical relationship between perseverance and wisdom. The content is comprehensive and well-structured, providing significant thought-provoking insights for the reader. Rating: 9 points
- Helper 2: The article deviates from the required thinking trajectory. Although it focuses on learning and process, it has little relevance to the original ant story, making it somewhat less practical. Rating: 3 points
** Relevance**:
- Helper 1: The article closely revolves around the behavior of the two ants, connecting these two behaviors to the relationship between perseverance and wisdom, making it highly relevant. Rating: 10 points
- Helper 2: The article has little relevance to the original ant story, shifting the focus to the relationship between process and outcome, failing to connect with the given material. Rating: 4 points
** Harmlessness**:
- Helper 1: The article contains positive and healthy content, without any harmful content. Score: 10 points
- Assistant 2: The content is positive and healthy, without any harmful content. Score: 10 points
### Analysis of Specific Evaluation Criteria
** Thematic Understanding**:
- Assistant 1: The author deeply understands the moral of the story of the two ants, emphasizing the dialectical relationship between perseverance and wisdom. The author's thesis is accurate. Score: 10 points
- Assistant 2: The author fails to understand or ignores the original story, deviating from the main topic to a discussion of process and results, resulting in a serious misunderstanding. Score: 3 points
** Argumentation**:
- Assistant 1: The central argument is clear, demonstrating the complementary relationship between perseverance and wisdom. The author's thinking is clear and the perspective is insightful. Score: 9 points
- Assistant 2: The argument is relatively one-sided, focusing only on different aspects of the process without fully analyzing the overall impact of the two ants' behaviors. Score: 6 points
** Richness of Argument**:
- Assistant 1: The author's thesis is rich and well-illustrated, including multi-angle analysis of perseverance and adaptability. The argument is well-founded. Score: 9 points
- Assistant 2: The argument is relatively weak, with limited examples (ceramics and learning), and the material provided in the question is poorly utilized. Score: 5 points
** Language Expression**:
- Assistant 1: The language is fluent and natural, the argument is well-written, and the rhetoric is appropriate. Score: 9 points
- Assistant 2: The language is generally coherent, but there is some repetition, such as the repeated emphasis on the importance of "process." Score: 6 points
** Compliance with Topic Requirements**:
- Assistant 1: The essay fully meets all the requirements of the question, with the essay choosing its own angle, theme, and title, and meeting the 800-word minimum requirement. Score: 10 points
- Assistant 2: The essay fails to meet the requirements of the question, is not based on the ant story, and is not sufficiently connected to the material. Score: 4 points
** Originality**:
- Assistant 1: The essay has a novel theme, combining perseverance and wisdom, and demonstrates deep thinking. Score: 8 points
- Assistant 2: The essay approaches the question from the perspective of the importance of process. While insightful, it lacks originality. Score: 6 points
## 4. Weighting
** General Evaluation Criteria **:
- Following Instructions: 15% - Usefulness: 15% - Relevance: 15% - Non-harmfulness: 10%
** Specific Evaluation Criteria **:
- Topic Understanding: 15% - Argument Development: 8% - Argument Richness: 10% - Language Expression: 12%
## 5. Scoring
** Assistant 1 Score **:
- Following Instructions: 10 × 15% = 1.5
- Usefulness: 9 × 15% = 1.35
- Relevance: 10 × 15% = 1.5
- Non-harmfulness: 10 × 10% = 1.0
- Topic Understanding: 10 × 15% = 1.5
- Argument Development: 9 × 8% = 0.72
- Argument Richness: 9 × 10% = 0.9
- Language Expression: 9 × 12% = 1.08
Assistant 1 Weighted Total Score: 9.54
**Assistant 2 Score**:
- Following Instructions: 2 × 15% = 0.3
- Usefulness: 3 × 15% = 0.45
- Relevance: 4 × 15% = 0.6
- Harmlessness: 10 × 10% = 1.0
- Topic Comprehension: 3 × 15% = 0.45
- Argument Development: 6 × 8% = 0.48
- Argument Richness: 5 × 10% = 0.5
- Language Expression: 6 × 12% = 0.72
Assistant 2 Weighted Total Score: 4.5
## 6. Output Final Score
\boxed{9.54,4.5}
```

Let's discuss the training data and the training process of GenRM.

The data originated from including 30K writing-related in-house pairs. Hopefully, they were well-labelled; however, scoring creativity is a very difficult thing, so the authors go an extra mile to clean the data.

First, they trained a scalar Reward Model on a larger set of 200K pairwise preferences and used it to choose high-quality pairs - those with higher chosen response reward and a larger reward gap. This resulted in about 10K original writing pairs

Then, they did **cold-start fine-tuning** on the reasoning traces of **Claude-3.5-Sonnet**. Since Claude is known to suffer from positional bias (favouring the first element of a pair), the authors fed it pairs in original and reversed order and discarded those for which Claude failed at lease once. This results in 500k pairs (+500k reverse ordered) for the cold-start fine-tuning.

Then, the RL phase follows. The authors do it through GRPO with quite straightforward rewards (correctness + format) with some minor tweaks.

Now that they have a pairwise reward model, it's time to use it to make an LLM very creative!

## Training a writer model with the pairwise reward model

The authors use **BRPO** (**Bootstrapped Relative Policy Optimization**) - a modified version of GRPO which uses no ground truth, because there is no such thing in measuring creativity. Instead, from a batch of outputs for a prompt $x$, they choose a randomly picked one $o_i$ and treat it as a ground truth. Then, for all other outputs $o_j$, they compute the reward $R_j = $ GenRM($o_i$, $o_j$) - and this goes into the advantage function.

![]({{ site.baseurl }}/assets/images/paperwatch-08-09-2025/zero-writer-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Why does it make sense? Such a reward pushes the LLM to generate outputs that are more creative than a randomly picked $o_i$. The authors filter out outliers - output groups with too high or too low average reward to avoid RL training instability.

To test their model, the authors used another scalar reward model **Eval RM**. And it seems that the pairwise reward model works better than a scalar one:

![]({{ site.baseurl }}/assets/images/paperwatch-08-09-2025/zero-writer-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

A small human study was also in favour of the model trained with pairwise reward.

I hope to see more papers about creative writing in the future! Maybe at some point LLMs will be not so irksomely repetitive as they are now.

# Deep Think with Confidence

[https://arxiv.org/pdf/2508.15260](https://arxiv.org/pdf/2508.15260)

If for some reason you have lots of compute to spend on a particular task, there are many techniques that can help you spend it. The most straightforward one is to sample several solutions and somehow choose one of them. But how? The most popular strategy is to select a solution with the most popular answer - it's knows as *self-consistency*. However, it is quite naive. Even a dreadfully wrong solution might arrive at the correct answer. And what if there is no short, close-form answer to the problem?

The authors choose to use **confidence**. On token level, they define it as

$$C_i = -\frac1{|W|}\sum_{w\in W}\log{p(x_{i} = w\mid x_{1:(i-1)})}$$ 

On the completion level, it is the average:

$$C_{\text{avg}} = \frac1L\sum{i=1}^LC_i$$

Here, $C_i$ shows how large are predicted probabilities on average. If all of them are around zero, the model isn't very confident in the next token - and this, by the way, may indicate hallucination. If several probabilites are high, $C_i$ will also be higher.

So, indeed, it's plausible that more confident solutions are more inclined to be correct. The authors are not the first to employ this, of course - but they add a touch of refinement to the approach.

First of all, instead of just comparing total confidence of different solutions, the authors suggest generating a *batch of completions* for a given problem in parallel and analyze their confidence on the fly.

![]({{ site.baseurl }}/assets/images/paperwatch-08-09-2025/deepconf-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

More accurately, they suggest consdering overlapping *groups* $G_j$ inside a solution and computing per-group average confidence $C_{G_j}$. Now, low average confidence in a group might indicate hallucination, and if $C_{G_j}$ goes below a certain threshold $\tau$, generation stops and the solution is instantly discarded. (The threshold is chosen during a warmup round; a bit of overengineering maybe.)

*Candidate solutions* are chosen from a batch as top-90% with respect to following statistics:

* Average confidence of the 10% least confident groups. These 10% are supposed to capture all the worst problems during generation (including hallucinations). So, if their confidence is decent, the whole solution is probably ok
* Confidence of the tail ground - the final $K$ tokens
* Just average confidence across groups.

(Another option is selecting top-10% from the solution batch as candidates - but this is more restricted than top-90% and hurts diversity.)

![]({{ site.baseurl }}/assets/images/paperwatch-08-09-2025/deepconf-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Having the top-90% solutions (or top-10%), we now select the *final answer* as the most popular one (well, nothing too original here). The *final solution* is chosen as the most confident one among those providing the right answer.

And, by the way, here's some experimental data supporting the idea that more confident solutions tend to be more accurate:

![]({{ site.baseurl }}/assets/images/paperwatch-08-09-2025/deepconf-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

The authors also suggest that batch width for a problem should depend on its complexity. If we denote by $V(a)$ the number of solutions in the current solution batch that arrived to the answer $a$, and by $\widehat{a}$ the correct answer, the authors define the complexity as

$$\beta = \frac{V(\widehat{a})}{\sum_aV(a)}$$

The authors suggest to increase the batch intil $\beta$ stays lower than a certain threshold - that is, until the chosen answer isn't supported by sufficiently many candidate solutions.

Here are some results.

At the left picture, the two bars marked by **deepconf@512** represent the confidence-driven selection from a batch of 512 solutions; cons@512 stands for the ordinary consensus of 512 solutions (self-consistency).

![]({{ site.baseurl }}/assets/images/paperwatch-08-09-2025/deepconf-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

**Offline** here means that all solutions were allowed to finish, while in the **online** settings those falling below group confidence threshold were cut short, as described above. As you see, in offline we have good quality. And in online, less tokens are generated due to timely discarding of low-confidence solutions.

 

# On the Theoretical Limitations of Embedding-Based Retrieval


[https://arxiv.org/pdf/2508.21038](https://arxiv.org/pdf/2508.21038)

RAG is a big thing for anyone who wants factuality, especially when it's your business' internal information. And of all forms of RAG, the super star is vector database search. But how much robust is it? Well, it's pretty clear that vector search won't probably help you to find information about a particular date. Embeddings accumulate general meaning, not particular facts. But can we actually measure the vector-ability of a dataset?

The authors of this paper start from demonstrating how drastic a failure of vector search might be.

For that, let's consider a dataset of documents $X = (x_1, \ldots, x_N)$ and a set of queries $Q = (q_1,\ldots, q_M)$. Both $N$ and $M$ here can be very, very large. Millions? Easily! Now, let's consider a *ground truth matrix* $A$ with

$$a_{ij} = \begin{cases}
1,\text{ if j-th document is relevant to i-th query},\\
-1,\text{ otherwise}
\end{cases}$$

(So, queries correspond to rows and documents correspond to columns.)

The embedding model $E$ produces the *prediction* matrix $B$ with

$$
b_{ij} = E(q_i)E(x_j)^T
$$

Ideally, looking at the $i$-th row of $B$, we should be able to choose exactly the documents with 1s in the corresponding row of $A$. There are, of course, different ways of choosing the docs (top-n, threshold, etc). So, there is no ideal math formulation of this. But the authors give several alternative math approximations which we'll unceremoniously call *good properties*:

* If $a_{ij} > a_{ik}$, then $b_{ij} > b_{ik}$,
* $\text{sign}(b_{ij}) = \text{sign}(a_{ij})$,
* etc

Now, the problem is: while, strictly speaking, $A$ may be any $\pm1$-matrix, $B$ is the product

$$B = \underbrace{\begin{pmatrix}E(q_1)\\E(q_2)\\ \vdots \\ E(q_M)\end{pmatrix}}_{M\times D}\cdot
\underbrace{\begin{pmatrix}E(x_1)^T & E(x_2)^T & E(x_N)^T\end{pmatrix}}_{D\times N}$$

Here, $D$ is the embedding's dimension. It's very likely much less than $M$ (the number of potential queries) and $N$ (the number of all documents). In particular, $\text{rank}(B)\leqslant D$.

So, ideal embeddings for $X$ and $Q$ might exist only if there exists a matrix of rank $\leqslant D$ which satisfies some of the *good properties*. 

To check it, the authors run an experiment with "free embeddings". Namely:

* For given $n$ and $k$, they take a matrix $A$ of size $\left(\begin{smallmatrix}n\\k\end{smallmatrix}\right)\times n$, whose rows are all possbile combinations of $k$ ones and $(n - k)$ minus ones. (These combinations are shuffled randomly.) Which means, this matrix $A$ correspond to a situation where every subset of $k$ virtual documents is the top-k for some virtual query.
* They try to find query and document *embedding vectors* that would have *good properties* with respect to this matrix. Yes, no actual queries or documents - just vectors and gradient descent! That's why it's called "*free* embeddings".
* They make this experiment with gradually increasing numbers of documents $n$ (and fixed $k$) until the optimization
is no longer able to solve the problem (i.e. achieve 100% accuracy). They call this moment the **critical-n point**.

Here you can see the evolution of critical-n points:

![]({{ site.baseurl }}/assets/images/paperwatch-08-09-2025/sign-rank-0.png){: .responsive-image style="--img-desktop:70%; --img-mobile:100%;"}

They seem to form a nice cubic pattern, and they interpolate it to get:

| critical-n | D    |
| ----------- | ---- |
| 500k        | 512  |
| 1.7m        | 768  |
| 4m          | 1024 |
| 107m        | 3072 |
| 250m        | 4096 |


The numbers may seem high, but actually they are reasonable for enterprise applications. Also, don't forget that they emerge from the "free embedding" problem, and in reality the numbers will be less optimistic.

Anyway, this shows that, with a sufficiently large number of documents, fixed-dimensions embeddings can't even theoretically provide ideal RAG. To illustrate this, the authors create the **LIMITS** dataset. The idea is to create a set of documents and queries such that for each query it's perfectly clear which documents are relevant and which are not. Here's an example of a document:

* *"Julia Matsil likes Feta Cheese, Sewing, Cymbals, Sichuan Peppercorns, Wrestling, Volleyball, Necklaces, Rafflesia Flowers, Nectarines, Scissors, Post-War Boom, the New York Yankees, Workbenches, Buckwheat, Astronomy, Surrealism, Cantaloupes, Kickboxing, Black Tea, Desks, Patience, Oil Painting, Holland Lops, Mousse, Ventriloquism, Sudoku Puzzles, Wonder, Screwdrivers, Iced Tea, Catfish, Robins, Rose Quartz, Parfaits, Wisdom, Goblin Sharks, The Sound of Crickets Chirping, East Asian Calligraphy, The Space Age, Potato Bread, Wabi-sabi, Irises, Sprinkles, Red Bell Peppers, Towels and Intuition."*

A query would be like "Who likes Feta Cheese", so theoretically relevant documents would be exactly those mentioning Feta Cheese. I'm not sure vector search is carved to work this way - finding mentioning of particular items. And, quite expectedly, good embedders fail on this dataset even for meagre $N = 46$ documents:

![]({{ site.baseurl }}/assets/images/paperwatch-08-09-2025/sign-rank-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Even fine tuning doesn't help. (Again, I would expect that.)

One of the things which make this dataset stand out is its graph density (too many queries for which each document is relevant):

![]({{ site.baseurl }}/assets/images/paperwatch-08-09-2025/sign-rank-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

This might be interpreted as the dataset's forte (tough benchmark for your RAG system) or as its weakness (it's not very realistic). I find myself in the latter camp. But however much I doubt the authors' methodology, I cannot agree more that RAG with vector search has its own restrictions, so:

* Use it with care; don't underestimate other search strategies
* Use two-step RAG patterns with re-ranking
