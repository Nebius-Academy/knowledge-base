---
layout: post
title: "Paperwatch 08.09.2025"
categories: blog
permalink: /paperwatch-08-09-2025/
---

**Paperwatch 08.09.2025 by Stanislav Fedotov (Nebius Academy)**

# Writing-Zero: Bridge the Gap Between Non-verifiable Tasks and Verifiable Rewards

I love trying LLMs for creative tasks, even if most of the time they get me disappointed. I actually think that RLHF is partially to blame. It's known to restrict the output distribution, and in pursue of human alignment it leads to partial mode collapsing, creating things like:

* The annoying GPT syle
* Favouritism towards certain items or words. For example, GPT-5 has already made me hate "surgical fixes" in coding and "resonance" in creative writing. GPT-4o's love for "shadow weaving" (whatever this could be) was also notorious.
* Shift from beauty to utility in image generation. Maybe GPT-5 responds to "serious" prompts better, but just never produces the beautiful and crazy stuff DALL-E 3 does!

Sorry, I've got a bit carried away, but now you probably feel more like boosting LLM creativity is important. 

To tell the truth, the authors of this paper don't exactly address the problem of mode collapsing in creative writing. They ponder whether RLVR (RL with Verifyable Rewards) can be used for tasks where quality is a subjective thing - such as creative writing. Usual verifyable rewards rely on some objective ground truth: is the answer correct, does the code pass the tests, does an LLM agent successfully perform its task etc. What to do when truth itn't objective?

Actually, we've long known the answer, and it is - train a reward model on human preferences! Just like it was done in the original RLHF.

Of course, there are some differences:
* In the original RLHF, we had scalar reward function, which means that it predicted only one number - the reward score.
* The authors of this paper fine tuned **Qwen3-32B-Base** into **GenRM** - a reasoning model for the **pairwise scoring** task, and they prompted it to reason in several stages:

    * First, it formulates numerical scoring criteria
    * Then, it assigns scores to each of the two inputs according to these criteria
    * Finally, it outputs numerical scores in `\boxed{}`

The examples in the paper are in Chinese, but I've translated one for you with Google translate. I think this example makes the authors' idea quite clear:

```
# Assessment Assistant Answers
## 1. Unique Assessment Criteria
Based on user needs, this is a Creative Writing question, requiring you to write an argumentative essay based on a specific story. To address this need, I have developed the following specific evaluation criteria:
- **Comprehension of the Theme (1-10 points)**
- Deep Comprehension (9-10 points): Accurately grasps the core idea of ​​the ant story and deeply analyzes its moral.
- Thorough Comprehension (6-8 points): Basic understanding of the story theme, but insufficient exploration.
- Superficial Comprehension (3-5 points): A superficial understanding of the story theme.
- Misunderstanding (1-2 points): A serious misunderstanding of the story theme.
- **Argumentation (1-10 points)**
- Lofty Aspiration (9-10 points): Profound argument, clear thinking, rigorous reasoning, and novel perspective.
- Clear Viewpoint (6-8 points): Clear argument, general reasoning, and some degree of persuasiveness.
- Vague Viewpoint (3-5 points): Unclear argument, disorganized reasoning.
- No Clear Viewpoint (1-2 points): Almost no central argument or a disorganized viewpoint.
- **Rich Argumentation (1-10 points)**
- Strong Argumentation (9-10 points)** - Substantial Argument (6-8 points): The argument is relatively sound, with appropriate and typical examples, and the reasoning is thorough. - Thin Argument (3-5 points): The argument is insufficient, and the examples are not typical. - Lack of Argument (1-2 points): There is little evidence to support the thesis. - **Language Expression (1-10 points)**
- Elegant and Fluent (9-10 points): The wording is precise, the narrative is vivid, the expression is fluent, and the rhetoric is rich. - Clear and Coherent (6-8 points): The language is fluent, the narrative is smooth, and there is a certain linguistic charm. - Basically Fluent (3-5 points): The text is basically fluent, but lacks elegant expression. - Confused Expression (1-2 points): The language is stiff and the expression is not fluent. - **Meets the Question Requirements (1-10 points)**
- Fully Meets (9-10 points): It fully meets all the requirements of the question (self-determined theme and title, no less than 800 words). - Basically Meets (6-8 points): The text is fluent, but lacks elegant expression. - Partially Meets (3-5 points): Clearly fails to meet certain requirements - Seriously Failed (1-2 points): Almost fails to meet the basic requirements of the topic
- **Originality (1-10 points)**
- Highly Original (9-10 points): Unique perspective, not bound by conventional wisdom, and demonstrates personal depth of thought - Moderately Original (6-8 points): Some independent thinking, but most ideas fall within conventional wisdom - Moderately Original (3-5 points): General thinking, not high originality - Mostly Unoriginal (1-2 points): Almost entirely repetitive clichés, lacking personal thought
## 2. Reflection and Answer
None
## 3. Analysis
### General Assessment Criteria Analysis
** Following Instructions**:
- Assistant 1: Completely followed all instructions, developed an appropriate title for the essay, clearly defined the main idea, and met the required word count. Rating: 10 points
- Helper 2: The article fails to fully follow the instructions. The article makes no reference to the story of the two ants and fails to provoke reflection and insight based on the story. Rating: 2 points
** Usefulness**:
- Helper 1: The article provides in-depth reflection on the dialectical relationship between perseverance and wisdom. The content is comprehensive and well-structured, providing significant thought-provoking insights for the reader. Rating: 9 points
- Helper 2: The article deviates from the required thinking trajectory. Although it focuses on learning and process, it has little relevance to the original ant story, making it somewhat less practical. Rating: 3 points
** Relevance**:
- Helper 1: The article closely revolves around the behavior of the two ants, connecting these two behaviors to the relationship between perseverance and wisdom, making it highly relevant. Rating: 10 points
- Helper 2: The article has little relevance to the original ant story, shifting the focus to the relationship between process and outcome, failing to connect with the given material. Rating: 4 points
** Harmlessness**:
- Helper 1: The article contains positive and healthy content, without any harmful content. Score: 10 points
- Assistant 2: The content is positive and healthy, without any harmful content. Score: 10 points
### Analysis of Specific Evaluation Criteria
** Thematic Understanding**:
- Assistant 1: The author deeply understands the moral of the story of the two ants, emphasizing the dialectical relationship between perseverance and wisdom. The author's thesis is accurate. Score: 10 points
- Assistant 2: The author fails to understand or ignores the original story, deviating from the main topic to a discussion of process and results, resulting in a serious misunderstanding. Score: 3 points
** Argumentation**:
- Assistant 1: The central argument is clear, demonstrating the complementary relationship between perseverance and wisdom. The author's thinking is clear and the perspective is insightful. Score: 9 points
- Assistant 2: The argument is relatively one-sided, focusing only on different aspects of the process without fully analyzing the overall impact of the two ants' behaviors. Score: 6 points
** Richness of Argument**:
- Assistant 1: The author's thesis is rich and well-illustrated, including multi-angle analysis of perseverance and adaptability. The argument is well-founded. Score: 9 points
- Assistant 2: The argument is relatively weak, with limited examples (ceramics and learning), and the material provided in the question is poorly utilized. Score: 5 points
** Language Expression**:
- Assistant 1: The language is fluent and natural, the argument is well-written, and the rhetoric is appropriate. Score: 9 points
- Assistant 2: The language is generally coherent, but there is some repetition, such as the repeated emphasis on the importance of "process." Score: 6 points
** Compliance with Topic Requirements**:
- Assistant 1: The essay fully meets all the requirements of the question, with the essay choosing its own angle, theme, and title, and meeting the 800-word minimum requirement. Score: 10 points
- Assistant 2: The essay fails to meet the requirements of the question, is not based on the ant story, and is not sufficiently connected to the material. Score: 4 points
** Originality**:
- Assistant 1: The essay has a novel theme, combining perseverance and wisdom, and demonstrates deep thinking. Score: 8 points
- Assistant 2: The essay approaches the question from the perspective of the importance of process. While insightful, it lacks originality. Score: 6 points
## 4. Weighting
** General Evaluation Criteria **:
- Following Instructions: 15% - Usefulness: 15% - Relevance: 15% - Non-harmfulness: 10%
** Specific Evaluation Criteria **:
- Topic Understanding: 15% - Argument Development: 8% - Argument Richness: 10% - Language Expression: 12%
## 5. Scoring
** Assistant 1 Score **:
- Following Instructions: 10 × 15% = 1.5
- Usefulness: 9 × 15% = 1.35
- Relevance: 10 × 15% = 1.5
- Non-harmfulness: 10 × 10% = 1.0
- Topic Understanding: 10 × 15% = 1.5
- Argument Development: 9 × 8% = 0.72
- Argument Richness: 9 × 10% = 0.9
- Language Expression: 9 × 12% = 1.08
Assistant 1 Weighted Total Score: 9.54
**Assistant 2 Score**:
- Following Instructions: 2 × 15% = 0.3
- Usefulness: 3 × 15% = 0.45
- Relevance: 4 × 15% = 0.6
- Harmlessness: 10 × 10% = 1.0
- Topic Comprehension: 3 × 15% = 0.45
- Argument Development: 6 × 8% = 0.48
- Argument Richness: 5 × 10% = 0.5
- Language Expression: 6 × 12% = 0.72
Assistant 2 Weighted Total Score: 4.5
## 6. Output Final Score
\boxed{9.54,4.5}
```

Let's discuss the training data and the training process of GenRM.

The data originated from including 30K writing-related in-house pairs. Hopefully, they were well-labelled; however, scoring creativity is a very difficult thing, so the authors go an extra mile to clean the data.

First, they trained a scalar Reward Model on a larger set of 200K pairwise preferences and used it to choose high-quality pairs - those with higher chosen response reward and a larger reward gap. This resulted in about 10K original writing pairs

Then, they did **cold-start fine-tuning** on the reasoning traces of **Claude-3.5-Sonnet**. Since Claude is known to suffer from positional bias (favouring the first element of a pair), the authors fed it pairs in original and reversed order and discarded those for which Claude failed at lease once. This results in 500k pairs (+500k reverse ordered) for the cold-start fine-tuning.

Then, the RL phase follows. The authors do it through GRPO with quite straightforward rewards (correctness + format) with some minor tweaks.

Now that they have a pairwise reward model, it's time to use it to make an LLM very creative!

## Training a writer model with the pairwise reward model

The authors use **BRPO** (**Bootstrapped Relative Policy Optimization**) - a modified version of GRPO which uses no ground truth, because there is no such thing in measuring creativity. Instead, from a batch of outputs for a prompt $x$, they choose a randomly picked one $o_i$ and treat it as a ground truth. Then, for all other outputs $o_j$, they compute the reward $R_j = $ GenRM($o_i$, $o_j$) - and this goes into the advantage function.

![]({{ site.baseurl }}/assets/images/paperwatch-08-09-2025/zero-writer-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Why does it make sense? Such a reward pushes the LLM to generate outputs that are more creative than a randomly picked $o_i$. The authors filter out outliers - output groups with too high or too low average reward to avoid RL training instability.

To test their model, the authors used another scalar reward model **Eval RM**. And it seems that the pairwise reward model works better than a scalar one:

![]({{ site.baseurl }}/assets/images/paperwatch-08-09-2025/zero-writer-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

A small human study was also in favour of the model trained with pairwise reward.

I hope to see more papers about creative writing in the future! Maybe at some point LLMs will be not so irksomely repetitive as they are now.

# Deep Think with Confidence

[https://arxiv.org/pdf/2508.15260](https://arxiv.org/pdf/2508.15260)

If for some reason you have lots of compute to spend on a particular task, there are many techniques that can help you spend it. The most straightforward one is to sample several solutions and somehow choose one of them. But how? The most popular strategy is to select a solution with the most popular answer - it's knows as *self-consistency*. However, it is quite naive. Even a dreadfully wrong solution might arrive at the correct answer. And what if there is no short, close-form answer to the problem?

The authors choose to use **confidence**. On token level, they define it as

$$C_i = -\frac1{|W|}\sum_{w\in W}\log{p(x_{i} = w\mid x_{1:(i-1)})}$$ 

On the completion level, it is the average:

$$C_{\text{avg}} = \frac1L\sum{i=1}^LC_i$$

Here, $C_i$ shows how large are predicted probabilities on average. If all of them are around zero, the model isn't very confident in the next token - and this, by the way, may indicate hallucination. If several probabilites are high, $C_i$ will also be higher.

So, indeed, it's plausible that more confident solutions are more inclined to be correct. The authors are not the first to employ this, of course - but they add a touch of refinement to the approach.

First of all, instead of just comparing total confidence of different solutions, the authors suggest generating a *batch of completions* for a given problem in parallel and analyze their confidence on the fly.

![]({{ site.baseurl }}/assets/images/paperwatch-08-09-2025/deepconf-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

More accurately, they suggest consdering overlapping *groups* $G_j$ inside a solution and computing per-group average confidence $C_{G_j}$. Now, low average confidence in a group might indicate hallucination, and if $C_{G_j}$ goes below a certain threshold $\tau$, generation stops and the solution is instantly discarded. (The threshold is chosen during a warmup round; a bit of overengineering maybe.)

*Candidate solutions* are chosen from a batch as top-90% with respect to following statistics:

* Average confidence of the 10% least confident groups. These 10% are supposed to capture all the worst problems during generation (including hallucinations). So, if their confidence is decent, the whole solution is probably ok
* Confidence of the tail ground - the final $K$ tokens
* Just average confidence across groups.

(Another option is selecting top-10% from the solution batch as candidates - but this is more restricted than top-90% and hurts diversity.)

![]({{ site.baseurl }}/assets/images/paperwatch-08-09-2025/deepconf-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Having the top-90% solutions (or top-10%), we now select the *final answer* as the most popular one (well, nothing too original here). The *final solution* is chosen as the most confident one among those providing the right answer.

And, by the way, here's some experimental data supporting the idea that more confident solutions tend to be more accurate:

![]({{ site.baseurl }}/assets/images/paperwatch-08-09-2025/deepconf-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

The authors also suggest that batch width for a problem should depend on its complexity. If we denote by $V(a)$ the number of solutions in the current solution batch that arrived to the answer $a$, and by $\widehat{a}$ the correct answer, the authors define the complexity as

$$\beta = \frac{V(\widehat{a})}{\sum_aV(a)}$$

The authors suggest to increase the batch intil $\beta$ stays lower than a certain threshold - that is, until the chosen answer isn't supported by sufficiently many candidate solutions.

Here are some results.

At the left picture, the two bars marked by **deepconf@512** represent the confidence-driven selection from a batch of 512 solutions; cons@512 stands for the ordinary consensus of 512 solutions (self-consistency).

![]({{ site.baseurl }}/assets/images/paperwatch-08-09-2025/deepconf-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

**Offline** here means that all solutions were allowed to finish, while in the **online** settings those falling below group confidence threshold were cut short, as described above. As you see, in offline we have good quality. And in online, less tokens are generated due to timely discarding of low-confidence solutions.


# Towards a Unified View of Large Language Model Post-Training

[https://www.arxiv.org/pdf/2509.04419](https://www.arxiv.org/pdf/2509.04419)

SFT and RL are considered as two very different LLM post-training methods, each with its own pros, and cons, and use cases. But the authors of this paper found a way of uniting them both under one mathematical framework and used it to suggest the **Hybrid Post-Training** (**HPT**) approach. Let's see how they did it.

First of all, their framework operates with *gradient updates*, suggesting a unified view on them for both SFT and RL. To start with, let's recall loss functions and gradient updates for SFT and PPO:

![]({{ site.baseurl }}/assets/images/paperwatch-08-09-2025/hpt-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

where $\tau_i$ is the completion for the prompt $q_i$ and $\tau_{i,j}$ is its $j$-th token.

![]({{ site.baseurl }}/assets/images/paperwatch-08-09-2025/hpt-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Here $\hat{A}$ is the advantage function, which can be complicated and is calculated from per-token rewards and predictions of a value function, which in turn, predicts the average future reward. Check [here](https://nebius-academy.github.io/knowledge-base/rl-training/#per-token-objective) for details, if you're curious. For GRPO, the formulas are more or less the same; with just a more straightforward advantage formula (normalization over a batch):

![]({{ site.baseurl }}/assets/images/paperwatch-08-09-2025/hpt-2.png){: .responsive-image style="--img-desktop:50%; --img-mobile:80%;"}

When you look at the gradient formulas like that, you start noticing similarity. Let's define the **Unified Policy Gradient Estimator**:

![]({{ site.baseurl }}/assets/images/paperwatch-08-09-2025/hpt-4.png){: .responsive-image style="--img-desktop:80%; --img-mobile:100%;"}

Then the gradient of SFT can be rewritten as its special case, where the advantage is just always 1 (it's not the same as having the reward always equal to 1, though):

![]({{ site.baseurl }}/assets/images/paperwatch-08-09-2025/hpt-3.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

There's another dimension to these formulas and to the difference between SFT and online/offline RL - it's where we get the trajectories $\tau_{i,j}$, $j=0,ldots,L$ to plug into the gradients. Naturally, there are three ways:

* For an "ideal" policy - that is, $\tau_{i,j}$ are the ground truth for the query $q_i$. This is SFT
* From an experience buffer - that is, $\tau_{i,j}$ were generated by some earlier version of the LLM in training. This is offline RL
* On the fly - this extreme version corresponds to fully online RL

You probably wonder why this matters, aside from being mathematically appealing? The thing is, that we can now bring these strategies together into a hyprid learning algorithm, conbining the SFT's power of giving new competences and RL's power of awakening latent capabilities. The authors also compare RL to exploration and SFT to exploitation, which is partially justified by the fact that RL allows the LLM to explore completions outside of the training data distribution - and, in particular, learn a thing or two about which answers are incorrect. On the other hand, it is believed that RL isn't capable of fostering new resoning strategies, while SFT with the right data, theoretically, can do it. (See [this paper](https://arxiv.org/pdf/2504.13837), for example.) As I like to repeat, RL only amplifies those capabilities already present, even if dormant, in the LLM.

An interesting illustration of the latter is the fact that RL increases *pass@1* metrics - that is, the correctness of the most likely completion - while dropping *pass@k* for large *k* in comparison with the best model. This might be interpreted as:

* The base model knows a larger variety of "tricks" but might not know well when to use them
* RL doesn't help the LLM to learn new "tricks"; instead, it amplifies some of them at the expense of the others

(See [this paper](https://arxiv.org/pdf/2504.13837), again, for more details.)


The **Hybrid Post-Training** (**HPT**) strategy combines SFT, online and offline RL in a quite straightforward way:

![]({{ site.baseurl }}/assets/images/paperwatch-08-09-2025/hpt-5.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Here, $\alpha$ and $\beta$ depend on the performance feedback $P$:

$$\alpha = \begin{cases}
1,\text{  if }P\geqslant\gamma
0,\text{ otherwise}
\end{cases}\qquad
\beta = \begin{cases}
0,\text{  if }P\geqslant\gamma
1,\text{ otherwise}
\end{cases}$$

where $\gamma$ is a certain threshold. So, if a model copes with the problem reasonably well, the LLM is allowed to explore the distribution throught RL. But if it underperforms, SFT comes to aide.

The choice of training and evaluation data is quite conservative - mostly math. An interesting thing is that the hybrind strategy steadily outperforms the usual SFT + RL pipeline:

![]({{ site.baseurl }}/assets/images/paperwatch-08-09-2025/hpt-6.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

HPT also tends to have higher results at *pass@k* checks for large *k*, which might indicate that HPT suppresses less problem-solving "tricks" than RL does:

![]({{ site.baseurl }}/assets/images/paperwatch-08-09-2025/hpt-7.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Even though the training setup the authors experimented with is very simplistic (basically, just Q&A), probably it could be extended to a broader strategy. This way we might eventially obtain models with all the good effects of RL but without as much mode collapsing.

# RL's Razor: Why Online Reinforcement Learning Forgets Less

[https://www.arxiv.org/pdf/2509.04259](https://www.arxiv.org/pdf/2509.04259)

When training an LLM with RL, you should always be aware of two risks:

* Gradients going unstable making the training process deteriorate. To counter this, we have the huge machinery of advantages, clippings, value functions, batch normalization etc
* The policy being trained straying too far away from the base one, which often tells of reward hacking and leads to downstream quality degradation. Stabilizing the gradients and controlling their magnitude does work towards preventing this kind of trouble. Another, specific remedy is the KL-regularization - the

  $$-D_{\text{KL}}\left(\pi_{\theta}\vert\vert\pi_{\text{old}}\right)$$

  term, which is the Kullback-Leibler divergence - the most popular measure of similarity between two distibutions. By keeping the KL-divergence small, we prevent $\pi_{\theta}$ from deterioration.

Meanwhile, every kind of post-training potentially leads to forgetting of the previous skills. While discussing the previous paper, we've seen that RL makes the LLM forget some of the reasoning "tricks" it possessed before training. Fine tuning, in turn, is long known to make LLMs forget previously known information. And without special care, it might too lead to skill deterioration outside of the fine-tuning dataset distribution. The authors demonstrate that while training on the same data, online RL tends to forget less about out-of-training-distribution tasks:

![]({{ site.baseurl }}/assets/images/paperwatch-08-09-2025/catorlstrophy-0){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

I can't help repeating here that RL doesn't grant the LLM any new skills; if the LLM learnt something with RL, it had already known how to do it before and just hadn't been using that knowledge. Also, if you've read my review of the previous paper, "Towards the unified view...", don't let yourself be confused: in that paper, an after-RL model was compared against the base model, while here the authors compare two models fine tuned from the same base - one with RL, another with SFT.

Moreover, the authors find a way of estimating the catastrophic forgetting. It turns out (see the middle picture below) that the good old Kullback-Leibler divergence strongly correlates with the post-SFT and post-RL score on the old tasks - almost linearly:

![]({{ site.baseurl }}/assets/images/paperwatch-08-09-2025/catorlstrophy-1){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

But that's not all. As we've already discussed, KL-divergence was an explicit regularization term in the original RLHF and many subsequent RL-post-training algorithms, but in some of them it was omitted, because other gradient stabilizing optimizations were doing the job. Now, the authors of this paper claim that even from the theoretical point of view KL term isn't needed:

  **Fully online RL with binary (0-or-1) reward function will, unders certain conditions, converge to the minimum of $D_{\text{KL}}(\pi_{\theta}\vert\vert\pi_{\text{old}})$ on the surface $\{\theta\mid\mathbb{E}_{y\sim\pi_{\theta}} = 1\}$**

  ![]({{ site.baseurl }}/assets/images/paperwatch-08-09-2025/catorlstrophy-2){: .responsive-image style="--img-desktop:60%; --img-mobile:80%;"}

Of course, these "certain conditions" should mean that RL does converge which shouldn't be taken for granted. Also, it's crucial for the proof that RL is trained *fully online* - no experience replay, no batching. Anyway, the theorem is proved for a simplistic setup

$$\mathbb{E}_{y\sim\pi_{\theta}}R(y)\to\max_\theta$$

without value functions and advantage.

The experiments in the paper are quite tiny, so I'll skip them.

# On the Theoretical Limitations of Embedding-Based Retrieval


[https://arxiv.org/pdf/2508.21038](https://arxiv.org/pdf/2508.21038)

RAG is a big thing for anyone who wants factuality, especially when it's your business' internal information. And of all forms of RAG, the super star is vector database search. But how much robust is it? Well, it's pretty clear that vector search won't probably help you to find information about a particular date. Embeddings accumulate general meaning, not particular facts. But can we actually measure the vector-ability of a dataset?

The authors of this paper start from demonstrating how drastic a failure of vector search might be.

For that, let's consider a dataset of documents $X = (x_1, \ldots, x_N)$ and a set of queries $Q = (q_1,\ldots, q_M)$. Both $N$ and $M$ here can be very, very large. Millions? Easily! Now, let's consider a *ground truth matrix* $A$ with

$$a_{ij} = \begin{cases}
1,\text{ if j-th document is relevant to i-th query},\\
-1,\text{ otherwise}
\end{cases}$$

(So, queries correspond to rows and documents correspond to columns.)

The embedding model $E$ produces the *prediction* matrix $B$ with

$$
b_{ij} = E(q_i)E(x_j)^T
$$

Ideally, looking at the $i$-th row of $B$, we should be able to choose exactly the documents with 1s in the corresponding row of $A$. There are, of course, different ways of choosing the docs (top-n, threshold, etc). So, there is no ideal math formulation of this. But the authors give several alternative math approximations which we'll unceremoniously call *good properties*:

* If $a_{ij} > a_{ik}$, then $b_{ij} > b_{ik}$,
* $\text{sign}(b_{ij}) = \text{sign}(a_{ij})$,
* etc

Now, the problem is: while, strictly speaking, $A$ may be any $\pm1$-matrix, $B$ is the product

$$B = \underbrace{\begin{pmatrix}E(q_1)\\E(q_2)\\ \vdots \\ E(q_M)\end{pmatrix}}_{M\times D}\cdot
\underbrace{\begin{pmatrix}E(x_1)^T & E(x_2)^T & E(x_N)^T\end{pmatrix}}_{D\times N}$$

Here, $D$ is the embedding's dimension. It's very likely much less than $M$ (the number of potential queries) and $N$ (the number of all documents). In particular, $\text{rank}(B)\leqslant D$.

So, ideal embeddings for $X$ and $Q$ might exist only if there exists a matrix of rank $\leqslant D$ which satisfies some of the *good properties*. 

To check it, the authors run an experiment with "free embeddings". Namely:

* For given $n$ and $k$, they take a matrix $A$ of size $\left(\begin{smallmatrix}n\\k\end{smallmatrix}\right)\times n$, whose rows are all possbile combinations of $k$ ones and $(n - k)$ minus ones. (These combinations are shuffled randomly.) Which means, this matrix $A$ correspond to a situation where every subset of $k$ virtual documents is the top-k for some virtual query.
* They try to find query and document *embedding vectors* that would have *good properties* with respect to this matrix. Yes, no actual queries or documents - just vectors and gradient descent! That's why it's called "*free* embeddings".
* They make this experiment with gradually increasing numbers of documents $n$ (and fixed $k$) until the optimization
is no longer able to solve the problem (i.e. achieve 100% accuracy). They call this moment the **critical-n point**.

Here you can see the evolution of critical-n points:

![]({{ site.baseurl }}/assets/images/paperwatch-08-09-2025/sign-rank-0.png){: .responsive-image style="--img-desktop:70%; --img-mobile:100%;"}

They seem to form a nice cubic pattern, and they interpolate it to get:

| critical-n | D    |
| ----------- | ---- |
| 500k        | 512  |
| 1.7m        | 768  |
| 4m          | 1024 |
| 107m        | 3072 |
| 250m        | 4096 |


The numbers may seem high, but actually they are reasonable for enterprise applications. Also, don't forget that they emerge from the "free embedding" problem, and in reality the numbers will be less optimistic.

Anyway, this shows that, with a sufficiently large number of documents, fixed-dimensions embeddings can't even theoretically provide ideal RAG. To illustrate this, the authors create the **LIMITS** dataset. The idea is to create a set of documents and queries such that for each query it's perfectly clear which documents are relevant and which are not. Here's an example of a document:

* *"Julia Matsil likes Feta Cheese, Sewing, Cymbals, Sichuan Peppercorns, Wrestling, Volleyball, Necklaces, Rafflesia Flowers, Nectarines, Scissors, Post-War Boom, the New York Yankees, Workbenches, Buckwheat, Astronomy, Surrealism, Cantaloupes, Kickboxing, Black Tea, Desks, Patience, Oil Painting, Holland Lops, Mousse, Ventriloquism, Sudoku Puzzles, Wonder, Screwdrivers, Iced Tea, Catfish, Robins, Rose Quartz, Parfaits, Wisdom, Goblin Sharks, The Sound of Crickets Chirping, East Asian Calligraphy, The Space Age, Potato Bread, Wabi-sabi, Irises, Sprinkles, Red Bell Peppers, Towels and Intuition."*

A query would be like "Who likes Feta Cheese", so theoretically relevant documents would be exactly those mentioning Feta Cheese. I'm not sure vector search is carved to work this way - finding mentioning of particular items. And, quite expectedly, good embedders fail on this dataset even for meagre $N = 46$ documents:

![]({{ site.baseurl }}/assets/images/paperwatch-08-09-2025/sign-rank-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Even fine tuning doesn't help. (Again, I would expect that.)

One of the things which make this dataset stand out is its graph density (too many queries for which each document is relevant):

![]({{ site.baseurl }}/assets/images/paperwatch-08-09-2025/sign-rank-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

This might be interpreted as the dataset's forte (tough benchmark for your RAG system) or as its weakness (it's not very realistic). I find myself in the latter camp. But however much I doubt the authors' methodology, I cannot agree more that RAG with vector search has its own restrictions, so:

* Use it with care; don't underestimate other search strategies
* Use two-step RAG patterns with re-ranking

# Why Language Models Hallucinate

[https://www.arxiv.org/pdf/2509.04664](https://www.arxiv.org/pdf/2509.04664)

This paper by OpenAI has a very intriguing name, so let's check what they have found.

The authors consider a type of hallucinations knows as **extrinsic hallucinations** or also **epistemic hallucinations** - failures at factuality. (Another type is **in-context** or **aleatoric hallucinations** - falures at relevance to the context provided in a prompt.) 

The authors consider facts as classes in certain multi-class classification problems - or, more accurately, multi-label classification problem, where several answers could potentially be correct. For example, the question "What was the name of king Charles I of England's son?" has three potentially correct answers - Charles, James, and, not so widely known, Henry. Naming a person's birthday is also a classification task. As you can imagine, both of these tasks involve choice from a very huge number of classes.

Some of such classification problems might be easier for LLMs, while some are harder:

![]({{ site.baseurl }}/assets/images/paperwatch-08-09-2025/why-hallucinate-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

I must say, the similar holds for humans. For example, guessing an answer to a mathematical problem may be considered as picking a needle in a set of integers or even real numbers - an seemingly impossible task - but the thing is that it shouldn't actually be *guessed*, but instead obtained through logic and reasoning. Naming a person's birthday (month + day) is a choice from mere 365 or 366 classes, but little can help us here. There may be some subtle hints like zodiac sign, but in the end it's almost random guessing. 

To be good at spitting out celebrities' birthdays, the LLM should just memorize them all, spending precious bits of memory for each of them. And this knowledge doesn't generalize. However many artists' birthdays the LLM "knows", the next one's can't be inferred from the existing knowledge. On the other hand, a somewhat controllable amount of memory spent on learning math rules, can potentially allow to solve infinitely many math problems. Moreover, understanding of math generalizes - modulo new definitions and theorems, some basic algebraic apparatus is exploited in calculus and probability theory alike.

Another huge problem with LLMs and classification tasks comes from our ways of evaluating them. Many popular benchmarks are classification tasks and *almost none of them suggest the "I don't know answer"*. Thus, giving a random answer is a better strategy than acknowledging ignorance. The latter indeed isn't among LLMs' strengths.

---

The authors  show that errors at generation are connected to classification errors through the following inequality:

$$\text{err} \leqslant 2\cdot\text{err}_{\text{iiv}} - \frac{\max_c|\mathcal{V_c}|}{|\min_c\mathcal{E_c}|} - \delta,$$

where

\begin{itemize}
\item It is considered that, given each context **c**, there are several plausible answers to a classification problem (for example, potential English names); $\mathcal{V_c}$ is the set of correct answers (the names of actual sons of Charles I of England) and $\mathcal{E_c}$ is the set of incorrect answers (all other names).

\item **err** is something like the probability of the LLM mentioning an incorrect answer in its completions. For example, describing a nonexistent son Genghis of Charles I of England when prompted to tell about the English Civil War.
\item $\text{err}_{\text{iiv}}$ is something like the probability of the LLM giving one of the wrong answers from $\mathcal{E}$ when prompted to work as a classifier. Just don't forget that there might be several valid options (Charles I of England had three sons). 
\item $\delta$ is measuring how well the model is *calibrated*, that is how much the model's estimation of probability is aligned with ground-truth accuracy. 

  I'll very briefly comment on that. The classifier itself works as follows - the model predicts a probability $\widehat{p}(r|c)$ for every class $r$ given the context $c$, and then those classes which get over a certain threshold

  $$\widehat{p}(r|c) > t$$

  are proclaimed as valid. (The authors choose some special $t$.) Now, $\delta$ is

  $$\delta = \left|\widehat{p}\left(\left\{(r, c)\mid \widehat{p}(r|c) > t\right\}\right) - p\left(\left\{(r, c)\mid \widehat{p}(r|c) > t\right\}\right)\right|$$

  It measures the alignment between model's "perception" of probability and its ground truth counterpart. It's important because if the model misjudges probabilities, classification with a threshold will give skewed results.
 \end{itemize}

Now I should confess: this explanation is clumsy at the very least. Both **err** and $\text{err}_{\text{iiv}}$ begin to make sense under a certain mathematical framework developed by the authors - but this framework is too abstract and somewhat deliberate, so I just don't see any sense to test your patience by describing it in detail. This is, I'm afraid a flaw shared by many attempts at building theoretical foundations of Deep Learning :( However, I hope, the basic idea is now clear.

There are also estimates connected to *singleton rate*. A *singleton* is a prompt (or rather a short part of it) which appears only once in the dataset, and the singleton rate is the ratio of singletons in the whole dataset. Then:

![]({{ site.baseurl }}/assets/images/paperwatch-08-09-2025/why-hallucinate-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

