---
layout: post
title: "Paperwatch 29.09.2025"
categories: blog
permalink: /paperwatch-29-09-2025/
---

**Paperwatch 19.09.2025 by Stanislav Fedotov (Nebius Academy)**

# LLMs at the ICPC finals 

ICPC stands for the **International Collegiate Programming Contest**. University teams compete there, solving tough algorithmic problems. Its world finals is a grand event whose winners are highly sought after programmers.

After Google's and OpenAI's gold medals at the International Mathematical Olympiad, ICPC was definitely next competition to fall to LLMs. And indeed:

* [Gemini achieves gold-medal level](https://deepmind.google/discover/blog/gemini-achieves-gold-level-performance-at-the-international-collegiate-programming-contest-world-finals/) and informal 2nd place, solving 10 out of 12 problems under the same time constraints as the student teams had. Moreover, it solved 8 problems within just 45 minutes.

  It's especially interesting that it solved Problem C, which was solved by no university team.

  You can check the solutions [here](https://github.com/google-deepmind/gemini_icpc2025).
  
* [Models by OpenAI solved 12 out of 12 problems](https://x.com/MostafaRohani/status/1968360976379703569). GPT-5 answered 11 problems correctly, and the last (and most difficult problem) was solved by the experimental reasoning model.

I'm really impressed with this acievement. However, we need to understand that ICPC problems usually have moderately long solutions, and for LLMs solving them should be much easier than working with huge, production-grade repositories.

# FlowRL: Matching Reward Distributions for LLM Reasoning

[https://arxiv.org/pdf/2509.15207](https://arxiv.org/pdf/2509.15207)

RL is a cool device of LLM training, producing great long-reasoning models and helping to align them with human values. But there's an annoying downside to it - a potential of mode collapsing. That is, out of many high-reward regions of text distribution, the LLM collapses during RL training to generating only a handful of peaks. The picture below illustrates this nicely:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/gflow-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

It is the mode collapsing phenomenon that I blame for the annoying GPT style, by the way.

So, finding a way of avoiding this pitfall is a noble quest. The authors took inspiration from GFlowNets (which I won't introduce here; but feel free to check [this paper](https://arxiv.org/pdf/2209.12782), for example) and reused some of the PPO/GRPO best practices, obtaining a nice approach to training long reasoners.

They start from a pretty straightforward idea of minimizing the KL divergence between the trained policy $\pi_{\theta}(y|x)$ and a distribution derived from the reward $r(x,y)$. The latter is usually defined as

$$\widehat{q}_{r}(y|x) = \frac{\exp(\beta r(x, y))}{Z(x)},$$

where $Z(x)$ is a normalizing coefficient needed to make $\widehat{q}_{r}$ an actual distribution (so that it sum to $1$ for fixed $x$). The problem is, however, that we don't know $Z(x)$. Mathematically it's

$$Z(x) = \sum_y\exp(\beta r(x, y)),$$

which is impossible to compute. But... why don't we learn it? So, it becomes $Z_{\phi}(x)$. And the training objective becomes

$$\mathbb{D}_{\text{KL}}\left(\pi_{\theta}(y|x)\left|\left| \frac{\exp(\beta r(x, y))}{Z_{\phi}(x)} \right.\right.\right) \longrightarrow \min_{\theta,\phi}$$

which is equivalent to

$$\log{Z_{\phi}(x)} + \log{\pi_{\theta}(y|x)} - \beta r(x, y) \longrightarrow \min_{\theta,\phi}$$

At this point, however, the authors run into the same problems RL trainers faced:

* The algorithm is totally on-policy, which makes it very inefficient. Insteady, you want to generate trajectories in batches and train on them. Say hi $\pi_{\text{old}}$ from PPO and GRPO, which is exactly the "old" policy that was used to generate trajectories in the current training batch.
* If you want to train a long reasoner model, you have to deal with long trajectories, and your gradients will likely explode.

No wonder the authors end up introducing all the perks of GRPO into the training:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/gflow-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

As you can see, we have here:

* GRPO-style advantage with clipping and importance sampling weights
* Normalizing by length (the $\frac1{\vert y \vert}$ multiples)
* Regularization with respect to the reference (pre-RL) policy

They train Qwen-2.5-7B/32B for math tasks and DeepSeek-R1-Distill-Qwen-7B for code tasks, respectively. borrowing math training dataset from the [DAPO paper](https://arxiv.org/pdf/2503.14476) and code training dataset from the [DeepCoder paper](https://arxiv.org/pdf/2507.12507). The results are quite nice, though the 8k tokens restriction makes me a bit skeptical.

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/gflow-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/gflow-3.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

As for the solution diversity (from which we actually started), the authors gave it some scoring with LLM as a judge:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/gflow-4.png){: .responsive-image style="--img-desktop:60%; --img-mobile:80%;"}

# K2-Think: A Parameter-Efficient Reasoning System

[https://arxiv.org/pdf/2509.07604](https://arxiv.org/pdf/2509.07604)

When I think about LLM capabilities, I see "memory" and reasoning as two separate things. For an LLM to be knowledgeable - converse about research-level math, for example - it should be quite large, because knowledge should be "stored" somewhere - in the LLM's parameters. Reasoning, strictly speaking, doesn't have such requirements. So, I'd expect that a small but "clever" LLM might exist - clever not in a sense that it can prove a new theorem, but in a sense that it can be proficient in reasoning about, say, high-school math. Of course, we have **Qwen3** models - all reasoners, and some very small - but they are not too powerful.

Let's see if the researchers from MBZUAI succeeded in it. Judging by the picture below, they did quite a good job :)

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/k2-0.gif){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

They started with **Qwen2.5-32B** which is not a very large LLM, and they trained it in several phases:

**Phase 1. SFT**. They used the existing **AM-Thinking-v1-Distilled** dataset, composed of CoT reasoning traces
and instruction/response pairs, with prompts drawn from tasks spanning mathematical reasoning, code
generation, scientific reasoning, instruction following, and general chat. The result is called **K2-Think-SFT**.

SFT seems to be crucial; without it, the subsequent RL training is much less effective.

**Phase 2: RL with Verifiable Rewards**. Here, they use the [GURU dataset](https://arxiv.org/pdf/2506.14965), which contains tasks from three domains:

* Math with the usual answer correctness verification
* Code with execution verification
* Science, where the answers are supposed to be scored by another model (which makes the reward slightly less credible...)

This produces the **K2-THINK** model. The context length at this point is 32k. (Not much, but probably ok for high-school tasks.)

**Phase 3 (which is not a training phase): Test-time Improvement**. It suggests the following test-time behavour:

* First, **Plan-Before-You-Think**: the planner agent is asked to extract key concepts from the query, and create a high-level plan from them. By the way, despite resulting in longer prompts this stage actually reduces the final solution length.
* Then, **K2-THINK** does its resoning job, generating $N$ independent outputs for a given prompt (in the experiments $N = 3$)
* Finally, an independent LLM chooses the best solution.

**Deployment**. **K2-THINK** is already quite small and efficient (especially compared the the huge and profusely thinking DeepSeek-R1), but it can be made even better with the right deployment.

The authors deploy K2-Think on Cerebras Wafer-Scale Engine (WSE) systems, leveraging the worldâ€™s largest
processor and speculative decoding. The WSE delivers approximately 2,000 tokens per second, representing a 10 times improvement over the nominal 200 tokens per second observed on typical deployment environments on a regular cloud provider. 

On the WSE, 32,000 token generation (which is typical for this model) is completed in just 16 seconds.

Here are qualitative results:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/k2-1.gif){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

The model seems to be able to surpass DeepSeek-V3.1 on some of the benchmarks despite being like 20x smaller. This sounds nice; just don't forget that **K2-THINK** in the table isn't just an LLM, but a model + some inference-time optimization (planner + best-of-3). So, its good to check what these optimizations bring to the table:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/k2-2.gif){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

I'm actually surprised that the authors stopped at just trying $N = 3$. They could use quite a large $N$ or an even more fancy inference strategy and still be way faster and way more memory-efficient than **DeepSeek-V3.1**. 

Anyway, this seems to show that even a small model, if trained well and given some additional inference-time optimization, can rival some of the reasoning giants. I wonder what if we could somehow combine a huge, knowledgeable expert with a small but powerful reasoner. 

The key advantage of a hybrid system might be that fine-tuning a, say, 32B model is far more feasible than a 600B one. We could subject the smaller model to more extensive SFT and RL, potentially making it a superior pure reasoner. Meanwhile, the larger companion model could serve as a vast knowledge base, providing context and factual grounding without requiring the same intensive alignment.

# Towards General Agentic Intelligence via Environment Scaling

[https://arxiv.org/pdf/2509.13311](https://arxiv.org/pdf/2509.13311)

AI Agents are a great thing now, and significant efforts are made to fine tune LLMs to be better at agentic tasks. Usually, such training is done with RL, not SFT. Partially, because LLM Agents are also *agents* in the RL terminology. Partially, because it's just very difficult to gather a training dataset for SFT. Indeed, for that you would need to get somewhere many ground-truth agentic trajectories for diverse tasks. Acknowledging this problem, the authors attempt to build a factory for autonomic generation of agentic trajectories.


# Towards a Physics Foundational Model

Today's video generation models are often criticized for "not understanding physics" - as much as LLMs are sometimes ridiculed for failures at elementary logic or spacial understanding. While I'd hope to see understanding of physics appear as an emergent capability, it's interesting to see a model trained specifically for dealing with physical simulations.

A physical model, of course, works with physical data. The authors deal with 2D physics and they data are trajectories consisting of 4-tensors (time, hight, width, field), where *field* is actually a broadly-understood physical value such as: pressure, density, temperature, x-axis velocity, and y-axis velocity. That is, an (t, h, w, f)-coordinate of such a tensor tells us what is the value of the f-th physical value at the point in time t and in the spacial position (h, w).

To avoid confusion: "field" here stands for "scalar of vertor field", which is just a mapping: point in space $\mapsto$ scalar or vector value. It is NOT potential of a force field (like I assumed initially...).

The basic problem the authors want to solve is predicting the fields at the next moment in time, given, say, they values (in all points in space) at the 4 previous ticks in time (or a longer trajectory). This might remind you of LLMs or video generation models, I suppose.

The authors solve this problem with a combination of two instruments:

* A **spaciotemporal transformer**, which, given a 4-tensor $X$, predicts its time derivative $\frac{\partial X}{\partial t}$.

  Here, "spaciotemporal" means that attention spans all the coordinates (time, height, width), while the "field" coordinate plays the role of "channels".
  
* A **numerical integrator** which, given $X$ and $\frac{\partial X}{\partial t}$, predicts the slice $X[t_{i+1}, :, : ,:]$ at the next time tick. Note that $X$ itself is a trajectory

  $$X[t_{0}, :, : ,:],\ X[t_{1}, :, : ,:], \ldots, X[t_{i}, :, : ,:]$$

  The numberical integrator has nothing to do with ML. The authors actually used the very simplistic [forward Euler method](https://en.wikipedia.org/wiki/Euler_method) - shifting $X[t_{i}, :, : ,:]$ alongside $\eta\frac{\partial X}{\partial t}(t_i)$ for some scalar $\eta$.

They call the whole system **GPhyT** (**GPT** + **Phy**sics).

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/physics-0.gif){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Now, having devised this model, the authors burden themselves with the followin research questions:

**Q1**: Can a single, large-scale transformer effectively model a wide range of disparate physical systems (e.g., incompressible flow, shock waves, convection) without any explicit, physics-describing features? 

**Q2**: Can this foundation model perform zero-shot generalization to new, unseen physical conditions (e.g., new boundary conditions, entirely new physics) by inferring the dynamics from the input alone? (This would be super nice to have!)

**Q3**: Can **GPhyT** maintain physical consistency and stability during extended autoregressive rollouts, a characteristic crucial for real-world application?

Let's see what kind of answers they obtained.

## Q1: can GPhyT work?

The authors trained their model on a union of several quite diverse simulation datasets. All of them are, however, about the flow of something - liquids or gases - under different forces and boundary conditions.

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/physics-1.gif){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

The training data points were 5-tuples: 4 input time ticks + 1 target time tick. They took tuples with different time increments to help the model to grasp different discretizations of time.

The final model is quite good and surpasses previous models. Please note that the vertical scale is logarithmic.

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/physics-2.gif){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

## Q2: is zero-shot generalization possible?

The authors tested their models on both new (and more challenging) variations of the existing tasks and on the totally new tasks, and GPhyT does a surprisingly good job:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/physics-3.gif){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Here's how the predictions look in practice. (GT is the ground truth.)

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/physics-4.gif){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

## Q3: is the model capable of long-horizon predictions

It's probably cool to predict the next-moment physics, but in reality we're interested in long-term prediction. So, the authors tested their model on a 50-timestep autoregressive prediction:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/physics-5.gif){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Here, the picture is slightly less favourable. For the task types that the model has seen during training, the results are more or less ok, but for out-of-distribution examples (picture on the right), error builds up quite quickly.

Overall, it's a nice even if imperfect attempt. It would actually be interesting to see a cross between a physics foundational model and a video generator. Of course, it's very hard to find data fit for both kinds of models, but what if?..
 
# Discovery of Unstable Singularities

[Paper](https://arxiv.org/pdf/2509.14185)

[Post](https://deepmind.google/discover/blog/discovering-new-solutions-to-century-old-problems-in-fluid-dynamics/)

The Navierâ€“Stokes equations are partial differential equations which describe the motion of viscous fluid substances (which is more or less all real fluids or air). For example, the flow of water in your water pipes is described by them.

The theory around these equations is quite involved. It's enough to note that smooth solutions are known only for come particular cases (that is, particular boundary conditions). In general, the existence of smooth solutions is unknown; moreover, this is one of the [Clay Millennium problems](https://en.wikipedia.org/wiki/Millennium_Prize_Problems#Birch_and_Swinnerton-Dyer_conjecture).

Finding special-case solutions of the Navier-Stokes is also not a simple task. So, the authors actually consider several two-dimensional simplifications of the original three-dimensional equations. Moreover, they don't study smooth solutions. Instead, they take a look at **singularities**.

To understand what singularity is, let's consider a frictionless fluid and an ideal duo of 2d counter-rotating vortices that collide at the time $t = 1$. Their radii shrink, angular speed of liquid inside them grows - and at $t = 1$ we have infinite velocity gradient, infinite pressure - and geometrical collapse, because both vortices cease to exist. Mathematically, the solution becomes singular.

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/navier-stokes-1.gif){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

This is probably not the clearest visualization, but you can see here that speeds (black arrows) grow as the vertices converge. At the point of collision they'll be infinite.

However, singularities may be of a different kind. A **stable** singularity is, well, stable with respect of changing the initial conditions. A possible example is separation of a water droplet from your leaking tap. At the very moment of separation, the curvature of the water's surface becomes infinite at the pinch-off point, so it's a singularity. If you change the initial conditions a little bit (for example, if you give the tap an infinitesimal push), the droplet will still fall; the singularity will be here to stay.

An **unstable** singularity is only generated by a very lucky set of initial conditions. Change them in the slightest - and singularity vanishes. The vortex pair collision is one of such examples. If their centers miss each other, no singularity takes place.

---

The authors have indeed discovered unstable singularities for some of the 2d simplifications of Navier--Stokes, so let's discuss the sources of their success. There were several:

**1. The right math formulation of the task**. The authors leverage the mathematical framework known as **self-similar ansatz**. It suggests modelling only spacial situation with the function $\Phi_{\theta}(y)$, while the evolution in time is described by simple contraction, with singularity occuring at $t = 1$ and $x = 0$:

$$
\phi(x,t) = (1-t)^{k(\lambda)} \Phi_{\theta}\left( \frac{x}{(1-t)^{1+\lambda}} \right)
$$

In the schematic below you can see an example of such $\Phi_{\theta}(y)$. More accurately, the 3d plot shows *vorticity* at different points of a 2d plane. The two peaks correspond to two counter-rotating vortices:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/navier-stokes-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

The formula $\phi(x, t)$ describes the contraction of this picture as $t$ tends to $1$.

Moreover, the authors further decompose $\Phi_{\theta}(y)$, implanting some further math properties into it:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/navier-stokes-3.png){: .responsive-image style="--img-desktop:60%; --img-mobile:80%;"}

**2. A Physics-Informed Neural Network (PINN)**. Basically, it's an MLP, but with additional non-trainable transformations that map MLP's final representations to fields and derivatives that can be plugged into the partial differential equation to obtain the residuals $\mathcal{R}_k$ ($k$ enumerate different equations in the system of PDEs). The loss consists of several components:

* *Equation loss* with is, in turn, a combination of square norms of $\mathcal{R}_k$, their first and second partial derivatives.
* *Data loss* which helps to avoid all-zero solutions. It is of the form
  
  $$\mathcal{L}_{\text{data}} = \widehat{c}_{\text{norm}}\left(
\Phi(y_{\text{norm}}) - C_{\text{norm}} 
\right)^2 + \widehat{c}_{\infty}\sum_{y_{\infty}\in Y_{\infty}}\Phi(y_{\infty})^2$$
  
  for a point $y_{\text{norm}}$ chosen from a high-gradient region and an assortment of points $Y_{\infty}$ which are far from the origin. Thus, we control the value at $y_{\text{norm}}$ and demand that $\Phi$ vanish far from the origin.

The model is trained on samples of coordinates $y$, where more points are taken from more crucial, high-gradient regions.

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/navier-stokes-4.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

After training their MLP for some time, the authors froze it and trained a Fourier feature network on the residuals to rule out high-frequency components. (This is the second training stage.)

**3. The Gauss-Newton optimization method** which is an approximate (no full Hessian needed) second-order method. This is important indeed. Since the authors search for *unstable* singularities they need to be extra careful about numeric precision.

The parameter $\lambda$ turns out to be quite important. Solutions only exist at a discrete set of values of $\lambda$. The one with the largest $\lambda$ - which means, fastest-converging - is stable, while the smaller ones give solutions with higher order of instability, meaning their formation is vulnerable to a greater number of distinct perturbation patterns that can knock the system off its singular trajectory.

In the end, the authors don't invent new ML architectures, but they chose well where to plug in the existing ones, and this is very exciting.

# Reinforced Generation of Combinatorial Structures: Applications to Complexity Theory

[https://arxiv.org/pdf/2509.18057](https://arxiv.org/pdf/2509.18057)

Another math-related paper from Google - an interesting case of applying [AlphaEvolve](https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/) to math problems.

This paper deals with the **Maximum cut** (**MAX-CUT**) problem. Given an undirected graph, you need to particion its vertices into two disjoint sets $S$ and $T$, maximizing the number of edges connecting vertices from $S$ and vertices from $T$. You can imagine it as splitting the graph into two disjoint parts, cutting as many arrows as possible in the process: 

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/navier-stokes-4.png){: .responsive-image style="--img-desktop:60%; --img-mobile:80%;"}

[Source](https://en.wikipedia.org/wiki/Maximum_cut)

## Chapter 1: finding a d-regular Ramanujan graph with a large max cut**

Finding the maximal cut (with the most arrows) is an NP-hard problem. Another potentially difficult problem is finding an algorithm possible of cerifying that `MAX-CUT(G) â‰¤ Ïƒ * |E|` for a given graph $G$ and a given constant $\sigma\in(0;1)$. (The algorithm should be able to work with any graph $G$.) Here, $\vert E\vert$ is the number of edges in the graph. Here, difficulty depends on $\sigma$. For $\sigma = 1$, the task is trivial - any cut will cut not more than all the edges. For smaller values of $\sigma$, there might exist no polynomial algorithm.

It's interesting to find the *lower bound* $\sigma^{MC}$ (MC stands for MAX-CUT), which is the infimum value of $\sigma$ for which a polynoimial cerification algorithm might exist. A higher $\sigma^{MC}$ would imply that the problem is "harder".

The authors study not just any graphs though. They deal with *d-regular graphs* - graphs where every vertext has exactly $d$ neighbours - and try to establish $\sigma_d^{MC}$ for them. More accurately, they consider $d = 3$ and $d = 4$. 

Now, "There is no polynomial algorithm" is a tough claim; so, finding $\sigma_d^{MC}$ is difficult. Instead, researchers search for its lower bounds ($\sigma_d^{MC}\geqslant$ smth), increasing them bit by bit. Moreover, it's a path paved with conjectured **hard instances**. In this case, [Ramanujan graphs](https://en.wikipedia.org/wiki/Ramanujan_graph) are believed to be hard instances. Let me explain what it means.

A polynomial algorithm can't answer a question `MAX-CUT(G) â‰¤ Ïƒ * |E|` by just checking all possible combinations of `Ïƒ * |E|` edges. So, it should rely on some kind of internal graph structure. Ramanujan graphs are close to random - and, among d-regular graphs, they are believed to be the less structured. So, mathematician conjectured that:

* If there is a d-regular Ramanujan graph with `MAX-CUT(G) > Ïƒ * |E|`, then $\sigma_d^{MC}\geqslant\sigma$. (An important note: there might be Ramanujan graphs with even larger maximal cuts, so we have $\geqslant$ here)

Intuitively, this boils down to the belief that this maximal cut in a Ramanujan graph can't be algorithmically established in a polynomial time. Again, this is a conjecture. But it might help to find $\sigma_d^{MC}$, whose value would lately be justified rigourously.

Now, we are ready to discuss the first contribution of AI to this paper.


The authors used **AlphaEvolve** to evolve a program generating a pair: `(Graph, Cut)`. And it eventually gives them Ramanujan graphs with large cuts: 

* For $d = 3$, they arrive at the same lower bound for $Ïƒ_3^{\text{MC}}$ that was known before, which is $0.944$.
* However, for $d = 4$, they are able to raise the lower bound from $0.875$ to $0.911$.

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/max-cut-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

## Chapter 2: Improving the hardness of approximation for general MAX-k-CUT

**MAX-k-CUT** is an advanced version of the MAX-CUT problem, where the graph is partitioned into $k$ disjoint parts, with the same objective of maximizing the number of edges connecting different parts. It's also NP-hard. However, sometimes you need to practically solve even NP-hard tasks. Your manager won't give you promotion for proving that a task is NP-hard, will they? 

For that, approximate algorithms exist. For MAX-CUT, there is Goemans-Williamson algorithm, which is guaranteed to find a cut that is at least approximately 0.878 times the size of the optimal MAX-CUT. (Your manager will probably be happy with that.) However, above some $\theta$, the task of finding $\geqslant\theta$ times the size of the optimal MAX-CUT becomes NP-hard too. And finding such $\theta$ is also an interesting direction for math research.

Proving that approximation of MAX-k-CUT is hard involves connecting the problem with `3LIN(k)`, which is a task of solving a system of equations

$$x_i + x_j + x_t = i\ (mod\,k)$$

For 3LIN(k), there is the HÃ¥stad's theorem stating that

* For any $k \geqslant 2$ and any $\varepsilon > 0$, it is NP-hard to distinguish between two cases for an instance of 3LIN(k) with $m$ constraints:

  * (Completeness Case): There exists an assignment of variables that satisfies at least $(1 - \varepsilon)$ fraction of the $m$ constraints.
  * (Soundness Case): No assignment of variables satisfies more than $\left(\frac1k + Îµ\right)$ fraction of the $m$ constraints.

There is a technique that performs reduction from 3LIN(k) to MAX-k-CUT, 

* If the constructed MAX-k-CUT graph has a very high cut, you can confidently conclude the original 3LIN(k) instance must have been a Completeness Case instance.
* If the constructed MAX-k-CUT graph has a very low cut, you can confidently conclude the original 3LIN(k) instance must have been a Soundness Case instance.

So, if we can approximate MAX-k-CUT with a polynomial algorithm, we can roll the result back to approximate 3LIN(k). The only problem is that you need to actually construct the MAX-k-CUT graph. And this is done with **gadgets** - small weighted graphs, one for each $=i$ (right hand side of the equation).

More accurately, an **$i$-gadget** (for $=i$) is a *weighted* graph with vertices particioned into:

1. **primary** $p_x,p_y,p_z$ (correspond to the three summands $x_i$, $x_j$, $x_t$ in the left hand side);
2. $k$ shared **global** vertices;
3. **auxiliary** vertices.

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/max-cut-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Now, let's take a system of equations. For each equation

$$x_i + x_j + x_t = i\ (mod\,k)$$

we take the corresponding $=i$ gadget and color its global variables - 1st in the 1st color, ..., k-th in the k-th color. We also color primary variables according to the values of $x = x_i$, $y = x_j$ and $z = x_t$ (since we consider everthing modulo $k$, these will also assume some of the $k$ colors). Now, let's glue all the gadgets along with their global vertices and also along the primary vertices that correspond to the same $x_i$ - these vertices are colored now. The question is - how can we color the auxiliary vertices to establish the maximal cut? ($k$ areas of each colour = $k$ parts into which we break down the graph!)

Now, it turns out that if you can approximately solve the MAX-k-CUT task, you can also approximately discern between the completeness and soundness cases. In particular, the authors prove that

* **Theorem 4.3.** For any $Îµ > 0$, there is a gadget reduction from $(1 âˆ’ O(Îµ), 1/3 + O(Îµ))$-approximating 3LIN(3) to $(57/62 âˆ’ Îµ, 55/62 + Îµ)$-approximating MAX-3-CUT. As a consequence, it is NP-hard to $(55/57 + Îµ)$-approximate MAX-3-CUT. 

* **Theorem 4.4**. For any $Îµ > 0$, there is a gadget reduction from $(1 âˆ’ O(Îµ), 1/4 + O(Îµ))$-approximating 3LIN(4) to $(0.9349 âˆ’ Îµ, 0.9238 + Îµ)$-approximating MAX-4-CUT. As a consequence, it is NP-hard to $(0.987 + Îµ)$-approximate MAX-4-CUT.

But you probably wonder how they used AI here. In two ways - for finding gadgets (with AlphaEvolve) and for evaluating their quality. For each gadget $I_i$, they define

$$c(I_i)=\min_{\text{triple satisfying }x + y + z = i\ (mod\,k)\text{; fixed coloring of globals}} \max_{\text{colorings of aux vertices}} \text{Cut}$$

$$s(I_i)=\max_{\text{violating triples, mutable coloring of globals}} \max_{\text{colorings of aux vertices}} \text{Cut}$$

$$c'(I_i)=\min_{\text{satisfying triples; mutable coloring of globals}} \max_{\text{colorings of aux vertices}} \text{Cut}$$

(Mutable coloring of globals means, to the best of my understanding, that we can permute their colours.)

I would now love to say that the authors evolve the gadgets to maximise $c(I_i) - s(I_i)$, but unfortunately it's a little bit more complicated. There are two bounds:

* **YES bound (lower bound on optimum):** if the source 3LIN($k$) instance is a PCP-YES (â‰ˆ all clauses satisfiable), then the glued MAX-$k$-CUT instance has optimum

  $$\text{OPT}_{\text{YES}}\ \ge\ m\big((1-\varepsilon)\,\bar c+\varepsilon\,\bar s\big)$$

* **NO bound (upper bound on optimum):** if itâ€™s a PCP-NO (â‰¤ $1/k+\varepsilon$ fraction satisfiable), then
  
  $$\mathrm{OPT}_{\text{NO}}\ \le\ m\big((1/k+\varepsilon)\,\bar c'\ +\ (1-1/k-\varepsilon)\,\bar s\big)$$

Here:

* $m$ = number of clauses in the source instance,
* $q_i$ = fraction of clauses using predicate $P_i(x{+}y{+}z\equiv i)$ ($\frac1k$ in the paper),
* $\bar c=\sum_i q_i\,c(I_i)$, $\bar c'=\sum_i q_i\,c'(I_i)$, $\bar s=\sum_i q_i\,s(I_i)$,
* and $c(I_i),c'(I_i),s(I_i)$ are the per-gadget completeness/soundness values (true-clause â‡’ at least $c$, false-clause â‡’ at most $s$; $c'$ is the "completeness with globals free"). 

From these two bounds they get a **threshold (hardness) ratio**, which is actually optimized:

$$
R\ :=\ \frac{\text{OPT}_{\text{NO}}}{\text{OPT}_{\text{YES}}}
\ \le\ \frac{(1/k+\varepsilon)\,\bar c'+(1-1/k-\varepsilon)\,\bar s}{(1-\varepsilon)\,\bar c+\varepsilon\,\bar s}.
$$

This $R$ is what is optimized by AlphaEvolve.

Another good thing AI did here is, as I've already mentioned, speeding up computation of $c(I_i)$ and $s(I_i)$. It's still exponential, but has a more efficient implementation. And this was also obtained with AlphaEvolve. I'd only add here that they had to make extra efforts to ensure that the evolved scorer doesn't cheat - this involved checking on a labeled dataset and using an external LLM judge.
