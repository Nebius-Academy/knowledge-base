---
layout: post
title: "Paperwatch 29.09.2025"
categories: blog
permalink: /paperwatch-29-09-2025/
---

**Paperwatch 19.09.2025 by Stanislav Fedotov (Nebius Academy)**

# LLMs at the ICPC finals 

ICPC stands for the **International Collegiate Programming Contest**. University teams compete there, solving tough algorithmic problems. Its world finals is a grand event whose winners are highly sought after programmers.

After Google's and OpenAI's gold medals at the International Mathematical Olympiad, ICPC was definitely next competition to fall to LLMs. And indeed:

* [Gemini achieves gold-medal level](https://deepmind.google/discover/blog/gemini-achieves-gold-level-performance-at-the-international-collegiate-programming-contest-world-finals/) and informal 2nd place, solving 10 out of 12 problems under the same time constraints as the student teams had. Moreover, it solved 8 problems within just 45 minutes.

  It's especially interesting that it solved Problem C, which was solved by no university team.

  You can check the solutions [here](https://github.com/google-deepmind/gemini_icpc2025).
  
* [Models by OpenAI solved 12 out of 12 problems](https://x.com/MostafaRohani/status/1968360976379703569). GPT-5 answered 11 problems correctly, and the last (and most difficult problem) was solved by the experimental reasoning model.

I'm really impressed with this acievement. However, we need to understand that ICPC problems usually have moderately long solutions, and for LLMs solving them should be much easier than working with huge, production-grade repositories.

# FlowRL: Matching Reward Distributions for LLM Reasoning

[https://arxiv.org/pdf/2509.15207](https://arxiv.org/pdf/2509.15207)

RL is a cool device of LLM training, producing great long-reasoning models and helping to align them with human values. But there's an annoying downside to it - a potential of mode collapsing. That is, out of many high-reward regions of text distribution, the LLM collapses during RL training to generating only a handful of peaks. The picture below illustrates this nicely:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/gflow-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

It is the mode collapsing phenomenon that I blame for the annoying GPT style, by the way.

So, finding a way of avoiding this pitfall is a noble quest. The authors took inspiration from GFlowNets (which I won't introduce here; but feel free to check [this paper](https://arxiv.org/pdf/2209.12782), for example) and reused some of the PPO/GRPO best practices, obtaining a nice approach to training long reasoners.

They start from a pretty straightforward idea of minimizing the KL divergence between the trained policy $\pi_{\theta}(y|x)$ and a distribution derived from the reward $r(x,y)$. The latter is usually defined as

$$\widehat{q}_{r}(y|x) = \frac{\exp(\beta r(x, y))}{Z(x)},$$

where $Z(x)$ is a normalizing coefficient needed to make $\widehat{q}_{r}$ an actual distribution (so that it sum to $1$ for fixed $x$). The problem is, however, that we don't know $Z(x)$. Mathematically it's

$$Z(x) = \sum_y\exp(\beta r(x, y)),$$

which is impossible to compute. But... why don't we learn it? So, it becomes $Z_{\phi}(x)$. And the training objective becomes

$$\mathbb{D}_{\text{KL}}\left(\pi_{\theta}(y|x)\left|\left| \frac{\exp(\beta r(x, y))}{Z_{\phi}(x)} \right.\right.\right) \longrightarrow \min_{\theta,\phi}$$

which is equivalent to

$$\log{Z_{\phi}(x)} + \log{\pi_{\theta}(y|x)} - \beta r(x, y) \longrightarrow \min_{\theta,\phi}$$

At this point, however, the authors run into the same problems RL trainers faced:

* The algorithm is totally on-policy, which makes it very inefficient. Insteady, you want to generate trajectories in batches and train on them. Say hi $\pi_{\text{old}}$ from PPO and GRPO, which is exactly the "old" policy that was used to generate trajectories in the current training batch.
* If you want to train a long reasoner model, you have to deal with long trajectories, and your gradients will likely explode.

No wonder the authors end up introducing all the perks of GRPO into the training:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/gflow-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

As you can see, we have here:

* GRPO-style advantage with clipping and importance sampling weights
* Normalizing by length (the $\frac1{\vert y \vert}$ multiples)
* Regularization with respect to the reference (pre-RL) policy

They train Qwen-2.5-7B/32B for math tasks and DeepSeek-R1-Distill-Qwen-7B for code tasks, respectively. borrowing math training dataset from the [DAPO paper](https://arxiv.org/pdf/2503.14476) and code training dataset from the [DeepCoder paper](https://arxiv.org/pdf/2507.12507). The results are quite nice, though the 8k tokens restriction makes me a bit skeptical.

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/gflow-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/gflow-3.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

As for the solution diversity (from which we actually started), the authors gave it some scoring with LLM as a judge:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/gflow-4.png){: .responsive-image style="--img-desktop:60%; --img-mobile:80%;"}

# K2-Think: A Parameter-Efficient Reasoning System

[https://arxiv.org/pdf/2509.07604](https://arxiv.org/pdf/2509.07604)

When I think about LLM capabilities, I see "memory" and reasoning as two separate things. For an LLM to be knowledgeable - converse about research-level math, for example - it should be quite large, because knowledge should be "stored" somewhere - in the LLM's parameters. Reasoning, strictly speaking, doesn't have such requirements. So, I'd expect that a small but "clever" LLM might exist - clever not in a sense that it can prove a new theorem, but in a sense that it can be proficient in reasoning about, say, high-school math. Of course, we have **Qwen3** models - all reasoners, and some very small - but they are not too powerful.

Let's see if the researchers from MBZUAI succeeded in it. Judging by the picture below, they did quite a good job :)

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/k2-0.gif){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

They started with **Qwen2.5-32B** which is not a very large LLM, and they trained it in several phases:

**Phase 1. SFT**. They used the existing **AM-Thinking-v1-Distilled** dataset, composed of CoT reasoning traces
and instruction/response pairs, with prompts drawn from tasks spanning mathematical reasoning, code
generation, scientific reasoning, instruction following, and general chat. The result is called **K2-Think-SFT**.

SFT seems to be crucial; without it, the subsequent RL training is much less effective.

**Phase 2: RL with Verifiable Rewards**. Here, they use the [GURU dataset](https://arxiv.org/pdf/2506.14965), which contains tasks from three domains:

* Math with the usual answer correctness verification
* Code with execution verification
* Science, where the answers are supposed to be scored by another model (which makes the reward slightly less credible...)

This produces the **K2-THINK** model. The context length at this point is 32k. (Not much, but probably ok for high-school tasks.)

**Phase 3 (which is not a training phase): Test-time Improvement**. It suggests the following test-time behavour:

* First, **Plan-Before-You-Think**: the planner agent is asked to extract key concepts from the query, and create a high-level plan from them. By the way, despite resulting in longer prompts this stage actually reduces the final solution length.
* Then, **K2-THINK** does its resoning job, generating $N$ independent outputs for a given prompt (in the experiments $N = 3$)
* Finally, an independent LLM chooses the best solution.

**Deployment**. **K2-THINK** is already quite small and efficient (especially compared the the huge and profusely thinking DeepSeek-R1), but it can be made even better with the right deployment.

The authors deploy K2-Think on Cerebras Wafer-Scale Engine (WSE) systems, leveraging the world’s largest
processor and speculative decoding. The WSE delivers approximately 2,000 tokens per second, representing a 10 times improvement over the nominal 200 tokens per second observed on typical deployment environments on a regular cloud provider. 

On the WSE, 32,000 token generation (which is typical for this model) is completed in just 16 seconds.

Here are qualitative results:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/k2-1.gif){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

The model seems to be able to surpass DeepSeek-V3.1 on some of the benchmarks despite being like 20x smaller. This sounds nice; just don't forget that **K2-THINK** in the table isn't just an LLM, but a model + some inference-time optimization (planner + best-of-3). So, its good to check what these optimizations bring to the table:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/k2-2.gif){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

I'm actually surprised that the authors stopped at just trying $N = 3$. They could use quite a large $N$ or an even more fancy inference strategy and still be way faster and way more memory-efficient than **DeepSeek-V3.1**. 

Anyway, this seems to show that even a small model, if trained well and given some additional inference-time optimization, can rival some of the reasoning giants. I wonder what if we could somehow combine a huge, knowledgeable expert with a small but powerful reasoner. 

The key advantage of a hybrid system might be that fine-tuning a, say, 32B model is far more feasible than a 600B one. We could subject the smaller model to more extensive SFT and RL, potentially making it a superior pure reasoner. Meanwhile, the larger companion model could serve as a vast knowledge base, providing context and factual grounding without requiring the same intensive alignment.
 
# Discovery of Unstable Singularities

[Paper](https://arxiv.org/pdf/2509.14185)

[Post](https://deepmind.google/discover/blog/discovering-new-solutions-to-century-old-problems-in-fluid-dynamics/)

The Navier–Stokes equations are partial differential equations which describe the motion of viscous fluid substances (which is more or less all real fluids or air). For example, the flow of water in your water pipes is described by them.

The theory around these equations is quite involved. It's enough to note that smooth solutions are known only for come particular cases (that is, particular boundary conditions). In general, the existence of smooth solutions is unknown; moreover, this is one of the [Clay Millennium problems](https://en.wikipedia.org/wiki/Millennium_Prize_Problems#Birch_and_Swinnerton-Dyer_conjecture).

Finding special-case solutions of the Navier-Stokes is also not a simple task. So, the authors actually consider several two-dimensional simplifications of the original three-dimensional equations. Moreover, they don't study smooth solutions. Instead, they take a look at **singularities**.

To understand what singularity is, let's consider a frictionless fluid and an ideal duo of 2d counter-rotating vortices that collide at the time $t = 1$. Their radii shrink, angular speed of liquid inside them grows - and at $t = 1$ we have infinite velocity gradient, infinite pressure - and geometrical collapse, because both vortices cease to exist. Mathematically, the solution becomes singular.

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/navier-stokes-1.gif){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

This is probably not the clearest visualization, but you can see here that speeds (black arrows) grow as the vertices converge. At the point of collision they'll be infinite.

However, singularities may be of a different kind. A **stable** singularity is, well, stable with respect of changing the initial conditions. A possible example is separation of a water droplet from your leaking tap. At the very moment of separation, the curvature of the water's surface becomes infinite at the pinch-off point, so it's a singularity. If you change the initial conditions a little bit (for example, if you give the tap an infinitesimal push), the droplet will still fall; the singularity will be here to stay.

An **unstable** singularity is only generated by a very lucky set of initial conditions. Change them in the slightest - and singularity vanishes. The vortex pair collision is one of such examples. If their centers miss each other, no singularity takes place.

---

The authors have indeed discovered unstable singularities for some of the 2d simplifications of Navier--Stokes, so let's discuss the sources of their success. There were several:

**1. The right math formulation of the task**. The authors leverage the mathematical framework known as **self-similar ansatz**. It suggests modelling only spacial situation with the function $\Phi_{\theta}(y)$, while the evolution in time is described by simple contraction, with singularity occuring at $t = 1$ and $x = 0$:

$$
\phi(x,t) = (1-t)^{k(\lambda)} \Phi_{\theta}\left( \frac{x}{(1-t)^{1+\lambda}} \right)
$$

In the schematic below you can see an example of such $\Phi_{\theta}(y)$. More accurately, the 3d plot shows *vorticity* at different points of a 2d plane. The two peaks correspond to two counter-rotating vortices:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/navier-stokes-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

The formula $\phi(x, t)$ describes the contraction of this picture as $t$ tends to $1$.

Moreover, the authors further decompose $\Phi_{\theta}(y)$, implanting some further math properties into it:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/navier-stokes-3.png){: .responsive-image style="--img-desktop:60%; --img-mobile:80%;"}

**2. A Physics-Informed Neural Network (PINN)**. Basically, it's an MLP, but with additional non-trainable transformations that map MLP's final representations to fields and derivatives that can be plugged into the partial differential equation to obtain the residuals $\mathcal{R}_k$ ($k$ enumerate different equations in the system of PDEs). The loss consists of several components:

* *Equation loss* with is, in turn, a combination of square norms of $\mathcal{R}_k$, their first and second partial derivatives.
* *Data loss* which helps to avoid all-zero solutions. It is of the form
  
  $$\mathcal{L}_{\text{data}} = \widehat{c}_{\text{norm}}\left(
\Phi(y_{\text{norm}}) - C_{\text{norm}} 
\right)^2 + \widehat{c}_{\infty}\sum_{y_{\infty}\in Y_{\infty}}\Phi(y_{\infty})^2$$
  
  for a point $y_{\text{norm}}$ chosen from a high-gradient region and an assortment of points $Y_{\infty}$ which are far from the origin. Thus, we control the value at $y_{\text{norm}}$ and demand that $\Phi$ vanish far from the origin.

The model is trained on samples of coordinates $y$, where more points are taken from more crucial, high-gradient regions.

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/navier-stokes-4.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

After training their MLP for some time, the authors froze it and trained a Fourier feature network on the residuals to rule out high-frequency components. (This is the second training stage.)

**3. The Gauss-Newton optimization method** which is an approximate (no full Hessian needed) second-order method. This is important indeed. Since the authors search for *unstable* singularities they need to be extra careful about numeric precision.

The parameter $\lambda$ turns out to be quite important. Solutions only exist at a discrete set of values of $\lambda$. The one with the largest $\lambda$ - which means, fastest-converging - is stable, while the smaller ones give solutions with higher order of instability, meaning their formation is vulnerable to a greater number of distinct perturbation patterns that can knock the system off its singular trajectory.

In the end, the authors don't invent new ML architectures, but they chose well where to plug in the existing ones, and this is very exciting.

# Reinforced Generation of Combinatorial Structures: Applications to Complexity Theory

[https://arxiv.org/pdf/2509.18057](https://arxiv.org/pdf/2509.18057)

Another math-related paper from Google - an interesting case of applying [AlphaEvolve](https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/) to math problems.

This paper deals with the **Maximum cut** (**MAX-CUT**) problem. Given an undirected graph, you need to particion its vertices into two disjoint sets $S$ and $T$, maximizing the number of edges connecting vertices from $S$ and vertices from $T$. You can imagine it as splitting the graph into two disjoint parts, cutting as many arrows as possible in the process: 

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/navier-stokes-4.png){: .responsive-image style="--img-desktop:60%; --img-mobile:80%;"}

[Source](https://en.wikipedia.org/wiki/Maximum_cut)



---

### **Contribution 1: Pinpointing the MAX-CUT Value in Random Graphs**

#### **The Mathematical Goal**
The goal was to determine, with high precision, the maximum fraction of edges that can be cut in a typical large **random d-regular graph** (where every vertex has $d$ neighbors). They aimed to find the tightest possible interval $[\alpha, \beta]$ where the true value must lie.

#### **The Method**
This required a two-pronged attack:
1.  **To find the lower bound ($\alpha$):** They needed to conduct a search to find an *explicit example* of a d-regular Ramanujan graph that demonstrably has a very large cut.
2.  **To find the upper bound ($\beta$):** They needed a theoretical argument that would hold for *all* such graphs. They achieved this by creating and solving a Linear Program (LP) whose formulation was based on the spectral properties (eigenvalues) and local tree-like structure of these graphs.

#### **The AI's Role in This Contribution**
*   The AI's role was **limited to the search for the lower bound (Part 1)**.
*   AlphaEvolve was used to evolve a computer program that constructed candidate graphs. Crucially, to avoid the slow process of finding a graph's MAX-CUT, the program was evolved to generate a **pair: (`Graph`, `Cut`)**.
*   The scoring function was very fast because it only needed to verify the properties of this pair: Is the graph d-regular and Ramanujan? Is the provided cut large?
*   The AI was **not** involved in the theoretical work of designing and solving the Linear Program for the upper bound (Part 2).
*   The high-speed solver discussed previously was **not needed and not used** for this contribution.

---

### **Contribution 2: Improving the Hardness of Approximation for General MAX-k-CUT**

#### **The Mathematical Goal**
The goal was to prove a stronger result about the fundamental computational difficulty of MAX-k-CUT. Specifically, they wanted to show that it is NP-hard to even *approximate* the optimal MAX-k-CUT value beyond a certain percentage. This applies to **any graph**, not just random ones.

#### **The Method**
The technique used is a **gadget-based reduction**. This involves finding a special-purpose, small, weighted graph called a "gadget." The mathematical properties of this gadget directly determine the strength of the final inapproximability proof. The core of this work was a search for a new gadget with better properties than any previously known.

#### **The AI's Role in This Contribution**
The AI's role was two-fold and absolutely central to this contribution:

1.  **Role A (The Primary Search):** AlphaEvolve was used to search for the optimal gadget. Just as in the first contribution, it did this by evolving a program that generated candidate gadgets.

2.  **Role B (Building a Critical Tool):** This primary search faced a severe bottleneck. To evaluate any candidate gadget, one must calculate its precise MAX-k-CUT value, which is an exponentially slow (NP-hard) computation. This made searching for complex gadgets infeasible.
    *   To solve this, the authors gave AlphaEvolve a **second, separate task**: to take a simple, slow, brute-force MAX-k-CUT solver and evolve its code to make it as fast as possible while remaining 100% correct.
    *   The AI succeeded by re-implementing the algorithm using advanced techniques like branch-and-bound. This resulted in a new solver that was **10,000 times faster**.
    *   This ultra-fast solver, built by the AI, was then used as the evaluation function for the primary gadget search (Role A).

In essence, the AI first acted as an **expert algorithm designer** to build a necessary tool, and then used that tool to act as a **mathematical explorer** to find the new gadget. This second contribution would have been computationally intractable without both of these AI roles.

### **Summary Table**

| Aspect | Contribution 1: Random Graphs | Contribution 2: Hardness of Approximation |
| :--- | :--- | :--- |
| **Mathematical Goal** | Find the precise MAX-CUT value for a specific family of graphs. | Prove a stronger limit on the approximability of MAX-k-CUT for all graphs. |
| **Core Method**| Search for a graph-cut pair (lower bound) & a theoretical proof (upper bound).| Search for a "gadget" graph. |
| **Main AI Role**| Evolved a program to generate `(Graph, Cut)` pairs.| Evolved a program to generate gadget graphs. |
| **Secondary AI Role** | **None.** | **Crucial:** Evolved a slow solver into a 10,000x faster one, making the main search feasible. |

