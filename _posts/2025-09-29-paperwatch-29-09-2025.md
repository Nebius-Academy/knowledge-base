---
layout: post
title: "Paperwatch 29.09.2025"
categories: blog
permalink: /paperwatch-29-09-2025/
---

**Paperwatch 19.09.2025 by Stanislav Fedotov (Nebius Academy)**

# LLMs at the ICPC finals 

ICPC stands for the **International Collegiate Programming Contest**. University teams compete there, solving tough algorithmic problems. Its world finals is a grand event whose winners are highly sought after programmers.

After Google's and OpenAI's gold medals at the International Mathematical Olympiad, ICPC was definitely next competition to fall to LLMs. And indeed:

* [Gemini achieves gold-medal level](https://deepmind.google/discover/blog/gemini-achieves-gold-level-performance-at-the-international-collegiate-programming-contest-world-finals/) and informal 2nd place, solving 10 out of 12 problems under the same time constraints as the student teams had. Moreover, it solved 8 problems within just 45 minutes.

  It's especially interesting that it solved Problem C, which was solved by no university team.

  You can check the solutions [here](https://github.com/google-deepmind/gemini_icpc2025).
  
* [Models by OpenAI solved 12 out of 12 problems](https://x.com/MostafaRohani/status/1968360976379703569). GPT-5 answered 11 problems correctly, and the last (and most difficult problem) was solved by the experimental reasoning model.

I'm really impressed with this acievement. However, we need to understand that ICPC problems usually have moderately long solutions, and for LLMs solving them should be much easier than working with huge, production-grade repositories.

# FlowRL: Matching Reward Distributions for LLM Reasoning

[https://arxiv.org/pdf/2509.15207](https://arxiv.org/pdf/2509.15207)

RL is a cool device of LLM training, producing great long-reasoning models and helping to align them with human values. But there's an annoying downside to it - a potential of mode collapsing. That is, out of many high-reward regions of text distribution, the LLM collapses during RL training to generating only a handful of peaks. The picture below illustrates this nicely:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/gflow-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

It is the mode collapsing phenomenon that I blame for the annoying GPT style, by the way.

So, finding a way of avoiding this pitfall is a noble quest. The authors took inspiration from GFlowNets (which I won't introduce here; but feel free to check [this paper](https://arxiv.org/pdf/2209.12782), for example) and reused some of the PPO/GRPO best practices, obtaining a nice approach to training long reasoners.

They start from a pretty straightforward idea of minimizing the KL divergence between the trained policy $\pi_{\theta}(y|x)$ and a distribution derived from the reward $r(x,y)$. The latter is usually defined as

$$\widehat{q}_{r}(y|x) = \frac{\exp(\beta r(x, y))}{Z(x)},$$

where $Z(x)$ is a normalizing coefficient needed to make $\widehat{q}_{r}$ an actual distribution (so that it sum to $1$ for fixed $x$). The problem is, however, that we don't know $Z(x)$. Mathematically it's

$$Z(x) = \sum_y\exp(\beta r(x, y)),$$

which is impossible to compute. But... why don't we learn it? So, it becomes $Z_{\phi}(x)$. And the training objective becomes

$$\mathbb{D}_{\text{KL}}\left(\pi_{\theta}(y|x)\left|\left| \frac{\exp(\beta r(x, y))}{Z_{\phi}(x)} \right.\right.\right) \longrightarrow \min_{\theta,\phi}$$

which is equivalent to

$$\log{Z_{\phi}(x)} + \log{\pi_{\theta}(y|x)} - \beta r(x, y) \longrightarrow \min_{\theta,\phi}$$

At this point, however, the authors run into the same problems RL trainers faced:

* The algorithm is totally on-policy, which makes it very inefficient. Insteady, you want to generate trajectories in batches and train on them. Say hi $\pi_{\text{old}}$ from PPO and GRPO, which is exactly the "old" policy that was used to generate trajectories in the current training batch.
* If you want to train a long reasoner model, you have to deal with long trajectories, and your gradients will likely explode.

No wonder the authors end up introducing all the perks of GRPO into the training:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/gflow-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

As you can see, we have here:

* GRPO-style advantage with clipping and importance sampling weights
* Normalizing by length (the $\frac1{\vert y \vert}$ multiples)
* Regularization with respect to the reference (pre-RL) policy

They train Qwen-2.5-7B/32B for math tasks and DeepSeek-R1-Distill-Qwen-7B for code tasks, respectively. borrowing math training dataset from the [DAPO paper](https://arxiv.org/pdf/2503.14476) and code training dataset from the [DeepCoder paper](https://arxiv.org/pdf/2507.12507). The results are quite nice, though the 8k tokens restriction makes me a bit skeptical.

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/gflow-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/gflow-3.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

As for the solution diversity (from which we actually started), the authors gave it some scoring with LLM as a judge:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/gflow-4.png){: .responsive-image style="--img-desktop:60%; --img-mobile:80%;"}
 
# Discovery of Unstable Singularities

[Paper](https://arxiv.org/pdf/2509.14185)

[Post](https://deepmind.google/discover/blog/discovering-new-solutions-to-century-old-problems-in-fluid-dynamics/)

The Navierâ€“Stokes equations are partial differential equations which describe the motion of viscous fluid substances (which is more or less all real fluids or air). For example, the flow of water in your water pipes is described by them.

The theory around these equations is quite involved. It's enough to note that smooth solutions are known only for come particular cases (that is, particular boundary conditions). In general, the existence of smooth solutions is unknown; moreover, this is one of the [Clay Millennium problems](https://en.wikipedia.org/wiki/Millennium_Prize_Problems#Birch_and_Swinnerton-Dyer_conjecture).

Finding special-case solutions of the Navier-Stokes is also not a simple task. So, the authors actually consider several two-dimensional simplifications of the original three-dimensional equations. Moreover, they don't study smooth solutions. Instead, they take a look at **singularities**.

To understand what singularity is, let's consider a frictionless fluid and an ideal duo of 2d counter-rotating vortices that collide at the time $t = 1$. Their radii shrink, angular speed of liquid inside them grows - and at $t = 1$ we have infinite velocity gradient, infinite pressure - and geometrical collapse, because both vortices cease to exist. Mathematically, the solution becomes singular.

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/navier-stokes-1.gif){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

This is probably not the clearest visualization, but you can see here that speeds (black arrows) grow as the vertices converge. At the point of collision they'll be infinite.

However, singularities may be of a different kind. A **stable** singularity is, well, stable with respect of changing the initial conditions. A possible example is separation of a water droplet from your leaking tap. At the very moment of separation, the curvature of the water's surface becomes infinite at the pinch-off point, so it's a singularity. If you change the initial conditions a little bit (for example, if you give the tap an infinitesimal push), the droplet will still fall; the singularity will be here to stay.

An **unstable** singularity is only generated by a very lucky set of initial conditions. Change them in the slightest - and singularity vanishes. The vortex pair collision is one of such examples. If their centers miss each other, no singularity takes place.

---

The authors have indeed discovered unstable singularities for some of the 2d simplifications of Navier--Stokes, so let's discuss the sources of their success. There were several:

**1. The right math formulation of the task**. The authors leverage the mathematical framework known as **self-similar ansatz**. It suggests modelling only spacial situation with the function $\Phi_{\theta}(y)$, while the evolution in time is described by simple contraction, with singularity occuring at $t = 1$ and $x = 0$:

$$
\phi(x,t) = (1-t)^{k(\lambda)} \Phi_{\theta}\left( \frac{x}{(1-t)^{1+\lambda}} \right)
$$

In the schematic below you can see an example of such $\Phi_{\theta}(y)$. More accurately, the 3d plot shows *vorticity* at different points of a 2d plane. The two peaks correspond to two counter-rotating vortices:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/navier-stokes-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

The formula $\phi(x, t)$ describes the contraction of this picture as $t$ tends to $1$.

Moreover, the authors further decompose $\Phi_{\theta}(y)$, implanting some further math properties into it:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/navier-stokes-3.png){: .responsive-image style="--img-desktop:60%; --img-mobile:80%;"}

**2. A Physics-Informed Neural Network (PINN)**. Basically, it's an MLP, but with additional non-trainable transformations that map MLP's final representations to fields and derivatives that can be plugged into the partial differential equation to obtain the residuals $\mathcal{R}_k$ ($k$ enumerate different equations in the system of PDEs). The loss consists of several components:

* *Equation loss* with is, in turn, a combination of square norms of $\mathcal{R}_k$, their first and second partial derivatives.
* *Data loss* which helps to avoid all-zero solutions. It is of the form
  
  $$\mathcal{L}_{\text{data}} = \widehat{c}_{\text{norm}}\left(
\Phi(y_{\text{norm}}) - C_{\text{norm}} 
\right)^2 + \widehat{c}_{\infty}\sum_{y_{\infty}\in Y_{\infty}}\Phi(y_{\infty})^2$$
  
  for a point $y_{\text{norm}}$ chosen from a high-gradient region and an assortment of points $Y_{\infty}$ which are far from the origin. Thus, we control the value at $y_{\text{norm}}$ and demand that $\Phi$ vanish far from the origin.

The model is trained on samples of coordinates $y$, where more points are taken from more crucial, high-gradient regions.

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/navier-stokes-4.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

After training their MLP for some time, the authors froze it and trained a Fourier feature network on the residuals to rule out high-frequency components. (This is the second training stage.)

**3. The Gauss-Newton optimization method** which is an approximate (no full Hessian needed) second-order method. This is important indeed. Since the authors search for *unstable* singularities they need to be extra careful about numeric precision.

The parameter $\lambda$ turns out to be quite important. Solutions only exist at a discrete set of values of $\lambda$. The one with the largest $\lambda$ - which means, fastest-converging - is stable, while the smaller ones give solutions with higher order of instability, meaning their formation is vulnerable to a greater number of distinct perturbation patterns that can knock the system off its singular trajectory.

In the end, the authors don't invent new ML architectures, but they chose well where to plug in the existing ones, and this is very exciting.



