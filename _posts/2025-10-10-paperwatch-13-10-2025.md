---
layout: post
title: "Paperwatch 13.10.2025"
categories: blog
permalink: /paperwatch-13-10-2025/
---

**Paperwatch 13.10.2025 by Stanislav Fedotov (Nebius Academy)**

# RLAD: Training LLMs to Discover Abstractions for Solving Reasoning Problems

[https://arxiv.org/pdf/2510.02263](https://arxiv.org/pdf/2510.02263)

If you want an LLM (or a human!) to solve a complex task, don't expect them to just write the final solution on the spot. Both AI and humans need time to come up with the right solution. That's why long reasoning models like DeepSeek-R1 are so much better in math tasks than other LLMs - they create the whole thee of thoughts in their outputs. 

The problem with long resoning is, though, that it's not very interpretable. Just try to read DeepSeek-R1's solution of any olympiad problem - you'll go crazy before you parse its entangled output. So, maybe we could make things less unruly by introducing some manual orchestration?

Well, the idea isn't new. The simplest form of orchestration is Self Consistency - generating many linear solutions in parallel and then somehow choosing one of them, but this is too naive. The more advanced one - Tree-of-Thoughts, builds the tree of solutions step by step, in a manually orchestrated loop - but it relies on being able to score partial solutions - the problem which isn't very well solved so far.

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/abstractions-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

[Source](https://arxiv.org/pdf/2305.10601)

So, it would be cool to have something in between the naivete of Self Consistency and the forbidding complexity of Tree-of-Thoughts. The authors come up with a nice two-step orchestration scheme, which implements the idea that having a planning stage is often beneficial to produce a solution:

* First, an **abstraction** is generated, which is a guideline towards solution. Here are several abstraction types the authors solidified in their experiments:

  **Caution Alert** - warns the model about what not to do. 

  Example: "Always record forbidden values from denominators before and after manipulation. After solving the polynomial, discard any roots that make a denominator zero... to avoid extraneous solutions."

  **Productive Launchpoint** - suggests a good way to frame the problem or a good first step.

  Example: "Translate comparative statements into algebraic equations using the chosen variables. Phrases like 'twice as many' or 'one less than' correspond to multiplication or addition/subtraction expressions."

  **Blind-Follow Trajectory** - a specific, repeatable procedure or formula that can be applied reliably to a certain type of problem.

  Example: "Logarithms offer a streamlined way to compute floor-based digit counts: for y>0, the number of integer digits is floor(log10 y) + 1. Use this to handle arbitrary exponents without juggling large powers explicitly."

  **Structural Shortcut** - and insight that helps to simplify the problem

  Example: "Use the perimeter constraint a+b+c=P to eliminate one variable, e.g. set c=P-a-b, reducing the problem to two degrees of freedom. This simplification turns the three-variable Heron expression into a function of a and b alone..."

* Then, the actual **solution** is generated, leveraging the abstraction.

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/abstractions-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

## Training

Of course, a model should be trained to work in such a way. The authors start with **Qwen3-1.7B**. (Which is a bit of cheating, because all Qwen3 models are long reasoners and don't just produce a Chain-of-Thought solution.) From it, they train two models - the **Abstraction Generator ($\pi^{abs}$)** and the **Solution Generator ($\pi^{sol}$)**. 

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/abstractions-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

There are two stages of training.

### Phase 1: Warm-starting the Abstraction Generator (SFT)

The abstraction generator is trained with SFT to get a basic understanding of what a useful abstraction looks like.

1.  **Data Generation:** A high-quality dataset of `(problem, abstraction)` pairs is created synthetically. To do this, the authors first use a moderately capable model (for example, the same Qwen3) to generate multiple solution attempts for a given problem. Then, a much stronger model (**o4-mini** in their case) is prompted to "summarize" these solution traces and extract the key strategic insights. This summary becomes the "ground-truth" abstraction `z` for the problem `x`.
2.  **Filtering:** An abstraction is only kept in the training set if it actually helps a base model. They test this by comparing the solution generator's performance on a problem `x` both with and without the abstraction `z`. As far as I understand, they used **GPT-4.1-mini** to compare the two solutions and to decide whether the abstraction was of use. If it judges favourably, the pair `(x, z)` is added to the "seed dataset". This ensures the initial training data consists of genuinely useful abstractions.
5.  **SFT Training:** The abstraction generator, $\pi^{abs}$, is then fine-tuned on this seed dataset (of prompt + abstraction pairs, without solutions)

### Phase 2: Joint RL Training (The RLAD Loop)

Here, the two models learn together. The training alternates between updating $\pi^{abs}$ and $\pi^{sol}$, and both time it uses RL.

#### Training the Solution Generator ($\pi^{sol}$)

The model is trained with DAPO (an improved version of GRPO) and with a simple accuracy reward.

There is a trick, however. If trained naively, $\pi^{sol}$ might learn to ignore a useless or mediocre abstraction and just solve the problem on its own. This would provide no useful learning signal back to the abstraction generator. To force the model to rely on the abstraction:

* The model is trained on a mix of prompts: some with an abstraction `(x, z)` and some without: `(x, ∅)`.
* For any no-abstraction trial (`z = ∅`), the reward is manually set to zero, regardless of whether the solution is correct or not.

#### Training the Abstraction Generator ($\pi^{abs}$)

Here the authors also use DAPO, but the reward is not based on any ground truth, but rather on its "**empirical utility**". The reward for a given `(x, z)` pair is the *expected accuracy* of the solution generator when it is conditioned on that abstraction `z`. 

In practice, the authors sample $n$ solutions from the solution generator and average their accuracies. 

## Results

The resulting ensemble works better than just **Qwen3** trained with DAPO with answer accuracy rewards:

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/abstractions-3.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

They also compare their performance as compute scales: after-DAPO model generates $n^2$ solution from which the final answer is chosen by majority voting, while RLAD generates $n$ abstractions and then from each of them $n$ independent solutions, with also major voting in the end:

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/abstractions-4.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

They further experimented with budget allocation. Say, if you can make $N$ LLM calls to solve a problem, is it better to generate more abstractions but less solutions for each of them - or less abstractions and more solutions for each of them? The takeaway is:

* For any given amount of compute, you achieve higher accuracy by allocating your budget toward generating more diverse abstractions rather than generating more solutions from a single abstraction. (Which makes sense, I suppose, at least until abstractions start repeating themselves.)

Another interesting experiment was trying the trained abstraction generator on different domain and with a more powerful solution generator (**gpt-4o-mini**). It turns out that abstractions are able to add value to even an already cool model:

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/abstractions-5.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

# CWM: An Open-Weights LLM for Research on Code Generation with World Models

[https://www.arxiv.org/pdf/2510.02387](https://www.arxiv.org/pdf/2510.02387)

This paper from Meta introduces a new model for coding (**CWM**) and a new mid-training step for training LLMs for coding - **Code World Modeling Mid-training**.

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/cwm-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Here, **code world modeling** means a very particular thing - the ability of an LLM not only to generate code but also to "understand" what this code does when executed - states and actions - and being able to predict ones from the others. For example, to generate something like this:

```
Let’s verify this by putting the code into a trace context and call the function in the main() function and then trace the execution of the main function.
We indicate the entry point of the execution trace with a # << START_OF_TRACE marker.
def f(d, k):
new_d = {}
for key, val in d.items():
if key < k:
new_d[key] = val
return new_d
def main(): # << START_OF_TRACE
return f({1: 2, 2: 4, 3: 3}, 3)
<|frame_sep|><|call_sep|>{}<|action_sep|>def main(): # << START_OF_TRACE
<|frame_sep|><|line_sep|>{}<|action_sep|> return f({1: 2, 2: 4, 3: 3}, 3)
<|frame_sep|><|call_sep|>{"d": "{1: 2, 2: 4, 3: 3}", "k": "3"}<|action_sep|>def f(d, k):
<|frame_sep|><|line_sep|>{"d": "..", "k": ".."}<|action_sep|> new_d = {}
<|frame_sep|><|line_sep|>{"d": "..", "k": "..", "new_d": "{}"}<|action_sep|> for key, val in d.items():
<|frame_sep|><|line_sep|>{"d": "..", "k": "..", "new_d": "..", "key": "1", "val": "2"}<|action_sep|> if key < k:
<|frame_sep|><|line_sep|>{"d": "..", "k": "..", "new_d": "..", "key": "..", "val": ".."}<|action_sep|> new_d[key] = val
<|frame_sep|><|line_sep|>{"d": "..", "k": "..", "new_d": "{1: 2}", "key": "..", "val": ".."}<|action_sep|> for key, val in d.items():
<|frame_sep|><|line_sep|>{"d": "..", "k": "..", "new_d": "..", "key": "2", "val": "4"}<|action_sep|> if key < k:
<|frame_sep|><|line_sep|>{"d": "..", "k": "..", "new_d": "..", "key": "..", "val": ".."}<|action_sep|> new_d[key] = val
<|frame_sep|><|line_sep|>{"d": "..", "k": "..", "new_d": "{1: 2, 2: 4}", "key": "..", "val": ".."}<|action_sep|> for key, val in d.items():
<|frame_sep|><|line_sep|>{"d": "..", "k": "..", "new_d": "..", "key": "3", "val": "3"}<|action_sep|> if key < k:
<|frame_sep|><|line_sep|>{"d": "..", "k": "..", "new_d": "..", "key": "..", "val": ".."}<|action_sep|> for key, val in d.items():
<|frame_sep|><|line_sep|>{"d": "..", "k": "..", "new_d": "..", "key": "..", "val": ".."}<|action_sep|> return new_d
<|frame_sep|><|return_sep|><|action_sep|> return new_d
<|arg_sep|>"{1: 2, 2: 4}"<|frame_sep|><|return_sep|><|action_sep|> return f({1: 2, 2: 4, 3: 3}, 3)
<|arg_sep|>"{1: 2, 2: 4}"<|frame_sep|>
Now let us analyze the trace. The return argument of the function call f({1: 2, 2: 4, 3: 3}, 3) in the main() function is "{1: 2, 2: 4}" in JSON format, so the return value is {1: 2, 2:
4}.
[ANSWER]
assert f({1: 2,
```

As you see, the LLM tries to predict the variable states. Note also the particular format:

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/cwm-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

But to train an LLM to generate something like this, you need to first gather some data. And that's probably the most interesting part of the paper

## Data collection

### Category 1: Python Execution Traces

The goal here is to create a dataset that shows the model exactly how the state of a program changes, line by line. To do this, they first had to solve the problem of creating runnable environments for a vast number of different software projects. The authors developed a pipeline that uses the github project's own continuous integration (CI) scripts (from GitHub Actions) to build the environment inside a container. This resulted in over 35,000 self-contained, executable versions of real-world repositories.

With these executable environments ready, they collected traces in three main ways:

1.  **Function-Level Tracing**: they took many individual functions and used tracer programs to record the state of all local variables after every single line of code was executed. This method yielded over 120 million traced function executions.

2.  **Repository-Level Tracing**: using the executable repository images, they ran the built-in unit tests for over 21,000 repositories, also recording the traces

3.  **Natural Language Tracing**: they used another LLM (Qwen3-32B) to translate the above traces them into human-readable, step-by-step explanations.

### Category 2: Trajectories gathered actively with ForagerAgent

The setup consists of:
*   **The Agent**: either **Llama3-70B** or **Qwen3-235B**
*   **The Environment**: A Docker container with a command-line shell and tools to edit files, run commands, and navigate the file system.
*   **The Task**: A software engineering problem to be solved.

They generated tasks for the agent in two main ways:

1.  **Mutate-fix tasks (synthetic problems)**: 
    *   First, they would take a working codebase from one of their executable repositories.
    *   They would then automatically introduce a small bug into the code by making a simple change (e.g., deleting a return statement, swapping two variables, changing an operator like `+` to `-`).
    *   They would verify that this mutation actually caused the project's unit tests to fail.
    *   Finally, they would task the ForagerAgent with inspecting the code, running the tests, and fixing the bug it had just introduced.

2.  **Issue-fix tasks (real-world Problems)**:
    *   They would check out the code from a repository at a state *before* a known bug was fixed.
    *   They would provide the ForagerAgent with the text from the actual GitHub issue describing the problem.
    *   The agent's task was then to fix the bug, aiming to make the failing unit tests pass.

For both types of tasks, the entire interaction - every command the agent typed and every response from the environment - was recorded as a "trajectory." They collected 3 million of these trajectories.

## Training

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/cwm-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

### Pre-training

Just pre-training, nothing too peculiar about it. (And yes, the authors train their model from scratch.)

### Mid-training

It's done on execution traces, as described above.

It's in the same time long-context adaptation. Unlike many models that undergo a separate fine-tuning phase to extend their context window, CWM integrated this directly into mid-training. The maximum context length is increased from 8k to 131k tokens for the entire 5 trillion token mid-training run. Indeed, a significant amount of the mid-training data is long-context, with about 30 % of documents exceeding 65k tokens

Of course, 131k tokens is a hell of a context. So it's lucky that the model's architecture has alternating pattern of local and global attention blocks in a 3:1 ratio. 

Also, mid-training sequences are distributed into buckets by their length and every batch comes from exactly one of the buckets. This helps with making GPU utilization more efficient.

### SFT

It's run for 100B tokens. The data is mostly generic SFT datasets, but the authors also added SWE RL trajectories, and some of them from previous attempts at training their model. It helps to prepare the LLM for RL (set up reasoning capabilities for SWE).

This stage also sets up **controlled reasoning**. During SFT, any reasoning text is wrapped in special tokens: `<|reasoning_thinking_start|>` and `<|reasoning_thinking_end|>`. The loss is masked on the `<|reasoning_thinking_start|>` token, meaning the model was never penalized for failing to predict it. The result is a model that learns to reason very well *inside* the tags, but does not learn to spontaneously produce the tags itself. This makes the reasoning capability "on-demand" - it is only activated when the user's prompt includes `<|reasoning_thinking_start|>`.

Another peculiar thing here is constant learning rate used instead of a more common decaying schedule. The authors claim that it helps to enable more stable, high-learning-rate training during the subsequent RL phase.

### RL

The authors use a heavily modified version of multi-turn GRPO, which adopts many improvements from DAPO and also cares for removal of trajectories containing gibberish (low-probability tokens).

RL involved four types of environments:

**1. Agentic Software Engineering (SWE)**


![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/cwm-3.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

This one is about resolving issues within a large, existing codebase. The final deliverable is a `git diff` patch file that solves the problem. A single session might last up to 128 turns.

The agent operates in a sandboxed Docker container that contains a full checkout of a software repository. It has a stateful `bash` shell that persists across turns. The agent is provided with a reliable set of tools:

*   `<tool: bash>`
*   `<tool: edit>` - modify files using a search-and-replace syntax
*   `<tool: create>` - create new files
*   `<tool: submit>` - the final action
    
The reward system is a bit more interesting than the usual pass/fail:

*   If the agent's final patch passes all hidden unit tests, the reward is +1
*   If the tests fail, the reward is determined by the textual similarity (patch similarity score) between the agent's patch and the ground-truth solution. If the computed similarity is above the threshold of 0.5, the reward is 0, otherwise, the reward is −1


**2. Coding (Competitive Programming)**

This is pretty straightforward. The agentic interaction is single-turn - just generate the solution without tools.

The reward is $\pm1$. To get 1, the solution should pass the tests within the specified time and memory limits, and be in correct format

**3. Agentic Coding**

Also competitive programming, but this time multi-turn, in an interactive environment with tools.

**4. Mathematics**

Mostly just the usual math training with answer correctness + format reward. However, during the later stages of RL training, a Python interpreter tool was enabled for a small fraction (2%) of the tasks, allowing the agent to perform calculations to support its reasoning. Which is quite interesting, because coding can really help in some math problems.

**Three-Stage RL Curriculum** 

The joint RL training is split into three distinct stages. Between stages, the authors alter the mix of tasks (for example, adding more programming languages), changed the difficulty (for example, removing hints from hard problems), and adjusted sampling strategies.

## Results

The resulting model is quite competitive, especially given its modest size. In the picture below "tts" stands for test-time scaling (which is usually running the model several times and choosing the best result).

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/cwm-4.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

The model is also quite good at some tasks which are related to coding but aren't coding per se. For example, predicting time and memory complexity, full execution trace prediction, and program termination prediction.


# Reinforcement Learning on Pre-Training Data

[https://arxiv.org/pdf/2509.19249](https://arxiv.org/pdf/2509.19249)

Ever since the emergence of DeepSeek-R1, people were fascinated by the power of RL (even if it has its own problems such as mode collapsing). There were even some attempts at using RL at pre-training. The authors of this paper don't go so far. Instead, they question if RL can use language modeling rewards.

In most LLM training cases, RL's objective is either of these:

* A *reward model* trained on preference data of sorts - that's the case of RLHF
* A *verifiable reward* such as accuracy of the answer for a math problem, success in an agentic task, passing of unit tests in a coding problem etc

The authors suggest using instead the following two rewards:

* **Autoregressive Segment Reasoning** (**ASR**) reward, which teaches the LLM to predict the next text segment (not just one token)
* **Middle Segment Reasoning** (**MSR**) which teaches the LLM to predict what should go between two text segments.

Now, you probably wonder how it works.

First of all, **RL does not supersede pre-training. It happens after normal pre-training and SFT**. More accurately, they take base models **Llama-3.2-3B-Base**, **Qwen3-4B-Base**, **Qwen3-8B-Base** and subject them to a cold-start instruction tuning. They chose to do their own SFT to be able to compare things between LLM families.

Now that the models are able to follow instructions, the **ASR** and **MSR** tasks are set up with prompting:

**ASR prompt**:

```
Complete the text provided under ### Context by predicting the next most probable sentence.

Please reason step by step to determine the best possible continuation, and then enclose your final
answer within <|startofprediction|> and <|endofprediction|> tags.

### Context

{context}
```

**MSR prompt**:

```
## Text Material ##:
{prompt}

<MASK>

{next_step}

## Task ##:
Fill in the <MASK> section of the material with appropriate sentences or a solution step.

Carefully reason step by step to determine the most suitable completion.
Finally, provide your best prediction for the <MASK> section.
Enclose your final answer for the <MASK> part within <|startofprediction|> and <|endofprediction|>.
```

Note that both prompts enforce reasoning.

An important thing is, of course, choosing the right reward. In the next token prediction task, you'd just compare the next generated token with the ground truth, using teacher forcing to ensure that the previous sentence parts are aligned. But with full-segment prediction this won't work. So, LLM-as-a-Judge used instead with the following prompt. (I'm not sure which exactly LLM is used as a judge.)

**Reward prompt** (I like how simple it is - just 0 or 1; this is usually quite effective)

```
## Task
Given a Predicted sentence and a Reference paragraph, determine whether the Predicted text is a prefix (
initial segment) of the Reference paragraph, and whether it expresses exactly the same semantic content
as the corresponding prefix of the Reference.
The Predicted text does not need to match the prefix of the Reference word-for-word, but it must convey
the same meaning.

Reference:
{reference}

Predicted:
{predicted}

## Scoring Rules

If the Predicted text semantically matches the prefix of the Reference, assign a score of 1.

If the Predicted text does not semantically match the prefix of the Reference, assign a score of 0.
When making your judgment, focus primarily on semantic equivalence, not on exact wording.

Only output the score on a single line; do not provide any explanatory text or additional content.

Output format (choose one):

Score: 0
or
Score: 1
```

In the end, the authors train a model with reasoning capability and +several points on math benchmarks. Also, if you do RLVR (DeepSeek-R1-style) on top of that, you get some extra points for it. Unfortunately, the authors don't make a reasonable comparison to other training methods, so it remains unclear how much viable their framework is. Its forte is, however, in scalability, because it doesn't need data with verifiable answers and can just be trained on arXiv papers. But again, it is not totally if pre-training on the same arxiv papers could be as effective.

As a bonus, here is an example of the final LLM's reasoning about the next sentence, which has some fancy emerging structure (which isn't guaranteed to reproduce on a different prompt, of course).

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/rl-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

# The Dragon Hatchling: the missing link between the transformer and models of the brain

[https://arxiv.org/pdf/2509.26507](https://arxiv.org/pdf/2509.26507)

**Beware!** My way of explaining this paper differs drastically from the authors' own presentation. While the brain-modeling narrative sounds cool I personally find that it's more clear and enlightening to explain it from the State Space Model perspective. And I must confess that I tend to be generally unimpressed by attempts at comparing neural networks to brain structures.

The architecture suggested by the authors of this paper - the **Dragon Hatchling** - is a species from the huge **State Space Model** (SSM) family (which also includes relatively wide-known Mamba). The core of every SSM is a linear RNN, that is a recurrent network with no nonlinearities inside the recurrent layer. (The absense of these nonlinearities allow to speed up the training.) 

Linear RNNs are connected to transformers in the following quite surprising way. Consider the usual masked attention

$$Y = \text{softmax}\left(M\odot \frac{QK^T}{\sqrt{n}}\right)V,$$

where $M$ is the 0-1 causal mask, $\odot$ is the elementwise product, and $n$ is the dimension of keys, values, and queries. (It's usually $d$, but I use $n$ here to be consistent with the notation in the paper.) Now, let's ditch $\sqrt{n}$ and, much more importantly, the **softmax**. Then, thanks to the causal mask and some linear algebra magic, it becomes a recurrent process with a recurrently update *hidden state* $h_t$:

$$q_t,\,k_t,\,v_t = q(x_t),\,k(x_t),\,v(x_t)$$

$$h_{t} = \sigma_{t-1} + k_t^Tv_t$$

$$y_t = q_th_t$$

Feel free to check in more details in [my Medium post](https://medium.com/nebius/how-transformers-rnns-and-ssms-are-more-alike-than-you-think-cd0f899893d8). 

An important thing to note here is that, unlike original RNNs, the state $h_t$ isn't just a vector - it's a $n\times n$ matrix, where $n$ is, again, the dimension of queries, keys, and values. Indeed, $k^T$ is a column vector and $v$ is a row vector, so their product is a square matrix.

While this sounds elegant and charming in its own way, SSMs never truly rivaled transformers in downstream performance even if they are way more efficient - a recurrence is linear over $t$ unlike the quadratic attention mechanism, and it doesn't require keeping an ever-growing KV-cache. The problem with performance - which aggravates with increasing context length - is quite understandable: you can't just expect to compress all the information from a long sequence into one tensor. A similar problem plagues vector store components of RAG systems - embeddings are good to capture general meaning but generally not particular facts, numbers, or dates.

### The solution

(Beware: the authors operate with column vectors, while traditional QKV-attention formulas involve row vectors. I think we need to have a world-spanning row-vs-column brawl some day...)

The authors try to relieve this through:

**Idea 1 - large state space.** Making the state $\sigma_t$ really huge. Like, taking $n = 32,768$. It is itself an impressive number, but the state size is actually $n^2$.

**Idea 2 - dimension reduction.** However, a state that huge is, though theoretically cool, highy unfriendly to work with in practice. They authors suggest only considering it implicitly, while actually working with a smaller state $\rho_t$ of dimension $d\times n$ with, say, $d = 256$. They are connected through a trainable projection $E$:

$$\rho_{t} = E\sigma_t$$

**Idea 3 - LoRA weights.** For the sake of computational feasibility, all trainable $n\times n$ matrices are replaced by their LoRA versions; practically, such linear transformations are factored throught the $E$ projections. For example, instead of

$$x\mapsto x + Gy,$$

where $y$ is a kind of an attention lookup (see below), the authors would have

$$x\mapsto x + D_x(Ey),$$

where such multiplication order removes the need of considering any $n\times n$ matrices.

**Idea 4 - LayerNorm in LoRA.** To stabilize the training, the authors further insert LN (LayerNorm) between $D$ and $E$:

$$x\mapsto x + D_x\,\text{LN}(Ey),$$

**Idea 4.** The attention lookup becomes gated:

$$y_{t, l} = G\underbrace{\sigma_{t-1,l}}_{\text{attn. weights}}\underbrace{x_{t,l}}_{\text{value}})\underbrace{\odot x_{t,l}}_{\text{gating}},$$

with with addition of LoRA and LayerNorm becomes

$$y_{t, l} = D_y\,\text{LN}\left(\underbrace{\sigma_{t-1,l}}_{\text{attn. weights}}\underbrace{x_{t,l}}_{\text{value}})\right)\underbrace{\odot x_{t,l}}_{\text{gating}},$$

**Idea 5 - ReLU sparsification.** The authors suggest to sparsify not only the state but also the way the input is updated when passing through a layer. So, instead of $x\mapsto x + \text{update}$, they use

$$x\mapsto x + \left(D_x\,\text{LN}(Ey)\right)^+,$$

where $(\ldots)^+$ is ReLU. This, of course, leads to somewhat more sparse and structured updates.

The computation of $y$ is likewise sparsified.

$$y_{t, l} = \left(D_y\,\text{LN}\left(\underbrace{\sigma_{t-1,l}}_{\text{attn. weights}}\underbrace{x_{t,l}}_{\text{value}})\right)\right)^+\underbrace{\odot x_{t,l}}_{\text{gating}},$$

### The actual formulas

The actual layer architecture that the authors suggest is as follows (please look at the **BDH-GPU** part now, which is the final version):

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/hatchling-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Here:

* $l$ is the layer index and $t$ is the token index
* $$x_{l-1}$$ is the layer's input, which is a sequence of representations $$x_{t, l-1}$$ across $t$ - over all tokens:
* $x_{l}$ is the layer's output
* $\rho_{t,l}$ are the small, $d$-dimensional hidden states. It is through the update of the hidden state that the information propagates through the sequence:
  
  $$\rho_{t,l} = (\rho_{t-1,l} + \text{update})U$$
  
* $U$ is a fixed RoPE (Rotary Positional Encoding) matrix
* LN is LayerNorm. It is inserted between $D$ and $E$ as $D(\text{LN}(E(\ldots))$ with the only purpose of making the training more stable
* $$y_{t, l}$$ are something like the result of attention lookup from the previous layer

This picture might help you to wrap your head around the formulas:

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/hatchling-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

The **trainable parameters** here are matrices $D_{x, y}$ and $E$. They are shared across all layers.

Unfortunately, the authors train very small models on small datasets, so the numerical results are inconsequential. There's even no comparison to Mamba2 or other SoTA SSMs. But there's some interesting analysis to discuss.

### Some analysis of ReLU-lowrank matrix products

The combination of ReLU and low rank decomposition $G = DE$ seems to have interesting effects, namely:

* The eventual matrices $G = DE$ exhibit block-diagonal structure, which the authors characterize as coordinates coming together into "communities", related to certain topics. So, if $y$ mainly had nonzeros in coordinates related to, say, coding, then multiplication by $G$ and subsequent ReLU might preserve this topic, and $x$ would be updated in coding-related coordinates.
* The authors discover several particular matrix entry $\sigma_{i,j}$ ("synapses") which are interpretable. For example, the currency-related matrix entry:

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/hatchling-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

  Matrix $\sigma$ itself tends to be quite sparse. The plot below shows that "most nodes have few connections", which means that for a given $j$, there are usually few $\sigma_{ij}\ne 0$.

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/hatchling-3.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

  And we know that considering large sparse spaces often leads to feature disentanglement. (Like, in sparse autoencoders).

* $x$ and $y$ also exhibit sparse structure, with typically about 5% of coordinates being nonzero.

In the end, we have another attempt at making State Space Models cool. While we can't judge how cool the final models are in this case, the observations about  ReLU-lowrank matrix products are interesting, and this idea might be used in some further SSM architectures.


### Biological connection

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/hatchling-4.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

In this interpretation, coordinates of $x$ and $y$ are interpreted as neurons that "fire" when the coordinate is nonzero. The empyrical observation that $x$ and $y$ are sparce correspond to the principle of **sparse coding** in the brain, where only a small fraction of neurons are active at any moment.

The matrices $G$ and $\sigma$ represent the connections between them; $$g_{ij}$$ and $$\sigma_{ij}$$ are **synapses**, connecting $j$-th source neuron to the $j$-th target neuron. The role of these matrices are quite different.

* $G$ does the "thinking", updating the main thought stream $x$. It is defined by life-long learning and doesn't change during execution of a particular task
* $\sigma$ does the "knowledge retrieval" from $y$, and these synapses are mutable, subject to what is known as **short-term synaptic plasticity**. The update rule

  $$\sigma_t = \sigma_{t-1} + y_t * x_t^T$$

  is the math interpretation of **Hebbian learning** ("neurons that fire together, wire together"). And indeed, $$(y_t * x_t^T)_{ij} = x_{t,i}\cdot y_{t,j}$$ is nonzero exactly when both $$x_{t,i}$$ and $$y_{t,j}$$ are nonzero.
