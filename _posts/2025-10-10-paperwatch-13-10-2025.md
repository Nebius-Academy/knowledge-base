---
layout: post
title: "Paperwatch 13.10.2025"
categories: blog
permalink: /paperwatch-13-10-2025/
---

**Paperwatch 13.10.2025 by Stanislav Fedotov (Nebius Academy)**

# RLAD: Training LLMs to Discover Abstractions for Solving Reasoning Problems

[https://arxiv.org/pdf/2510.02263](https://arxiv.org/pdf/2510.02263)

If you want an LLM (or a human!) to solve a complex task, don't expect them to just write the final solution on the spot. Both AI and humans need time to come up with the right solution. That's why long reasoning models like DeepSeek-R1 are so much better in math tasks than other LLMs - they create the whole thee of thoughts in their outputs. 

The problem with long resoning is, though, that it's not very interpretable. Just try to read DeepSeek-R1's solution of any olympiad problem - you'll go crazy before you parse its entangled output. So, maybe we could make things less unruly by introducing some manual orchestration?

Well, the idea isn't new. The simplest form of orchestration is Self Consistency - generating many linear solutions in parallel and then somehow choosing one of them, but this is too naive. The more advanced one - Tree-of-Thoughts, builds the tree of solutions step by step, in a manually orchestrated loop - but it relies on being able to score partial solutions - the problem which isn't very well solved so far.

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/abstractions-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

[Source](https://arxiv.org/pdf/2305.10601)

So, it would be cool to have something in between the naivete of Self Consistency and the forbidding complexity of Tree-of-Thoughts. The authors come up with a nice two-step orchestration scheme, which implements the idea that having a planning stage is often beneficial to produce a solution:

* First, an **abstraction** is generated, which is a guideline towards solution. Here are several abstraction types the authors solidified in their experiments:

  **Caution Alert** - warns the model about what not to do. 

  Example: "Always record forbidden values from denominators before and after manipulation. After solving the polynomial, discard any roots that make a denominator zero... to avoid extraneous solutions."

  **Productive Launchpoint** - suggests a good way to frame the problem or a good first step.

  Example: "Translate comparative statements into algebraic equations using the chosen variables. Phrases like 'twice as many' or 'one less than' correspond to multiplication or addition/subtraction expressions."

  **Blind-Follow Trajectory** - a specific, repeatable procedure or formula that can be applied reliably to a certain type of problem.

  Example: "Logarithms offer a streamlined way to compute floor-based digit counts: for y>0, the number of integer digits is floor(log10 y) + 1. Use this to handle arbitrary exponents without juggling large powers explicitly."

  **Structural Shortcut** - and insight that helps to simplify the problem

  Example: "Use the perimeter constraint a+b+c=P to eliminate one variable, e.g. set c=P-a-b, reducing the problem to two degrees of freedom. This simplification turns the three-variable Heron expression into a function of a and b alone..."

* Then, the actual **solution** is generated, leveraging the abstraction.

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/abstractions-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

## Training

Of course, a model should be trained to work in such a way. The authors start with **Qwen3-1.7B**. (Which is a bit of cheating, because all Qwen3 models are long reasoners and don't just produce a Chain-of-Thought solution.) From it, they train two models - the **Abstraction Generator ($\pi^{abs}$)** and the **Solution Generator ($\pi^{sol}$)**. 

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/abstractions-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

There are two stages of training.

### Phase 1: Warm-starting the Abstraction Generator (SFT)

The abstraction generator is trained with SFT to get a basic understanding of what a useful abstraction looks like.

1.  **Data Generation:** A high-quality dataset of `(problem, abstraction)` pairs is created synthetically. To do this, the authors first use a moderately capable model (for example, the same Qwen3) to generate multiple solution attempts for a given problem. Then, a much stronger model (**o4-mini** in their case) is prompted to "summarize" these solution traces and extract the key strategic insights. This summary becomes the "ground-truth" abstraction `z` for the problem `x`.
2.  **Filtering:** An abstraction is only kept in the training set if it actually helps a base model. They test this by comparing the solution generator's performance on a problem `x` both with and without the abstraction `z`. As far as I understand, they used **GPT-4.1-mini** to compare the two solutions and to decide whether the abstraction was of use. If it judges favourably, the pair `(x, z)` is added to the "seed dataset". This ensures the initial training data consists of genuinely useful abstractions.
5.  **SFT Training:** The abstraction generator, $\pi^{abs}$, is then fine-tuned on this seed dataset (of prompt + abstraction pairs, without solutions)

### Phase 2: Joint RL Training (The RLAD Loop)

Here, the two models learn together. The training alternates between updating $\pi^{abs}$ and $\pi^{sol}$, and both time it uses RL.

#### Training the Solution Generator ($\pi^{sol}$)

The model is trained with DAPO (an improved version of GRPO) and with a simple accuracy reward.

There is a trick, however. If trained naively, $\pi^{sol}$ might learn to ignore a useless or mediocre abstraction and just solve the problem on its own. This would provide no useful learning signal back to the abstraction generator. To force the model to rely on the abstraction:

* The model is trained on a mix of prompts: some with an abstraction `(x, z)` and some without: `(x, ∅)`.
* For any no-abstraction trial (`z = ∅`), the reward is manually set to zero, regardless of whether the solution is correct or not.

#### Training the Abstraction Generator ($\pi^{abs}$)

Here the authors also use DAPO, but the reward is not based on any ground truth, but rather on its "**empirical utility**". The reward for a given `(x, z)` pair is the *expected accuracy* of the solution generator when it is conditioned on that abstraction `z`. 

In practice, the authors sample $n$ solutions from the solution generator and average their accuracies. 

## Results

The resulting ensemble works better than just **Qwen3** trained with DAPO with answer accuracy rewards:

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/abstractions-3.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

They also compare their performance as compute scales: after-DAPO model generates $n^2$ solution from which the final answer is chosen by majority voting, while RLAD generates $n$ abstractions and then from each of them $n$ independent solutions, with also major voting in the end:

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/abstractions-4.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

They further experimented with budget allocation. Say, if you can make $N$ LLM calls to solve a problem, is it better to generate more abstractions but less solutions for each of them - or less abstractions and more solutions for each of them? The takeaway is:

* For any given amount of compute, you achieve higher accuracy by allocating your budget toward generating more diverse abstractions rather than generating more solutions from a single abstraction. (Which makes sense, I suppose, at least until abstractions start repeating themselves.)

Another interesting experiment was trying the trained abstraction generator on different domain and with a more powerful solution generator (**gpt-4o-mini**). It turns out that abstractions are able to add value to even an already cool model:

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/abstractions-5.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

# Reinforcement Learning on Pre-Training Data

[https://arxiv.org/pdf/2509.19249](https://arxiv.org/pdf/2509.19249)

Ever since the emergence of DeepSeek-R1, people were fascinated by the power of RL (even if it has its own problems such as mode collapsing). There were even some attempts at using RL at pre-training. The authors of this paper don't go so far. Instead, they question if RL can use language modeling rewards.

In most LLM training cases, RL's objective is either of these:

* A *reward model* trained on preference data of sorts - that's the case of RLHF
* A *verifiable reward* such as accuracy of the answer for a math problem, success in an agentic task, passing of unit tests in a coding problem etc

The authors suggest using instead the following two rewards:

* **Autoregressive Segment Reasoning** (**ASR**) reward, which teaches the LLM to predict the next text segment (not just one token)
* **Middle Segment Reasoning** (**MSR**) which teaches the LLM to predict what should go between two text segments.

Now, you probably wonder how it works.

First of all, **RL does not supersede pre-training. It happens after normal pre-training and SFT**. More accurately, they take base models **Llama-3.2-3B-Base**, **Qwen3-4B-Base**, **Qwen3-8B-Base** and subject them to a cold-start instruction tuning. They chose to do their own SFT to be able to compare things between LLM families.

Now that the models are able to follow instructions, the **ASR** and **MSR** tasks are set up with prompting:

**ASR prompt**:

```
Complete the text provided under ### Context by predicting the next most probable sentence.

Please reason step by step to determine the best possible continuation, and then enclose your final
answer within <|startofprediction|> and <|endofprediction|> tags.

### Context

{context}
```

**MSR prompt**:

```
## Text Material ##:
{prompt}

<MASK>

{next_step}

## Task ##:
Fill in the <MASK> section of the material with appropriate sentences or a solution step.

Carefully reason step by step to determine the most suitable completion.
Finally, provide your best prediction for the <MASK> section.
Enclose your final answer for the <MASK> part within <|startofprediction|> and <|endofprediction|>.
```

Note that both prompts enforce reasoning.

An important thing is, of course, choosing the right reward. In the next token prediction task, you'd just compare the next generated token with the ground truth, using teacher forcing to ensure that the previous sentence parts are aligned. But with full-segment prediction this won't work. So, LLM-as-a-Judge used instead with the following prompt. (I'm not sure which exactly LLM is used as a judge.)

**Reward prompt** (I like how simple it is - just 0 or 1; this is usually quite effective)

```
## Task
Given a Predicted sentence and a Reference paragraph, determine whether the Predicted text is a prefix (
initial segment) of the Reference paragraph, and whether it expresses exactly the same semantic content
as the corresponding prefix of the Reference.
The Predicted text does not need to match the prefix of the Reference word-for-word, but it must convey
the same meaning.

Reference:
{reference}

Predicted:
{predicted}

## Scoring Rules

If the Predicted text semantically matches the prefix of the Reference, assign a score of 1.

If the Predicted text does not semantically match the prefix of the Reference, assign a score of 0.
When making your judgment, focus primarily on semantic equivalence, not on exact wording.

Only output the score on a single line; do not provide any explanatory text or additional content.

Output format (choose one):

Score: 0
or
Score: 1
```

In the end, the authors train a model with reasoning capability and +several points on math benchmarks. Also, if you do RLVR (DeepSeek-R1-style) on top of that, you get some extra points for it. Unfortunately, the authors don't make a reasonable comparison to other training methods, so it remains unclear how much viable their framework is. Its forte is, however, in scalability, because it doesn't need data with verifiable answers and can just be trained on arXiv papers. But again, it is not totally if pre-training on the same arxiv papers could be as effective.

As a bonus, here is an example of the final LLM's reasoning about the next sentence, which has some fancy emerging structure (which isn't guaranteed to reproduce on a different prompt, of course).

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/rl-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

# The Dragon Hatchling: the missing link between the transformer and models of the brain

[https://arxiv.org/pdf/2509.26507](https://arxiv.org/pdf/2509.26507)

**Beware!** My way of explaining this paper differs drastically from the authors' own presentation. While the brain-modeling narrative sounds cool I personally find that it's more clear and enlightening to explain it from the State Space Model perspective. And I must confess that I tend to be generally unimpressed by attempts at comparing neural networks to brain structures.

The architecture suggested by the authors of this paper - the **Dragon Hatchling** - is a species from the huge **State Space Model** (SSM) family (which also includes relatively wide-known Mamba). The core of every SSM is a linear RNN, that is a recurrent network with no nonlinearities inside the recurrent layer. (The absense of these nonlinearities allow to speed up the training.) 

Linear RNNs are connected to transformers in the following quite surprising way. Consider the usual masked attention

$$Y = \text{softmax}\left(M\odot \frac{QK^T}{\sqrt{n}}\right)V,$$

where $M$ is the 0-1 causal mask, $\odot$ is the elementwise product, and $n$ is the dimension of keys, values, and queries. (It's usually $d$, but I use $n$ here to be consistent with the notation in the paper.) Now, let's ditch $\sqrt{n}$ and, much more importantly, the **softmax**. Then, thanks to the causal mask and some linear algebra magic, it becomes a recurrent process with a recurrently update *hidden state* $h_t$:

$$q_t,\,k_t,\,v_t = q(x_t),\,k(x_t),\,v(x_t)$$

$$h_{t} = \sigma_{t-1} + k_t^Tv_t$$

$$y_t = q_th_t$$

Feel free to check in more details in [my Medium post](https://medium.com/nebius/how-transformers-rnns-and-ssms-are-more-alike-than-you-think-cd0f899893d8). 

An important thing to note here is that, unlike original RNNs, the state $h_t$ isn't just a vector - it's a $n\times n$ matrix, where $n$ is, again, the dimension of queries, keys, and values. Indeed, $k^T$ is a column vector and $v$ is a row vector, so their product is a square matrix.

While this sounds elegant and charming in its own way, SSMs never truly rivaled transformers in downstream performance even if they are way more efficient - a recurrence is linear over $t$ unlike the quadratic attention mechanism, and it doesn't require keeping an ever-growing KV-cache. The problem with performance - which aggravates with increasing context length - is quite understandable: you can't just expect to compress all the information from a long sequence into one tensor. A similar problem plagues vector store components of RAG systems - embeddings are good to capture general meaning but generally not particular facts, numbers, or dates.

### The solution

(Beware: the authors operate with column vectors, while traditional QKV-attention formulas involve row vectors. I think we need to have a world-spanning row-vs-column brawl some day...)

The authors try to relieve this through:

**Idea 1 - large state space.** Making the state $\sigma_t$ really huge. Like, taking $n = 32,768$. It is itself an impressive number, but the state size is actually $n^2$.

**Idea 2 - dimension reduction.** However, a state that huge is, though theoretically cool, highy unfriendly to work with in practice. They authors suggest only considering it implicitly, while actually working with a smaller state $\rho_t$ of dimension $d\times n$ with, say, $d = 256$. They are connected through a trainable projection $E$:

$$\rho_{t} = E\sigma_t$$

**Idea 3 - LoRA weights.** For the sake of computational feasibility, all trainable $n\times n$ matrices are replaced by their LoRA versions; practically, such linear transformations are factored throught the $E$ projections. For example, instead of

$$x\mapsto x + Gy,$$

where $y$ is a kind of an attention lookup (see below), the authors would have

$$x\mapsto x + D_x(Ey),$$

where such multiplication order removes the need of considering any $n\times n$ matrices.

**Idea 4 - LayerNorm in LoRA.** To stabilize the training, the authors further insert LN (LayerNorm) between $D$ and $E$:

$$x\mapsto x + D_x\,\text{LN}(Ey),$$

**Idea 4.** The attention lookup becomes gated:

$$y_{t, l} = G\underbrace{\sigma_{t-1,l}}_{\text{attn. weights}}\underbrace{x_{t,l}}_{\text{value}})\underbrace{\odot x_{t,l}}_{\text{gating}},$$

with with addition of LoRA and LayerNorm becomes

$$y_{t, l} = D_y\,\text{LN}\left(\underbrace{\sigma_{t-1,l}}_{\text{attn. weights}}\underbrace{x_{t,l}}_{\text{value}})\right)\underbrace{\odot x_{t,l}}_{\text{gating}},$$

**Idea 5 - ReLU sparsification.** The authors suggest to sparsify not only the state but also the way the input is updated when passing through a layer. So, instead of $x\mapsto x + \text{update}$, they use

$$x\mapsto x + \left(D_x\,\text{LN}(Ey)\right)^+,$$

where $(\ldots)^+$ is ReLU. This, of course, leads to somewhat more sparse and structured updates.

The computation of $y$ is likewise sparsified.

$$y_{t, l} = \left(D_y\,\text{LN}\left(\underbrace{\sigma_{t-1,l}}_{\text{attn. weights}}\underbrace{x_{t,l}}_{\text{value}})\right)\right)^+\underbrace{\odot x_{t,l}}_{\text{gating}},$$

### The actual formulas

The actual layer architecture that the authors suggest is as follows (please look at the **BDH-GPU** part now, which is the final version):

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/hatchling-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Here:

* $l$ is the layer index and $t$ is the token index
* $$x_{l-1}$$ is the layer's input, which is a sequence of representations $$x_{t, l-1}$$ across $t$ - over all tokens:
* $x_{l}$ is the layer's output
* $\rho_{t,l}$ are the small, $d$-dimensional hidden states. It is through the update of the hidden state that the information propagates through the sequence:
  
  $$\rho_{t,l} = (\rho_{t-1,l} + \text{update})U$$
  
* $U$ is a fixed RoPE (Rotary Positional Encoding) matrix
* LN is LayerNorm. It is inserted between $D$ and $E$ as $D(\text{LN}(E(\ldots))$ with the only purpose of making the training more stable
* $$y_{t, l}$$ are something like the result of attention lookup from the previous layer

This picture might help you to wrap your head around the formulas:

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/hatchling-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

The **trainable parameters** here are matrices $D_{x, y}$ and $E$. They are shared across all layers.

Unfortunately, the authors train very small models on small datasets, so the numerical results are inconsequential. There's even no comparison to Mamba2 or other SoTA SSMs. But there's some interesting analysis to discuss.

### Some analysis of ReLU-lowrank matrix products

The combination of ReLU and low rank decomposition $G = DE$ seems to have interesting effects, namely:

* The eventual matrices $G = DE$ exhibit block-diagonal structure, which the authors characterize as coordinates coming together into "communities", related to certain topics. So, if $y$ mainly had nonzeros in coordinates related to, say, coding, then multiplication by $G$ and subsequent ReLU might preserve this topic, and $x$ would be updated in coding-related coordinates.
* The authors discover several particular matrix entry $\sigma_{i,j}$ ("synapses") which are interpretable. For example, the currency-related matrix entry:

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/hatchling-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

  Matrix $\sigma$ itself tends to be quite sparse. The plot below shows that "most nodes have few connections", which means that for a given $j$, there are usually few $\sigma_{ij}\ne 0$.

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/hatchling-3.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

  And we know that considering large sparse spaces often leads to feature disentanglement. (Like, in sparse autoencoders).

* $x$ and $y$ also exhibit sparse structure, with typically about 5% of coordinates being nonzero.

In the end, we have another attempt at making State Space Models cool. While we can't judge how cool the final models are in this case, the observations about  ReLU-lowrank matrix products are interesting, and this idea might be used in some further SSM architectures.


### Biological connection

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/hatchling-4.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

In this interpretation, coordinates of $x$ and $y$ are interpreted as neurons that "fire" when the coordinate is nonzero. The empyrical observation that $x$ and $y$ are sparce correspond to the principle of **sparse coding** in the brain, where only a small fraction of neurons are active at any moment.

The matrices $G$ and $\sigma$ represent the connections between them; $$g_{ij}$$ and $$\sigma_{ij}$$ are **synapses**, connecting $j$-th source neuron to the $j$-th target neuron. The role of these matrices are quite different.

* $G$ does the "thinking", updating the main thought stream $x$. It is defined by life-long learning and doesn't change during execution of a particular task
* $\sigma$ does the "knowledge retrieval" from $y$, and these synapses are mutable, subject to what is known as **short-term synaptic plasticity**. The update rule

  $$\sigma_t = \sigma_{t-1} + y_t * x_t^T$$

  is the math interpretation of **Hebbian learning** ("neurons that fire together, wire together"). And indeed, $$(y_t * x_t^T)_{ij} = x_{t,i}\cdot y_{t,j}$$ is nonzero exactly when both $$x_{t,i}$$ and $$y_{t,j}$$ are nonzero.
