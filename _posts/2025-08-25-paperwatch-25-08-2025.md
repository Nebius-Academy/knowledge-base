---
layout: post
title: "Paperwatch 25.08.2025"
categories: blog
permalink: /paperwatch-25-08-2025/
---

**Paperwatch 25.08.2025 by Stanislav Fedotov (Nebius Academy)**

# Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory

[https://www.arxiv.org/pdf/2508.09736](https://www.arxiv.org/pdf/2508.09736)

This paper by ByteDance suggests the **M3-Agent**, a multimodal agent framework equipped with long-term memory.

I would say that for now, LLMs and MLLMs don't excel at long-term memorization. Even as the models progress, in long chats concepts still drift and details get forgotten - the model eventually starts to random guess or to use common sense / pre-trained memory instead of in-context memory. Let's see how the authors of this paper approached the problem.

They consider memorization of videos and challenge themselves for two tasks as tough as they are vital:

1. **Infinite information processing** - that is, handling of arbitrary long data streams. Not just one video, but, say, a lifetime visual perception of a robot. 
2. **World knowledge construction**. It's relatively simple to extract low-level visual details, but much harder and more important is to work with high-level entities. Like, for example, keeping track of particular characters or sounds and matching them across the video. 

I can't help saying that these two things aren't just relevant for video processing. LLM chatbots would also benefit from clever, infinite-length memorization. I won't be against having it in ChatGPT.

Let's discuss the implementation. To tell the truth, it's a bit too narrow and too manually engineered to my taste. The authors suggest three node types:

* `image` - used exclusively for facial features. The content of an image node is a base64-encoded image.
* `audio` - used to store voice prints for speaker identification. The content of an audio node is a base64-encoded audio clip.
* `text` - used to store all natural language information. This includes two memory types:

  * **Episodic memory**: Records concrete events observed within the video. For example, "Alice takes the
coffee and says, ‘I can’t go without this in the morning,’" and "Alice throws an empty bottle into the
green garbage bin."
  * **Semantic memory**: Derives general knowledge from the clip. For example, "Alice prefers to drink coffee
in the morning" and "The green garbage bin is used for recycling."

Every memory node is actually richer than just an image or a text - it also contains an embedding and an importance weight:

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/m3-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Let's briefly discuss how inference-time memorization works.

Videos are processed in consequent 30-second segments. 

First, dedicated (and quite powerful) video and sound processing tools are used to mine faces and voices from these short clips. Then, a fine tuned version of **Qwen2.5-Omni-7b** gets both a video fragment, and extracted information - and it is prompted to do many things at once:

- match face IDs with voice IDs
- produce episodic and semantic memories (yes, in one prompt) 

To match voice/face IDs across different fragments, the authors suggest using vector search on embeddings stored in their nodes.

Memorization is needed for something, and in this case - for question answering. M3-agent has a parallel-running process for it, called **control**. By default is dormant, but as soon as a user asks a question (which might be any time), it wakes up and does several round of search-and-interpretation with two kinds of search mechanisms:

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/m3-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

The interpretation is performed by a fine tuned **Qwen3-32B**, and the loop breaks when the LLM decides that it has sufficient information for giving the final answer.

**Training**

Of course, both **Qwen2.5-Omni-7b** and **Qwen3-32B** should be trained for their tasks.

The **memorization** model **Qwen2.5-Omni-7b** is trained via SFT.

For that, the authors collect a memory dataset with the help of GPT-4o and Gemini-1.5-Pro. More accurately:

* *For faces and voices*: Each long video is segmented into a series of short clips (no longer than 5 seconds). Then, dedicated video and sound processing tools are used to mine faces and voices from these short clips, matching face and voice IDs across them. To match a face with its voice, the authors use fragments ("meta-clips") where only once face ID and only once voice ID are present. Inconsistencies are ruled out with importance weights.

* **Episodic memory** nodes are created with a joint force of Gemini-1.5-Pro and
GPT-4o. GPT-4o generates a detailed visual description of the video using frames sampled at 0.5 fps. Then, Gemini-1.5-Pro, also using audio input, also the final episodic memory.

* **Semantic memory** is created in a similar way - GPT-4o forms a preliminary output, then Gemini refines it. They get:

  * 16 frames of a video, for GPT-4o, or a clip in .mp4 format with audio for Gemini
  * A list of facial features detected in the video, each linked to a unique face ID.
  * A list of speech segments in the video, including start_time, end_time, speaker ID, and the corresponding transcribed text

  They are prompted to extract:

  * Character-Level Attributes - name, personality, profession etc
  * Interpersonal Relationships & Dynamics
  * Video-Level Plot Understanding
  * Contextual & General Knowledge like settings or genre, cultural norms, common-sense conventions etc

The **control** model **Qwen3-32B** is trained via Reinforcement Learning on Q&A with answer supervision - more accurately with [DAPO](https://arxiv.org/pdf/2503.14476), a cool and fashionable modification of GRPO.


Again, quite rule-based and restrictive, to my taste. But this is likely defined by the results the authors strove to demonstrate. And to move towards that, let's discuss the parallel **control** stream.

**Results**

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/m3-3.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

In comparison to "socratic" methods, M3-agent brings:

1. Structured, entity-centric memory as opposed to a collection of unstructured texts
2. Multi-turn search-and-interpretation for the control process

In the end, the results are quite nice. However, I hope to see more memorization processes that allow LLMs to choose entity types and content in a less restricted way. Most likely, that would require longer training; maybe even memory pre-training of sorts. But that might be worth it. 

# Thyme: Think Beyond Images

[https://arxiv.org/pdf/2508.11630](https://arxiv.org/pdf/2508.11630)

It's curious to see how research revisits past idea. Even before researchers it became populat to establish multimodality in LLMs through architectural means (visual encoders and decoders, latent diffusion etc), there were attempts at *orchestrating* multimodality in an agentic way. For example, [ViperGPT](https://arxiv.org/pdf/2303.08128) generated image-processing Pythonic function calls:

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/thyme-0.png){: .responsive-image style="--img-desktop:70%; --img-mobile:90%;"}

[Source](https://arxiv.org/pdf/2303.08128)

Now, agency returns. **Thyme** suggests an agentic loop, where 

* a multimodal LLM processes a prompt (text + image(s)) 
* then, it may choose to call a function for, say, upscaling a part of an image to learn more about what happens there
* then, it analyzes the resulting image and, if the information is still insufficient, it may call another image-processing or image-editing tool

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/thyme-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

In the end, it's another version of visual reasoning without actual generation of intermediate images. The latter in known to be finicky, and some efforts have been spent to reduce this burden; for example, training an LLM to ["think" with latent image tokens](https://arxiv.org/pdf/2506.17218). I'm myself is quite optimistic about agentic approach. It's very rewarding, of course, to have LLMs solve problems through just their mental power - but if you have tools, why not use them? This is true for math computations as well, and it's good to see that the authors also relied on tools for it.

Of course, an MLLM (**Qwen2.5-VL-72B** in this case) should be trained to work efficiently in such an agentic loop, and the authors employ a two-stage strategy:

1. **SFT** on a large, curated dataset of 500,000 samples to teach the model foundational coding skills.

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/thyme-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

The authors started with a 4M-sized dataset of benchmark tasks and generated solutions, discarding those unsuccessful and featuring broken code.

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/thyme-3.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Some 100k training samples were also chosen from a simple benchmark to serve as examples of situations where the final model should answer without generating any code. The authors used a larger model, **Qwen2.5-VL-72B**, to assess whether code generation.

Furthermore, another 15k examples were collected somewhat manually to enrich the training distribution - with examples of working with low-quality images, multi-step self-correction etc. For constructing the multi-step part, Gemini and GPT were also employed.

It's notable though that multi-step data backfired in a sense - the model sometimes learned that it can always correct itself in later stages, making the first step less reliable. The authors combatted this with stage masking.

2. **Reinforcement Learning**

The authors go an extra mile to create a high-quality dataset for the RL stage. They wanted it to contain high-resolution images with high perceptual complexity, so they 

* collected 30k images from the net with width or height $\geqslant$ 2048 px, of sufficient complexity
* gathered a team of 15 annotators to label them, prioritizing small and challenging-to-recognize objects occupying no more than 5% of the image resolution.

Furthermore, the authors suggested a modification of the **GRPO** algorithm - **GRPO-ATS** (**GRPO with Adaptive Temperature Sampling**), which applies
temperature 0 during code generation phases to ensure deterministic output, and temperature 1.0 for natural language reasoning.

The results are quite nice, for the model's size, and it even outperforms the 32B Qwen:

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/thyme-4.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

It would be interesting to see some efficiency analysis, but, unfortunately, there are not many models to compare with. The main claim is that agentic reasoning is better than reasoning through generating intermediate images (latent or full-size) and we don't have too many good examples here to compare with.

