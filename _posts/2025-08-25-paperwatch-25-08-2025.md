---
layout: post
title: "Paperwatch 25.08.2025"
categories: blog
permalink: /paperwatch-25-08-2025/
---

**Paperwatch 25.08.2025 by Stanislav Fedotov (Nebius Academy)**



# Thyme: Think Beyond Images

[https://arxiv.org/pdf/2508.11630](https://arxiv.org/pdf/2508.11630)

It's curious to see how research revisits past idea. Even before researchers it became populat to establish multimodality in LLMs through architectural means (visual encoders and decoders, latent diffusion etc), there were attempts at *orchestrating* multimodality in an agentic way. For example, [ViperGPT](https://arxiv.org/pdf/2303.08128) generated image-processing Pythonic function calls:

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/thyme-0.png){: .responsive-image style="--img-desktop:70%; --img-mobile:90%;"}

[Source](https://arxiv.org/pdf/2303.08128)

Now, agency returns. **Thyme** suggests an agentic loop, where 

* a multimodal LLM processes a prompt (text + image(s)) 
* then, it may choose to call a function for, say, upscaling a part of an image to learn more about what happens there
* then, it analyzes the resulting image and, if the information is still insufficient, it may call another image-processing or image-editing tool

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/thyme-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

In the end, it's another version of visual reasoning without actual generation of intermediate images. The latter in known to be finicky, and some efforts have been spent to reduce this burden; for example, training an LLM to ["think" with latent image tokens](https://arxiv.org/pdf/2506.17218). I'm myself is quite optimistic about agentic approach. It's very rewarding, of course, to have LLMs solve problems through just their mental power - but if you have tools, why not use them? This is true for math computations as well, and it's good to see that the authors also relied on tools for it.

Of course, an MLLM (**Qwen2.5-VL-72B** in this case) should be trained to work efficiently in such an agentic loop, and the authors employ a two-stage strategy:

1. **SFT** on a large, curated dataset of 500,000 samples to teach the model foundational coding skills.

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/thyme-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

The authors started with a 4M-sized dataset of benchmark tasks and generated solutions, discarding those unsuccessful and featuring broken code.

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/thyme-3.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Some 100k training samples were also chosen from a simple benchmark to serve as examples of situations where the final model should answer without generating any code. The authors used a larger model, **Qwen2.5-VL-72B**, to assess whether code generation.

Furthermore, another 15k examples were collected somewhat manually to enrich the training distribution - with examples of working with low-quality images, multi-step self-correction etc. For constructing the multi-step part, Gemini and GPT were also employed.

It's notable though that multi-step data backfired in a sense - the model sometimes learned that it can always correct itself in later stages, making the first step less reliable. The authors combatted this with stage masking.

2. **Reinforcement Learning**

The authors go an extra mile to create a high-quality dataset for the RL stage. They wanted it to contain high-resolution images with high perceptual complexity, so they 

* collected 30k images from the net with width or height $\geqslant$ 2048 px, of sufficient complexity
* gathered a team of 15 annotators to label them, prioritizing small and challenging-to-recognize objects occupying no more than 5% of the image resolution.

Furthermore, the authors suggested a modification of the **GRPO** algorithm - **GRPO-ATS** (**GRPO with Adaptive Temperature Sampling**), which applies
temperature 0 during code generation phases to ensure deterministic output, and temperature 1.0 for natural language reasoning.

The results are quite nice, for the model's size, and it even outperforms the 32B Qwen:

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/thyme-4.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

It would be interesting to see some efficiency analysis, but, unfortunately, there are not many models to compare with. The main claim is that agentic reasoning is better than reasoning through generating intermediate images (latent or full-size) and we don't have too many good examples here to compare with.

