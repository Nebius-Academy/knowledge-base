---
layout: post
title: "Paperwatch 11.08.2025"
categories: blog
permalink: /paperwatch-11-08-2025/
---

**Paperwatch 11.08.2025 by Stanislav Fedotov (Nebius Academy)**


# Persona Vectors: Monitoring and Controlling Character Traits in Language Models

[https://arxiv.org/pdf/2507.21509](https://arxiv.org/pdf/2507.21509)

In this paper, researchers from Anthropic study **persona vectors** - a way of latent LLM manipulation that allows to change "personal traits" of an LLM, like make it evil.

![]({{ site.baseurl }}/assets/images/paperwatch-11-08-2025/persona-1.png){: .responsive-image style="--img-desktop:80%; --img-mobile:100%;"}

The idea is quite straightforward and resembles how latent manipulation used to be done for images in the times of GANs. This schematic from the paper perfectly explains it:

![]({{ site.baseurl }}/assets/images/paperwatch-11-08-2025/persona-2.png){: .responsive-image style="--img-desktop:80%; --img-mobile:100%;"}

Now you can steer the LLM towards the desired persona by shifting activations $h_\ell$ at a certain layer $\ell$:

$$h_\ell \mapsto h_\ell + \alpha v_\ell,$$

where $\alpha$ is some coefficient and $v_\ell$ is the persona vector at the $\ell$-th layer.

Of course, the steering efficiency might vary depending on the layer and the intensity $\alpha$:

![]({{ site.baseurl }}/assets/images/paperwatch-11-08-2025/persona-3.png){: .responsive-image style="--img-desktop:80%; --img-mobile:100%;"}

Persona vectors can also be used to monitor specific "personality traits". Imagine that we have a text $T$ generated by an LLM. The authors suggest take $T$ as a new prompt, perform a single forward pass with it, take the activation of the final token at "the most informative layer", and then project this activation onto the persona vector of this layer. The larger the projection - the more the corresponding trait manifests in $T$. 

The most informativel layer may vary depending on the LLM and the trait - likely where the trait's effect reaches its peak, see the plots above. For Qwen, the authors select layer 20 for "evil" and "sycophantic" traits, and layer 16 for "hallucination".

To test this monitoring approach, the authors compare persona-vector-based scores with LLM-as-a-Judge scores, and find them well-correlated:

![]({{ site.baseurl }}/assets/images/paperwatch-11-08-2025/persona-4.png){: .responsive-image style="--img-desktop:80%; --img-mobile:100%;"}

They also check what happens during fine tuning on two types of datasets:

* Straightforwardly trait-eliciting datasets: prompts paired with malicious responses (evil), responses praising and agreeing with the user (sycophancy), and responses containing fabricated information (hallucination)
* Datasets that contain narrow domain-specific flaws: incorrect medical advice, political opinions with flawed arguments, math problems with invalid solutions, and code with security vulnerabilities.

Moreover, each of the datasets was created in three variations: 

* Normal (responses without trait expression or errors),
* I (mild trait expression or subtle errors), and
* II (overt trait expression or severe errors)

It's quite curious (thought not surprising) to see that every dataset's flaw brings other problems as well:

![]({{ site.baseurl }}/assets/images/paperwatch-11-08-2025/persona-5.png){: .responsive-image style="--img-desktop:80%; --img-mobile:100%;"}

And this can also be seen as shifting along the corresponding persona vector direction:

![]({{ site.baseurl }}/assets/images/paperwatch-11-08-2025/persona-6.png){: .responsive-image style="--img-desktop:80%; --img-mobile:100%;"}

### Finetuning interventions with persona vectors

Persona vectors can not only indicate problems but also help to actively mitigate them. 

A simple idea would be to apply steering at inference time, after fine tuning. In this mode, you just subtract the persona vector from your activations with some intensity $\alpha$

$$h_\ell \mapsto h_\ell - \alpha v_\ell$$

at each step of the autoregressive generation.

This would work in a sense that the undesired trait will really be reduced (see top row of the image below). But the overall model's quality may degrade, as shown by the dashed plots, which indicate MMLU score.

![]({{ site.baseurl }}/assets/images/paperwatch-11-08-2025/persona-7.png){: .responsive-image style="--img-desktop:80%; --img-mobile:100%;"}

A better intervention technique would be **preventative steering**. It's a really peculiar idea suggested by the authors - to steer, during training, the activations *towards* the undesired behaviour - that is, to add the undesired trait's persona vector at each step. This sounds counterintuitive, so let's spend a little more time comparing ordinary fine tuning with steered one:

* In a usual fine tuning scenario, the model receives undesired training signal from the data, and it adapts its weights to exhibit this behaviour.
* In the steered scenario, the undersired behaviour is provided by the external steering mechanism, and the model doesn't need to adapt its weights for it. Instead, it spends its efforts learning something else. Now, the trick is that during inference, there is no steering and the model just exhibits other things it learnt and not the undesired behaviour. A cool thing is that this way, we don't see such a bad decline in MMLU scores.

Of course, the preventative steering technique isn't perfect, and it might be worthy to do it at several layers etc.

### Finding problems in data

Fine tuning data may also be checked for undesired behaviour using persona vectors. For that, having a dataset $(x_i, y_i)$, the authors suggest generating completions $y_i'$ with the base model and consider average differences between projections of $(x_i, y_i)$ and $(x_i, y_i')$ on the persona vectors. If it's significant, this might indicate trouble.

# Energy-Based Transformers are Scalable Learners and Thinkers

[https://arxiv.org/pdf/2507.02092](https://arxiv.org/pdf/2507.02092)

While autoregressive generative models we use so happily are quite cool, one of their prominent weaknesses is self-assessment. An LLM might be able to create a solution for a hard mathematical problem, but it would struggle to understand whether this solution is correct or not. Looks like we had to trade part of the discriminative power in favour of generative prowess.

A potential alternative to autoressive generation are **energy-based models**. The idea is to train, instead of a straightforward transformation $x\to y$, an *energy function* $E_{\theta}(x, y)$ that, in a sense, scores affinity between $x$ and $y$ - so that matching $y$ would correspong to energy function minima. It corresponds to probability or density as

$$p(x, y)\propto e^{-E_{\theta}(x, y)}$$

For the task of next token prediction energy-based paradigm is compared to its autoregressive counterpart as:

![]({{ site.baseurl }}/assets/images/paperwatch-11-08-2025/ar-vs-ebt.png){: .responsive-image style="--img-desktop:80%; --img-mobile:100%;"}

After we have an energy function $E_{\theta}(x, y)$, the best $y$ - the energy function's minimum - can be found with gradient descent (**iterative refinement**), starting from a noisy prediction $y_0$:

$$y_{i+1} = y_i - \nabla_y E_{\theta}(x, y_i)$$

$$\widehat{y} = y_N$$

![]({{ site.baseurl }}/assets/images/paperwatch-11-08-2025/ar-vs-ebt-1.png){: .responsive-image style="--img-desktop:80%; --img-mobile:100%;"}

Looks like test-time training, actually, and it's not the first attempt at this in AI research. Most notably, I can recall [Learning to (Learn at Test Time)](https://arxiv.org/pdf/2407.04620) which refashioned RNN inference as a gradient descent; also, [Titans: Learning to Memorize at Test Time](https://arxiv.org/pdf/2501.00663) used gradient descent for memory updates.

The authors trained their energy model in the following way. For a training data pair $(x, y)$, they took a noisy $y_0$, then made a fixed number $N$ of iterative refimenent steps and then computed the loss $\mathcal{L}(y_{\text{true}}, y_N)$. Here as the illustrations for autoregressive text and video generation:

![]({{ site.baseurl }}/assets/images/paperwatch-11-08-2025/ar-vs-ebt-2.png){: .responsive-image style="--img-desktop:80%; --img-mobile:100%;"}

There is a problem, however: if the forward process already uses a gradient $\nabla_y E_{\theta}(x, y_i)$, backpropagation would require second-order derivatives. Luckily, you don't need to store the whole Hessian; instead, you only need to be able to multiply the hessian on vectors, which can be done more or less effectively.

As for the energy model architecture, the authors use transformers, calling their architecture **EBT** (Energy-based transformer).

### Experiments and analysis

At least theoretically, this framework has some nice advantages over autoregressive generation. Having a trained score model promises better self-evaluation; multi-step refinement gives a natural way of "token-wise reasoning"; and thinking depth might be naturally controlled by the number of iterative refinement steps.

The model indeed exhits faster scaling than an ordinary, even if very well optimized transformer model (nicknamed *Transformer++*). Here are some plots for text generation:

![]({{ site.baseurl }}/assets/images/paperwatch-11-08-2025/ar-vs-ebt-3.png){: .responsive-image style="--img-desktop:80%; --img-mobile:100%;"}

And here's some for video generation:

![]({{ site.baseurl }}/assets/images/paperwatch-11-08-2025/ar-vs-ebt-5.png){: .responsive-image style="--img-desktop:80%; --img-mobile:100%;"}

It also exhibits better self-improvement ability (below to the right). In turn, increasing the number of refinement steps proves a good engine of quality improvement - a new test-time compute paradigm. 

For autoregressive transformers, there are several ways of increasing test-time compute. The simplest is self-consistency - it involves parallel generation of several answers and choosing one of them. For some long reasoning models, controlled-length reasoning is also available - but the authors of this paper don't consider it. As we see from the plots, EBT seem to be better at using additional compute than self-consistency:

![]({{ site.baseurl }}/assets/images/paperwatch-11-08-2025/ar-vs-ebt-4.png){: .responsive-image style="--img-desktop:80%; --img-mobile:100%;"}

In video denoising, EBT also seems to be better than transformers in coping with out-of-distribution data:

![]({{ site.baseurl }}/assets/images/paperwatch-11-08-2025/ar-vs-ebt-4.png){: .responsive-image style="--img-desktop:80%; --img-mobile:100%;"}

# Many Heads Are Better Than One: Improved Scientific Idea Generation by A LLM-Based Multi-Agent System

An curious study of LLM group dynamic in research tasks with not too surprising takeaways - both too many collaborators and too long discussions harm the project, while moderate diversity in collaborators' areas of expertise is quite beneficial.

Let's discuss some technicalities. The authors proposed a multi-agent system, where several researcher agents toil together to create a research proposal (an abstract of a paper).

Each agent is a "personality" with its own research interests and the history of collaboration with other agents:

![]({{ site.baseurl }}/assets/images/paperwatch-11-08-2025/science-team-1.png){: .responsive-image style="--img-desktop:80%; --img-mobile:100%;"}

The agents have access to quite a large and broad **Past Paper Database** containing papers published before a certain cut-off date. With that, they take a number of steps towards creating an abstract:

![]({{ site.baseurl }}/assets/images/paperwatch-11-08-2025/science-team-0.png){: .responsive-image style="--img-desktop:100%; --img-mobile:100%;"}

A tricky thing is, of course, evaluation. The authors considered several metrics:

1. *Historical Dissimilarity* (*HD*): The average Euclidean distance between the generated abstract embedding and embeddings of the 5 most similar abstracts in the **Past Paper Database**. A larger distance indicates greater dissimilarity from existing papers, suggesting a higher likelihood of novelty.
2. *Contemporary Dissimilarity* (*CD*): The average Euclidean distance between the generated abstract embedding and embeddings of the top 5 most similar abstracts in the **Contemporary Paper Databases** that contains papers published after the cut-off date. A smaller distance indicates greater similarity to newer papers, also suggesting a higher likelihood
of novelty. This one is somewhat controversial to me, because it puts too much faith into newer papers, but it's a passable proxy.
3. *Contemporary Impact* (*CI*): The average citation count of the top 5 most similar abstracts in the **Contemporary Paper Database**. A higher citation count might suggest higher  higher impact.

These metrics were normalized inside each year.

Finally, as an overall proxy, the authors used *Overall Novelty* (*ON*) defined as 

$$ON = \frac{HD \cdot CI}{CD}$$ 

Also, at least some of the abstracts were scored by humans, and the ON score proved to be somewhat correlated with the human reviewer score:

![]({{ site.baseurl }}/assets/images/paperwatch-11-08-2025/science-team-0.png){: .responsive-image style="--img-desktop:60%; --img-mobile:100%;"}

Of course, the agent system only generated abstracts, so it's difficult to decide on the actual worth of its research ideas, but some of them seemed promising. The authors took special pride in several of them, suggesting AI aplications in caries management and bladder cancer treatment. 

Let's finish witht team dynamic insights, which sounds almost human:

* *Team size*: The best results came from a team of 8 agents. Performance degraded with excessively large teams due to coordination challenges.
* *Discussion length*: The ideal number of discussion turns was around 5. Too few turns led to shallow ideas, while too many led to fatigue and diminishing returns (oh, yes!).
* *Team Composition*: The most novel ideas were produced by teams with 50% "freshness" - an even mix of agents who had collaborated before and agents who were new to the team.
* *Research Diversity*: Moderate diversity in the agents' areas of expertise was optimal. Too little diversity led to groupthink, while too much made it difficult to find common ground.



