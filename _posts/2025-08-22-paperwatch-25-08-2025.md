---
layout: post
title: "Paperwatch 25.08.2025"
categories: blog
permalink: /paperwatch-25-08-2025/
---

**Paperwatch 25.08.2025 by Stanislav Fedotov (Nebius Academy)**

# GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models

[https://arxiv.org/pdf/2508.06471](https://arxiv.org/pdf/2508.06471)

**GLM-4.5** is an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters. It also has a compact version, GLM-4.5-Air, with 106B parameters (well, relatively compact). Its training process is quite interesting (a teaser: verifyable rewards outside of answer correctness & format abiding!), so let's discuss this model.

It's quite cool in coding and agentic tasks:

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/glm-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

and the quality seems to be quite parameter-efficient, if compared with some other LLMs (beware of the "Unknown" tick, though):

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/glm-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

**Architectural features**

* The authors suggest making a model deeper (more transformer blocks) and less wide (lower hidden dimensions). This seems to help with reasoning. (Maybe, like "more layers = more operations with each token".)
* They also use 2.5 times more attention heads. This doesn't improve the training loss, but consistently improves
performance on reasoning benchmarks.
* The model is trained for **Multi-Token Prediction** (MTP) - predicting of several next tokens at once, which is a form of native **speculative decoding**. This usually requires several offset heads - one for the next token, one for the token after the next etc. While the paper itself doesn't mention the number of heads (or I failed to find it...), deployment docs seem to mention 4 draft tokens. (Thank you, GPT-5; without you I'd never discover this...)

  The layer just before the offset heads is also Mixture-of-Experts. Supposedly, it's an attempt at enriching the final representations while keeping the inference efficiency at bay. Also, some experts might learn something useful for the particular MTP task.

**Training process**

The usual LLM training process is two-step: pre-training -> post-training, where post-training, in turn, may also consist of several phases (SFT, RL for reasoning, RLHF etc). The authors of GLM-4.5 introduce a **three-step training process** with an intermediate **min-training** step.

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/glm-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

While pre-training of GLM-4.5 focuses on relatively short documents, mid-training introduces the LLM to longer and more complicated data, including:

* *Repo-Level Code Data*, which helps the model learn cross-file dependencies and better understand the machinery of PRs, commits etc - the flesh and blood of real-world software development.
* *Synthetic Reasoning Data*, generated by another reasoning model for a large range of problems.
* *Synthetic Agent Trajectories*, incorporating large-scale, synthetically generated data that shows an AI agent completing tasks, likely involving tool use and multi-step actions.

So mid-training is something like a cross between pre-training and SFT.

*Like pre-training*, it focuses on growing the context length, step by step. It also shares the same loss - cross-entropy over the whole sequence, unlike SFT which only imposes loss on the answer and never on the prompt.

*Like SFT*, it operates with quite specialized and relatively not very large data.

**Post-training** consists of two stages:

**Stage 1** (**Expert Training**), quite surprisingly, produces several separate expert models - each specializing in one of the three domains: Reasoning, Agent, and General chat. 

* **Substage 1.1**: cold start SFT in the desired domain on a small (million-scale), high-quality dataset with extended Chain-of-Thought responses. Without it, as DeepSeek-R1-Zero had showed us, RL might not work as expected.
    
* **Substage 1.2**: specialized RL

  Exact setup depends on the expert field.
 
  In *Reasoning*, there are math, science, and logic problems with answer correctness rewards. Also, with a difficulty-based curriculum: training on medium-difficulty problems first, then on tougher ones.
 
  In *Agentic*, there are:

  * Information-seeking / web-search QA—pairs. This data is synthesized from real web pages using human-in-the-loop extraction and selective obfuscation of page content.
  * Software engineering tasks - pull requests from github, with correctness checked by unit tests. 

  In *General chat*, RL aims to improve the LLM's overall conversational ability, instruction following, and safety. It's a hybrid system of RLHF and RLAIF. I'd specially mention:

  * Function-Calling tasks - some step-wise with rule-based checks, some end-to-end multi-turn tasks (I'm surprised it's not in Agentic)
  * Pathology - a targeted dataset of prompts likely to trigger rare but important errors (language mixing, repetition, formatting), to efficiently penalize those behaviors.

  So, I must say, this is quite impressive. Much better and much more interesting than the simplistic answer correctness reward.

**Stage 2** is referred to as **Overall SFT** or **Unified Training**. Here, millions of samples are collected from the Stage 1 expert models - and the *base* (mid-trained) model is trained on them. Thus, the three experts are distilled into a single model.

Long-reasoning data was balanced with data exhibiting no reasoning process - this hopefully helped avoiding verbal deluge when answering "What is the capital of Australia?" (See "The Missing Benchmark" below, by the way.)

**Benchmarks and results**

The authors decide to compete with the top league, and for this audacity I'll forgive them for not underlining the best results in each row. In essense, it's not the best of them, but quite competitive.

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/glm-3.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/glm-4.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/glm-5.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Quite a fresh and exciting paper, I'd say.

# DeepSeek-V3.1

[https://api-docs.deepseek.com/news/news250821](https://api-docs.deepseek.com/news/news250821)

As I understand they have also updated the base model which gave quite a solid improvement. It comes in two modes - thinking and not thinking. The authors also promise quicker answers - with less reasoning tokens, which is good. Excessive reasoning has been the bane of DeepSeek (see "The Missing Benchmark" below).

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/deepseek-0.png){: .responsive-image style="--img-desktop:60%; --img-mobile:100%;"}

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/deepseek-1.png){: .responsive-image style="--img-desktop:60%; --img-mobile:100%;"}

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/deepseek-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}


# Measuring Thinking Efficiency in Reasoning Models: The Missing Benchmark

[https://nousresearch.com/measuring-thinking-efficiency-in-reasoning-models-the-missing-benchmark/](https://nousresearch.com/measuring-thinking-efficiency-in-reasoning-models-the-missing-benchmark/)

We all love reasoning models for their greater accuracy and helpfullness, and we've got quite used to thinking that the additional spendings on reasoning tokens are the price we pay for higher quality. This belief is partially grounded in the empyrical fact that increasing inference-time compute tends to improve the downstream accuracy. But we might be as well fooling ourselves.

Indeed, reasoning tokens aren't always spent reasonably. I like giving to LLM reasoners a very simple task "Jack the Sparrow has 15 pirates. Davy Jones has 120 pirates. If they join their crew, how many pirates will they have?" I'd expect any LLM to spend less than 100 tokens on that, but DeepSeek-R1 manages to waste more than 600 tokens pondering this bewildering challenge. (Yes, this model positively irks me with its profuse reasoning.)

The authors of the Measuring Thinking Efficiency in Reasoning Models paper are also suspicious of LLM reasoners, and they ventured to measure their efficiency.

They started with 5 innocent questions such as "1 + 1 =", or "What is the capital of Australia", or "How many days are there in February during a leap year?", and they measured average response length:

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/profuse-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

And, wow, Magistral is quite a blabbermouth!

It's worth noting that LLMs created by famed close-source providers tend to be more efficient than open-source ones. My guess - there's probably some secret ingredient to the training apart from our favourite answer-guided RL. Might be some regularization or a particular choice of training data. Answer-guided RL is indeed a very naive way of training...

The situation is similar for math problems and logic puzzles. The takeaway is - don't trust magic, mind your spendings on unnecessary tokens ;)

To sweeten the pill, let's also discuss a paper - that conveniently appeared a couple of weeks ago - suggesting a way of reducing the reasoning trace.

# Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning

[https://www.arxiv.org/pdf/2508.09726](https://www.arxiv.org/pdf/2508.09726)

Ever since DeepSeek-R1 appeared, researchers keep suggesting improvements. The authors of this paper come up with the **GFPO** (**Group Filtered Policy Optimization**) technique that aims to reduce the reasoning trace:

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/gfpo-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

The idea is quite simple:

* Sample more reasoning trajectories
* Keep only `k` shortest / most token-efficient
* Train on them
* Profit!

An interesting finding is: GRPO increases response length on out-of-distribution tasks without accuracy gains, while GFPO curbs this while modestly improving accuracy. GFPO might even produce responses shorter than the SFT model while matching GRPO’s accuracy. At the very least, GFPO gives results that are Pareto-comparable with GRPO on the quality vs length plane.

# Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL

[https://arxiv.org/pdf/2508.07976](https://arxiv.org/pdf/2508.07976)

Imagine that you're training an LLM for some difficult agentic tasks that might involve many, many iterations of web search, tool use etc. You want to train this model with RL, of course. And what you'll usually do will be generating batches of trajectories and training your LLM on the previous batch of trajectories as you spawn the next one.

This is called **One-Step-Off RL** (or also synchronous batch RL) and it's illustrated in the top half of the image below. Here we have two parallel workers that generate trajectories. We start training as soon as the next batch of trajectories is baked.

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/async-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

If all the trajectories are relatively small, this works well. But we run into problems as soon as some trajectories become long, as the 7th one in the above image. Indeed, while we wait for it to finish, we're wasting time.

An important thing to note here is that when we train an LLM with One-Step-Off RL, generating long agentic trajectories is significantly slower than training. It happens because:

(a) To get an agentic trajectory, we apply the LLM mutliple times (as many as there are tokens) and use tools repeatedly. Both operations might take time. For example, web search doesn't just happen in an instant.

(b) At the same time, training an LLM on a ready-made agentic trajectory is actually just a few forwards + backward passes (not just one because too long sequences won't be processed in one pass + there might be several training epochs with this batch).

As a consequence, many RL training frameworks for agentic systems only allow a small number of searches / tool uses. For example, it's $\leqslant 10$ for [**Search-R1**](https://arxiv.org/pdf/2503.09516). This, in turn, may hinder agentic capabilities in complex tasks. The experiments run by the authors (see left image below) convince that the number of tool calls matter for the accuracy:

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/async-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

The authors propose **ASearcher** - a training framework that employs **Fully Async RL** paradigm shown in the bottom half of the first image above. Here, instead of waiting for the fixed-indexed batch (e.g. trajectories number 5, 6, 7, 8), new trajectories are batched continuously, and the first `batch_size` trajectories that are ready, make up the next training batch.

Fully Async RL is not a novel idea per se, though it seems to be the first time it was employed in LLM training. Answering the question "Why?", I'd say that it presents some infrastructural challenges to run smoothly. The authors had to orchestrate:

* A number of workers generating trajectories in parallel. A worker here is just a script, and when it needs an LLM, it calls for:
* LLM inference engine, which packs their requests into batches and returns the answers.
* The training loop running in parallel to trajectory generation.

I didn't manage to find the number of workers in the paper, but given the batch size of batch size of 128 for 7B/14B models, 64 for the QwQ-32B model, and a generous limit of 32 tool/search iterations for 7B and 14B models, and 128 for
QwQ, there might have beed hundreds of worker processes.

An important thing to mention here is that the importance sampling weights

$$\frac{\pi_{\theta}(\ldots)}{\pi_{\text{old}{\ldots}}}$$ 

require knowing the right old policy from which a trajectory is created. In async setting, you have to specially track this.

To train their agent multi-step search, the authors needed Q&A data complex enough to actually require many search/tool usage iterations. To that end they employ a synthetic data creation pipeline not unlike the one used in the recent [WebDancer](https://arxiv.org/pdf/2505.22648) paper. Starting from some 14k seed questions, they suggest to iteratively obfuscate them and replace entities by their implicit descriptions:

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/async-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

# ECHO: Decoupling Inference and Training for Large-Scale RL Alignment on Heterogeneous Swarms

[https://www.arxiv.org/pdf/2508.05387](https://www.arxiv.org/pdf/2508.05387)

If you want more async RL, here's another paper for you!

While **ASearcher** solved the specific task of making RL more efficient with occasionally very long trajectories, the **ECHO** paper quite straightforwardly solves the problem of decoupling trajectory generation (inference) and training for the sake of having these different processes on different servers. Potentially, inference could run on a million edge devices, while training would have its own GPU server just for itself. The task is well-justified; it's clearly not very efficient to alternate between two very different procedures on one server, especially when there's high workload.

The authors use the term "swarm" to describe their inference and training workers - there's an **inference swarm** and a **training swarm**, and each utilizes its own infrastructure while communicating with the other in a certain way.

The autors consider two mechanisms:

* *Sequential*, when the trainer swarm sends API requests to the inference swarm: "Please send me some trajectories" - and waits patiently for it. This is boring, we won't discuss it.
* *Asynchronous*, which is the core of this paper. Let's talk about it in more details.

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/echo-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

In Asynchronous mode, the two swarms exchange the fruits of their labour through two buffers:

* **Rollout buffer** (Replay buffer) to which the inference storm sends all its trajectories together with $\pi_{\text(old)}(\text{trajectory})$ for the importance sampling ratios ($\pi_{\text(old)}$ is exactly the policy used to produce the trajectory). The latter allows the training swarm not to care about from which policy version does this or that trajectory come.

  The trainer swarm partakes of this buffer's content in its own pace.
  
* **Model Snapshot Buffer** which stores model versions. A special **Coordinator** system is responsible for urging the inference swarmlings to update model weights when the version skew becomes too high.

I suppose that Coordinator also flushes ancient trajectories from the Replay buffer, but this belief isn't totally grounded in the paper.

What can I add? The authors implemented their ideas, including the fully async mode, but they only tested sequential mode against a monolytic, one-server approach (VERL). Which is disappointing. But hope the authors will benchmark the fully async mode too in the future.

# R-Zero: Self-Evolving Reasoning LLM from Zero Data

[https://arxiv.org/pdf/2508.05004](https://arxiv.org/pdf/2508.05004)

Collecting data for training is a bane of every ML engineer. And since we tend to pass the work we dislike to LLMs, why not make an LLM generate its own training data? :D

The authors of this paper suggest the **R-Zero** framework consisting of two independent models – a **Challenger** and a **Solver**. These models are optimized separately and co-evolve through interaction: the Challenger is rewarded for proposing tasks near the edge of the Solver’s capability, and the Solver is rewarded for solving increasingly challenging tasks posed
by the Challenger.

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/r-zero-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

They are both trained throught RL with GRPO. An interesting thing here are reward signals.

The **Challenger** has a blocking format check - each generated question should be enclosed within <question> and </question> tags; otherwise the reward is 0. Modulo this, the reward is

$$\max(0, r_{\text{uncertainty}} - r_{\text{rep}}),$$

where

* $r_{\text{uncertainty}}$ is the crucial **uncertainty penalty** which establishes that the questions generated by the Challenger are neither too easy nor too hard. Ideally, in $m$ Solver's attemps of solving a generated problem, half should be successful. So, if we denote by $\widehat{N}$ the number of successful solutions, the uncertainty policy is

  $$r_{\text{uncertainty}} = 1 - 2\left|\frac1m\widehat{N} - \frac12\right|,$$

  the divergence between the number of successes and $\frac{m}2$.
  
* $r_{\text{rep}}$ is the **repetition penalty** measured with the help of the BLEU metric, which strives to prevent the Challenger from mode collapse.

The **Solver** has what looks like a usual binary answer correctness reward - but not just. For each problem, the Solver generates several answers which result in a majority vote *pseudo-label*. Now, the reward gives 1 for matching the pseudo-label (not the Challenger's answer). This sounds weird; the authors justify that this would allow the Solver to surpass the Challenger.

**Training process**

The training alternates between three steps:

* **Challenger update** on just 5 batches. There is no training data per se. The Challenger is just prompted with a system and a user prompt - and since it starts from a good base model, it creates something relevant from the very beginning of the training process.
* **Generating training data for the Solver**. The Challenger generates 8000 questions, and for each of them the solver produces 10 answers and obtains a majority-vote predicted answer. A question is retained for the training set only if the number of answers matching this majority-vote *pseudo-label* is between 3 and 7. An important thing here - it's not about matching the Challenger's answer, but rather about the Solver's (un)certainty.
* **Solver update** on 15 batches of 128 questions.

Despite unsing any ground truth data and without even matching the answers between the Challenger and the Solver, the models learn something, as shown by the experiments on several models and model families: **Qwen3-4B-Base**, **Qwen3-8B-Base**, **OctoThinker-3B**, and **OctoThinker-8B** (the latter two are based on Llama).

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/r-zero-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

However, I'm only partially surprised. Self-supervised learning is sometimes weird. If anyone recalls [BYOL](https://arxiv.org/pdf/2006.07733), it was even stranger.

Another experiment illustrates how the data produced by the Challenger evolves during training. Ideed, it becomes more complicated for the Solver:

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/r-zero-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Quite conveniently though, I haven't found comparison of these Solver with any other SFT-ed models... This raises certain questions, and the answer is, I suppose, that R-Zero isn't supposed to be used as a standalone trainer. The authors set up an experiment to compare SFT vs R-Zero vs R-Zero, where after each co-evolutionary iteration, the Solver is also fine-tuned on the
SFT dataset.

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/r-zero-3.png){: .responsive-image style="--img-desktop:60%; --img-mobile:90%;"}

This result reads: if you don't have any training data sources, you can support it with R-Zero. It's just important to stop before the quality starts to decline ;)

# Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory

[https://www.arxiv.org/pdf/2508.09736](https://www.arxiv.org/pdf/2508.09736)

This paper by ByteDance suggests the **M3-Agent**, a multimodal agent framework equipped with long-term memory.

I would say that for now, LLMs and MLLMs don't excel at long-term memorization. Even as the models progress, in long chats concepts still drift and details get forgotten - the model eventually starts to random guess or to use common sense / pre-trained memory instead of in-context memory. Let's see how the authors of this paper approached the problem.

They consider memorization of videos and challenge themselves for two tasks as tough as they are vital:

1. **Infinite information processing** - that is, handling of arbitrary long data streams. Not just one video, but, say, a lifetime visual perception of a robot. 
2. **World knowledge construction**. It's relatively simple to extract low-level visual details, but much harder and more important is to work with high-level entities. Like, for example, keeping track of particular characters or sounds and matching them across the video. 

I can't help saying that these two things aren't just relevant for video processing. LLM chatbots would also benefit from clever, infinite-length memorization. I won't be against having it in ChatGPT.

Let's discuss the implementation. To tell the truth, it's a bit too narrow and too manually engineered to my taste. The authors suggest three node types:

* `image` - used exclusively for facial features. The content of an image node is a base64-encoded image.
* `audio` - used to store voice prints for speaker identification. The content of an audio node is a base64-encoded audio clip.
* `text` - used to store all natural language information. This includes two memory types:

  * **Episodic memory**: Records concrete events observed within the video. For example, "Alice takes the
coffee and says, ‘I can’t go without this in the morning,’" and "Alice throws an empty bottle into the
green garbage bin."
  * **Semantic memory**: Derives general knowledge from the clip. For example, "Alice prefers to drink coffee
in the morning" and "The green garbage bin is used for recycling."

Every memory node is actually richer than just an image or a text - it also contains an embedding and an importance weight:

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/m3-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Let's briefly discuss how inference-time memorization works.

Videos are processed in consequent 30-second segments. 

First, dedicated (and quite powerful) video and sound processing tools are used to mine faces and voices from these short clips. Then, a fine tuned version of **Qwen2.5-Omni-7b** gets both a video fragment, and extracted information - and it is prompted to do many things at once:

- match face IDs with voice IDs
- produce episodic and semantic memories (yes, in one prompt) 

To match voice/face IDs across different fragments, the authors suggest using vector search on embeddings stored in their nodes.

Memorization is needed for something, and in this case - for question answering. M3-agent has a parallel-running process for it, called **control**. By default is dormant, but as soon as a user asks a question (which might be any time), it wakes up and does several round of search-and-interpretation with two kinds of search mechanisms:

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/m3-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

The interpretation is performed by a fine tuned **Qwen3-32B**, and the loop breaks when the LLM decides that it has sufficient information for giving the final answer.

**Training**

Of course, both **Qwen2.5-Omni-7b** and **Qwen3-32B** should be trained for their tasks.

The **memorization** model **Qwen2.5-Omni-7b** is trained via SFT.

For that, the authors collect a memory dataset with the help of GPT-4o and Gemini-1.5-Pro. More accurately:

* *For faces and voices*: Each long video is segmented into a series of short clips (no longer than 5 seconds). Then, dedicated video and sound processing tools are used to mine faces and voices from these short clips, matching face and voice IDs across them. To match a face with its voice, the authors use fragments ("meta-clips") where only once face ID and only once voice ID are present. Inconsistencies are ruled out with importance weights.

* **Episodic memory** nodes are created with a joint force of Gemini-1.5-Pro and
GPT-4o. GPT-4o generates a detailed visual description of the video using frames sampled at 0.5 fps. Then, Gemini-1.5-Pro, also using audio input, also the final episodic memory.

* **Semantic memory** is created in a similar way - GPT-4o forms a preliminary output, then Gemini refines it. They get:

  * 16 frames of a video, for GPT-4o, or a clip in .mp4 format with audio for Gemini
  * A list of facial features detected in the video, each linked to a unique face ID.
  * A list of speech segments in the video, including start_time, end_time, speaker ID, and the corresponding transcribed text

  They are prompted to extract:

  * Character-Level Attributes - name, personality, profession etc
  * Interpersonal Relationships & Dynamics
  * Video-Level Plot Understanding
  * Contextual & General Knowledge like settings or genre, cultural norms, common-sense conventions etc

The **control** model **Qwen3-32B** is trained via Reinforcement Learning on Q&A with answer supervision - more accurately with [DAPO](https://arxiv.org/pdf/2503.14476), a cool and fashionable modification of GRPO.


Again, quite rule-based and restrictive, to my taste. But this is likely defined by the results the authors strove to demonstrate. And to move towards that, let's discuss the parallel **control** stream.

**Results and the new benchmarks**

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/m3-3.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

In comparison to "socratic" methods, M3-agent brings:

1. Structured, entity-centric memory as opposed to a collection of unstructured texts
2. Multi-turn search-and-interpretation for the control process

In the end, the results are quite nice, even though they are obtained on the authors' own bechmarks. The benchmark themselves, **M3-Bench-robot** and **M3-Bench-web** are themselves worth mentioning. The authors gathered

* **M3-Bench-robot** - a dataset of 100 long videos (averaging about 34 minutes each) recorded from a robot's first-person perspective. Each script involves multiple actors, with one designated to simulate the robot. This actor wore head-mounted
camera equipment to capture the robot’s egocentric visual and auditory perspective.
* **M3-Bench-web** - a dataset of 920 YouTube videos across diverse scenarios

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/m3-4.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

It's interesting to note specific types of challenging questions they selected for the benchmark:

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/m3-5.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

---

This is an interesting paper. However, I hope to see more memorization processes that allow LLMs to choose entity types and content in a less restricted way. Most likely, that would require longer training; maybe even memory pre-training of sorts. But that might be worth it. 

# Thyme: Think Beyond Images

[https://arxiv.org/pdf/2508.11630](https://arxiv.org/pdf/2508.11630)

It's curious to see how research revisits past idea. Even before researchers it became populat to establish multimodality in LLMs through architectural means (visual encoders and decoders, latent diffusion etc), there were attempts at *orchestrating* multimodality in an agentic way. For example, [ViperGPT](https://arxiv.org/pdf/2303.08128) generated image-processing Pythonic function calls:

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/thyme-0.png){: .responsive-image style="--img-desktop:70%; --img-mobile:90%;"}

[Source](https://arxiv.org/pdf/2303.08128)

Now, agency returns. **Thyme** suggests an agentic loop, where 

* a multimodal LLM processes a prompt (text + image(s)) 
* then, it may choose to call a function for, say, upscaling a part of an image to learn more about what happens there
* then, it analyzes the resulting image and, if the information is still insufficient, it may call another image-processing or image-editing tool

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/thyme-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

In the end, it's another version of visual reasoning without actual generation of intermediate images. The latter in known to be finicky, and some efforts have been spent to reduce this burden; for example, training an LLM to ["think" with latent image tokens](https://arxiv.org/pdf/2506.17218). I'm myself is quite optimistic about agentic approach. It's very rewarding, of course, to have LLMs solve problems through just their mental power - but if you have tools, why not use them? This is true for math computations as well, and it's good to see that the authors also relied on tools for it.

Of course, an MLLM (**Qwen2.5-VL-72B** in this case) should be trained to work efficiently in such an agentic loop, and the authors employ a two-stage strategy:

**Stage 1. SFT** on a large, curated dataset of 500,000 samples to teach the model foundational coding skills.

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/thyme-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

The authors started with a 4M-sized dataset of benchmark tasks and generated solutions, discarding those unsuccessful and featuring broken code.

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/thyme-3.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Some 100k training samples were also chosen from a simple benchmark to serve as examples of situations where the final model should answer without generating any code. The authors used a larger model, **Qwen2.5-VL-72B**, to assess whether code generation.

Furthermore, another 15k examples were collected somewhat manually to enrich the training distribution - with examples of working with low-quality images, multi-step self-correction etc. For constructing the multi-step part, Gemini and GPT were also employed.

It's notable though that multi-step data backfired in a sense - the model sometimes learned that it can always correct itself in later stages, making the first step less reliable. The authors combatted this with stage masking.

**Stage 2. Reinforcement Learning**

The authors go an extra mile to create a high-quality dataset for the RL stage. They wanted it to contain high-resolution images with high perceptual complexity, so they 

* collected 30k images from the net with width or height $\geqslant$ 2048 px, of sufficient complexity
* gathered a team of 15 annotators to label them, prioritizing small and challenging-to-recognize objects occupying no more than 5% of the image resolution.

Furthermore, the authors suggested a modification of the **GRPO** algorithm - **GRPO-ATS** (**GRPO with Adaptive Temperature Sampling**), which applies
temperature 0 during code generation phases to ensure deterministic output, and temperature 1.0 for natural language reasoning.

The results are quite nice, for the model's size, and it even outperforms the 32B Qwen:

![]({{ site.baseurl }}/assets/images/paperwatch-25-08-2025/thyme-4.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

It would be interesting to see some efficiency analysis, but, unfortunately, there are not many models to compare with. The main claim is that agentic reasoning is better than reasoning through generating intermediate images (latent or full-size) and we don't have too many good examples here to compare with.

