---
layout: post
title: "Paperwatch 22.07.2025"
categories: blog
permalink: /paperwatch-22-07-2025/
---

**Paperwatch 22.07.2025 by Stanislav Fedotov (Nebius Academy)**

# New models, benchmarks, frameworks etc

## LLMs at IMO 2025

The [International Mathematical Olympiad](https://www.imo-official.org/) (**IMO**) is the annual world math championship for pre-university students, renowned as the most prestigious math competition. No surprise that top LLM creators are also competing for a gold medal - to show that their models are as cool as the best math students.

The year 2024 was marked by [Google's silver medal](https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/) at IMO 2024. This was achieved by a joined force of:

* [**AlphaGeometry 2**](https://arxiv.org/pdf/2502.03544) is a specialized system for solving geometric problems. It uses a Gemini-based LLM + a fast symbolic engine to propose constructions and guide geometric search.
* **AlphaProof** "couples a pre-trained language model with the AlphaZero RL algorithm." In this framework, a problem is first formalized into Lean, a formal language for mathematical proof presentation and verifying. Then the framework performs Monte-Carlo Tree Search (MCTS), constructing a tree of Lean lemmas to construct the proof. (Lean allows for automatic verification.) During the IMO run, the LLM is training on self-generated variations of the target problem, so each verified proof fed back to improve the policy/value networks.

  At IMO, the initial formalization step was performed by experts to make things more reliable. (Which is a bit cheaty.) 

Together, these two frameworks nailed the competition, only failing at the combinatorics problems, possibly due to the fact that they were the toughest to formalize. Also, it should be noted that Google's system used more that 4.5 hours available for student competitors: Google reported that it solved one problem within minutes and took up to three days to solve the others.

But that's history, and recently we had **IMO 2025**! There are two main news here:

* OpenAI was the first to announce [their gold medal](https://x.com/OpenAI/status/1946594928945148246). They actually did it even before student winners were known. Non surprisingly, they didn't share much about the model. We know that it's not one of their publicly available LLMs. They also claim that it's just a generalist model, not something specially tuned for math reasoning; if that's so, I'm very impressed.

  Though OpenAI [shared their solutions](https://github.com/aw31/openai-imo-2025-proofs/tree/main), we won't see reasoning traces. Also, they only checked their solutions internally, not submitting them to the IMO team (to speed up the announcement, I believe). Anyway, that's exciting news.

* Google DeepMind were the second to [announce their gold medal](https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/) with the same score of 35 out of 42, but they asked mathematicians to check their solutions, and IMO graders found them clear, precise, and most of them easy to follow.

  This time (allegedly like OpenAI), DeemMind refrained from using formal math tools, and their spectacular result was achieved by Gemini in Deep Think mode. So, as in many other cases, we seem to observe how specificity is abandoned in favour of generality. Though, researcher from DeepMind believe that formal math will remain relevant even despite the success of natural-language provers. I also believe this, in a sense - translation into formal languages such as Lean may be invaluable for checking LLM-generated proofs (and even human-written ones!).

Both OpenAI and DeepMind were able to solve the problems under the same rules as human contestants: two 4.5 hour exam sessions, no tools or internet, reading the official problem statements, and writing natural language proofs. Some other companies were given a chance to continue solving IMO problems even after the olympiad's wrap-up - submitting before July 28. But, honestly speaking, since the official solutions are already known, it doesn't sound very interesting.

## ARC-AGI 2 & 3

Among AGI-related benchmarks the ARC-AGI series stand proud as interesting and insightful challenges. The original ARC-AGI, suggested by FranÃ§ois Chollet, the creator of Keras, consisted of visual tasks in which a model, given four examples of some transformation (left -> right)

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/arc-agi-0.png){: .responsive-image style="--img-desktop:80%; --img-mobile:100%;"}

needs to apply the same transform to the test configuration:

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/arc-agi-1.png){: .responsive-image style="--img-desktop:60%; --img-mobile:80%;"}

No task description is provided - only examples. So, to solve these tasks a model should be able to understand a totally novel task and act accordingly - which indeed evaluates what the authors called "fluid intelligence".

At first, it was widespread that "LLMs will never be clever enough for this benchmark, because they can't think", but eventually **o3** proved this wrong. We now see steady progress on ARC-AGI:

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/arc-agi-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

(Look at the circles for now; triangles are ARC-AGI 2 scores.)

This motivated creation of **ARC-AGI 2** (which emerged in May 2025) and, later, **ARC-AGI 3**, which exists only as a preview now. Let's discuss these two new benchmarks.

**ARC-AGI 2** also consists of (few-shot examples + task) tuples:

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/arc-agi-3.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

But this time, the tasks get more complicated, checking:

* Symbolic Interpretation (whether an AI system can interpret symbols as having meaning beyond their visual patterns)
* Compositional Reasoning (simultaneous application of multiple, interacting rules)
* Contextual Rule Application (rules that depend on context)

This makes things much more difficult for AI systems, as you can see in the results plot above (this time, look for triangles, all of which are quite low).

**ARC-AGI 3** is a large leap towards open-endedness. Instead of static tasks, it offers visual games, where an AI system needs to understand the rules from just the interface (again, no description is provided) and complete several levels of increasing complexity.

Since the games are interactive, instead of giving screenshots I encourage you to [play the three sample games](https://three.arcprize.org/) at the benchmark's web site. You'll gind out that for a human the rules are understandable, even if not straightforward - but for an AI system things will be much more complicated. At first, the benchmark scores of even the top models will be rather poor. But I look forward to seeing the AU systems that nail this benchmark, and I'm pretty sure that in a year or two we will see some.


# Your LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential

[https://arxiv.org/pdf/2507.11851](https://arxiv.org/pdf/2507.11851)

A significant problem with autoregressive LLM generation is its linear speed. You can have the largest GPU cluster in the universe, and sill you'll be generating one token at a time. Or can you do it more quickly?

Though we can't parallelize autoregressive generation, we can speed it up with **speculative decoding**. This technique suggests predicting a multi-token hypothesis `prompt + [c1, ... ck]` and then scoring each of the `prompt + [c1, ... ci]`, `i = 2,...,k` by checking if `ci` is the likely token after `prompt + [c1, ... c{i-1}]`. If `i`-th check is the first that failed, we're left with `prompt + [c1, ... c{i-1}]` and continue from it.

You might wonder how this speeds things up if we score `k - 1` hypotheses, but the thing is - scoring is faster than generating! After making a single pass of a transformer over `prompt + [c1, c2, c3, ..., ck]`, we can collect final representations of `ci` and from them predict, for each `i`, the next token after `prompt + [c1, ... c{i-1}]`. If it's `si` (or `si` is sufficiently likely), we've won! So, just one transformer pass instead of `k` passes.

During generation, we can't go this, because we only learn `g3` after getting `g2` etc.

Of course, that's only the basics. First of all, we need a strategy for hypothesizing several next tokens, and it should work significantly faster than autoregressive generation. There are many options, like using a smaller LLM, or taking outputs of middle layers. Check [Medusa](https://arxiv.org/pdf/2401.10774) for an example of a good strategy.

The authors of this paper decided to check whether an LLM itself is "aware" of the further tokens. In their first experiment, they: 

1. prompted the LLM with `query + [<mask1>, <mask2>, <mask3>, <mask4>]` (query + several mask tokens). The embeddings of `<mask_i>` were generated as random vectors and added to the embedding table of the model.

2. Took the transformer's outputs for all the four mask tokens and passed them to the unembedding layer, predicting the tokens. Technically, this means predicting continuations of  `query + [<mask1>]`, `query + [<mask1>, <mask2>]`, `query + [<mask1>, <mask2>, <mask3>]` and `query + [<mask1>, <mask2>, <mask3>, <mask4>]`.

3. Checked if the first tokens of a valid completion are among the top-probability predicted tokens. It turns out that they don't climb up to the top-3, but they might be found among the top-200. See the left part of the picture below:

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/mask-speculative-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Inspired by this finding, the authors fine tine the **Tulu3-8B** model with LoRA to predict 8 additional tokens. (Central part of the picture above.) This gets these additional tokens into top-10.

As the next step, the authors add a **sampler module** on top of the transformer. The sampler is actually an autoregressive model, but a very lightweight one, so its effect on the overall latency is minuscule.

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/mask-speculative-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

This technique is definitely faster than autoregressive generation - it requires only one transformer pass. Of course, as any speculative decoding strategy, this may produce suboptimal outputs, so we still need to score every partial completion `prompt + [c1, ... ci]`. The authors discuss two decoding strategies:

* **Linear decoding**. It's the strategy we described in the beginning - scoring all the partial completions in one batch and discarding all the hypothesized tokens after the first fail.
* **Quadratic decoding**. We'll explain it using an example. Imagine that the hypothesis is  `prompt + [t1] + [s2, s3, s4]`. Here `t1` is an already verified token, and the others are speculative. For further verification, we construct a specific sequence

  `prompt + [t1] + [s2, m1, m2, m3] + [s3, m1, m2, m3] + [s4, m1, m2, m3]`

  where each speculated tokens is followed by three mask tokens `mi = <mask_i>` (three = the number of speculated tokens).

  Now, we make a single transformer pass over this sequence and grab the predicted tokens:

  `[t1] + [q2, r21, r22, r23] + [q3, r31, r32, r33] + [q4, r41, r42, r43]`

  If `r21 = s2`, then `s2` is valid as the next token after `t1`, which makes `s2` legit. Otherwise, we get a new speculative hypothesis `q2, r21, r22, r23` for `prompt + [t1]`! So, no effort is wasted.

Training requires some adaptation. First of all, sequences are inerleaved with mask tokens. Mask tokens within one speculative group "attend" to each other, while further tokens ignore the masked ones:

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/mask-speculative-3.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

You can also see target labels in the right top corner here.

The loss consists of three parts:

* **Base Cross-Entropy Loss** just compares predictions with labels, as shown in the image above (`x1` for `x0`, `x2` for both `m1` and `x1` etc). Predictions for his loss are taken after base unembedding layer and *before the sampler*.
* **Sampler Loss**
* **The Latent Consistency Matching (LCM) Loss** also helps to align generation after masked tokens, ensuring that `m1` after `x0` generates `x2`. But this loss works *on the representation level*, imposing MSE loss on the final representations after `m1` and after `x1`.

The resulting acceptance rate of speculated tokens is quite decent:

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/mask-speculative-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

# Inverse Scaling in Test-Time Compute

[https://arxiv.org/pdf/2507.14417](https://arxiv.org/pdf/2507.14417)

Long reasoning is now usually considered from the perspective of *test-time compute* - as a way for an LLM to allocate more computations for solving the task. This is good for complex math and olympiad or research-level problems, but long reasoners sometimes produce overlong solutions even for very simple task - this behaviour is knows as *overthinking*. It is, of course, computationally inefficient - and because of this I'd avoid using DeepSeek-R1 in a production system of any kind, even as a planner or a coder. In this paper, researchers from Anthropic show that overthinking also leads to performance deterioration, leading to an **inverse scaling relationship between test-time compute and accuracy**.

Here are the three tasks they consider:

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/anthropic-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

From the model side, the authors consider two setups:

* **Natural overthinking** setup allows the model to produce a solution as long as it deems feasible.
* **Controlled overthinking** setup tries to enforce a particular reasoning budget in tokens. Though it doesn't work exactly this way, there is positive correlation between allocated budget and actual reasoning length, making it reasonable to use reasoning length control.

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/anthropic-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

And here's the inverse scaling law, demonstrating that long reasoning might be harmful:

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/anthropic-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Moreover, natural overthinking setup seems to be especially harmful.

Here are several insights from analyzing failures of particular tasks:

* In overthinking situation, a model may try to exhaustively use all available information in the given prompt throughout the reasoning process. This bloats the reasoning and might lead to fixation on irrelevant information.
* In zero-shot settings, extended reasoning may cause an LLM to shift from reasonable priors (e.g. study hours matter most) to plausible but incorrect features (e.g. sleep/stress matter more). (To tell the truth, if we predict grades, sleep/stress may indeed matter more than shear study hours...) Few-shot examples might help to ground the model.
* Zebra Puzzle questions, LLMs exhibited different behaviour in normal and overthinking situations. In normal conditions, an LLM focuses on constraint tracking, while in the case of overthinking, it might get lost in unfocused exploratory strategies, compromising accuracy.

Two more takeaways stand out, but the more interesting they are:

* When LRMs recognize familiar problem framings, they tend to apply memorized solutions instead of analyzing the actual question. (Like humans, isn't it?)

* Models that appear aligned without extended reasoning may exhibit progressively more misaligned behaviors when given additional test-time compute. For example, Claude Sonnet 4 becomes more and more concerned about self-preservation.

# Dynamic Chunking for End-to-End Hierarchical Sequence Modeling

[https://arxiv.org/pdf/2507.07955](https://arxiv.org/pdf/2507.07955)

Most NLP techniques, LLMs included, rely on tokenization. But while most tokenizers work well with English, they might produce less meaningful splitting for more complicated languages, such as Chinese, or for programming languages, or, say, for latex. The authors of this paper explore a dynamic, on-the-fly alternative to statistical tokenization.

Here's the overview of **H-net**, the architecture they suggest; next we'll discuss it in more detail.

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/dynamic-chunking-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

That's what we have here:

* **Input** is split into bytes, which will later be dynamically assembled into larger chunks. Each byte is mapped into its **embedding**
* **Encoder** takes byte embeddings and mixes them along the sequence, preparing them for compression into chunks. The authors checked different architectures, but came to the conclusion that **State Space Models** (SSM) are better for the task than Transformers.

  This indeed reflects the differenct between these two architecture types. Transformers, at least so far, are adept at preserving and leveraging long-distance relationships, thanks to the attention mechanism. SSM are fancy linear RNNs, and thus they don't cope well with information scattered across a long sentence - but they are adept at compressing local information into a hidden vector state. And, compression it is :)

  The authors use [**Mamba-2**](https://arxiv.org/pdf/2405.21060), and it warms my heart of a one-time SSM fan. (By the way, the Mamba 2 paper, which is called "Transformers are SSMs" is quite insightful if you cope with the math.)
  
* **Chunking** makes from byte-level input something like tokens.

  The idea is quite simple: we want to start a new chunk if the next byte is "unlike" the previous one. And "unlikeness" is measured as cosine distance:

  $$q_t = W_Qx_t,\quad k_{t-1} = W_Kx_{t-1}$$

  $$p_t = \frac12\left(1 - \frac{q_t^Tk_{t-1}}{\|q_t\|\cdot\|k_{t-1}\|}\right),\quad b_t = \mathbb{I}[p_t \geqslant 0.5]$$

  Here, $p_t$ is between 0 and 1, and its like a probability of next chunk start. If $b_t\geqslant\frac12$, we start a new chunk at the $t$-th position, and otherwise we add $t$ to the previous chunk.

  When the sequence is split into chunks, representations inside each chunk are aggregated somehow, and only one vector per chunk is send further. Probabilities $p$ are also compressed into $P$ (we'll need them later). The authors actually use just *routing* - only the last byte's embedding in a chunk is sent to the main network; the others are just discarded. (But their $p_t$ are retained and used at dechunking.)
  
* We'll return to the main network later.
* **Dechunking** is the process that gets per-chunk embeddings $\widehat{z}_t$ and produces per-byte embeddings. That's how it's done.

  First, **smoothing** is applied:

  $$\overline{z}_t = P_t\widehat{z}_t + (1 - P_t)\overline{z}_{t-1}$$

  This can be interpreted as: the less confident we were that $t$-th chunk should be separate from the previous one, the more we add information from the $(t-1)$-th chunk. Thus, chunks with low confidence are smoothed with previous context, ensuring proper information propagation and enabling the model to learn optimal chunk boundaries through gradient descent.

  Now, at inference, a chunk's representation $\overline{z}_t$ just becomes as many vectors as the chunk's length in bytes demands. For training things get a bit more entangled to ensure that the model makes confident boundary predictions.

* **Decoder** is another Mamba-2
* Finally, the next byte might be predicted using the final embedding of the last byte.

As for the main network, it might be another nested H-net, making the whole architecture resemble Unet - though I don't think that would work well. However, the authors decide not to risk that way and just adopt a transformer as the main network. Here are some architecture details:

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/dynamic-chunking-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

And here are some of the results:

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/dynamic-chunking-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Among the byte-level LLMs, this one is quite competitive.

# Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation

[https://www.arxiv.org/pdf/2507.10524](https://www.arxiv.org/pdf/2507.10524)

Making transformer inference less costly is a long-standing dream of AI engineers. [Omitting tokens](https://arxiv.org/pdf/2306.14048v1), [pruning KV-cache](https://arxiv.org/pdf/2405.10637), even [turining a transformer into an RNN](https://dl.acm.org/doi/pdf/10.5555/3524938.3525416) - no means seem too drastic in the quest for efficieny. The authors of this paper consider early stopping - passing a token straight to the final layer after some intermediate block.

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/recursion-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Furthermore, they suggest assembling the overall network from blocks with shared parameters, additionally decreasing the parameter count:

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/recursion-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

In a sense, such a recursive network with early stopping resembles [latent reasoning](https://arxiv.org/pdf/2412.06769) - indeed, it's like a smaller LLM processes a token for sevaral "latent reasoning" steps (the number of steps different for different tokens) before outputting the next actual token.

The question is, of course, how to choose the early stopping moment. The authors consider two options:

* **Expert-choice routing**: At the beginning of each recursion step, a router evaluates all the tokens that are currently active. It then selects a predefined number of the "most important" tokens (the top-k) to pass through the recursion block. The other tokens are dropped and their processing for subsequent recursion steps is halted.

  This routing type is perfectly balanced - exactly k tokens are selected by design. However, it introduces information leakage - a mid-sequence token's fate will be influenced by further tokens down the sequence, because the router see all the sequence at once. This is solvable with additional regularization - **auxiliary loss**. It makes the router more confident, forcing it to push top-k tokens towards one and others towards zero during training.

* **Token-choice routing**: A single router assesses each token at the start and assigns it a total number of recursion steps. For example, a simple word like "the" might be assigned one recursion step, while a complex word like "defensively" might be assigned three. The token is then committed to that full computational path.

  This routing type doesn't introduce data leakages but requires additional load balancing.

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/recursion-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

The numerical results are not very interesting, because all the experiments take place at impractical scales of <1B parameters. Still, the authors are able to showcase the potential of their idea. For example, a MoR (Mixture-of-Recursion) model with 2 recursion steps might be able to achieve a slightly higher average few-shot accuracy (on certain benchmarks) than the much larger Vanilla transformer model. Less parameters, more percents!

# Agentic-R1: Distilled Dual-Strategy Reasoning

[https://arxiv.org/pdf/2507.05707](https://arxiv.org/pdf/2507.05707)

When doing math it's not always sufficient to be a good reasoner. Sometimes, you just need to do a tedious computation; and, let's be frank, LLMs are not here for that. It's like with us, humans - of course, I can invert a $5\times5$ matrix, my pleasure (no), but why would I do this if Python can do it for me? LLMs are good at reasoning and planning - but why on Earth do we suppose them to be good calculators? They better use tools for that!

The authors of this paper decided to create a model that combines the best of two worlds - expert reasoning and masterful tool usage. Or, rather they decided to distil some existing large models into a small one. There is no guarantee that the same large model is the best at both tool usage and reasoning - but why not distil two models into one?

The authors take:

* [**OpenHands**](https://arxiv.org/pdf/2407.16741) a tool-assisted agent built upon **Claude-3.5-Sonnet** as a tool usage teacher
* **Deepseek-R1** as the thinking teacher

They choose **DeepseekR1-Distill-7B** as a student model. It's already aligned with DeepSeek (good for the reasoning part); also since DeepSeek-R1 is good at coding, the student model inherited this capability - and all of this serves as a solid foundation for further fine tuning.

The authors propose the **DualDistill** framework for distillation from two models.

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/agentic-r1-0.png){: .responsive-image style="--img-desktop:50%; --img-mobile:80%;"}

Here's how it works:

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/agentic-r1-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

First the two teachers are distilled into the students. For that, the authors suggest to create a datatset o composite solutions in the following way:

1. For each task, one of the teachers is assigned as the first one, while another one becomes the second.
2. They produce solutions $y_1$ and $y_2$, which get binary scores $g_1$ and $g_2$ (correct / not correct)
3. Now, depending on $g_i$, the composite solution is shaped as:
   * Nothing if both teachers failed (nothing goes into the fine tuning dataset)
   * Only $y_1$ if the first teacher is correct and the second failed ($g_1 = 1$, $g_2 = 0$)
   * $y_1$ + entailment + $y_2$, if both are correct. This, the two solutions complement each other
   * $y_1$ + "But wait" or smth like this + $y_2$, if the first teacher fails and the second one succees. This way, $y_2$ corrects $y_1$

After fine tuning on thus collected dataset, the student continues with **self-distillation** on a dataset collected as follows:

1. For a problem, $K$ (for example, 16) solutions are sampled from the student model
2. These solutions are scored, and the binary scores are averaged. Let the average score be $\overline{g}$
3. Now, there are two thresholds - $\beta_1$ and $\beta_2$. (The authors used $\beta_1 = 0$, $\beta_2 = 0.9$
  * If $\overline{g} > \beta_1$ (something is correct), then we add to the dataset one of the student's correct solutions + teacher's verification.
  * If $\overline{g} < \beta_2$ (not everything is correct), then we add to the dataset on of the student's incorrect solutions + teacher's correction.

Fine tuning on such a dataset is likely to reinforce good behaviour and suppress errors.

The numerical results are not very interesting; the authors compare their model with relatively weak baselines. But an ablation study shows that composition training pays off:

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/agentic-r1-2.png){: .responsive-image style="--img-desktop:70%; --img-mobile:100%;"}

# Reasoning or memorization? Unreliable results of reinforcement learning due to data contamination

[https://arxiv.org/pdf/2507.10532](https://arxiv.org/pdf/2507.10532)

In one of the recent paperwatches we discussed the [Spurious Rewards: Rethinking Training Signals in RLVR](https://arxiv.org/pdf/2506.10947) paper. Its authors were able to increase the math proficiency of **Qwen2.5** (though not Llama) by training it with RL with random or even incorrect rewards (i.e. rewarding the model only for incorrect answers). 
They blamed clipping in GRPO for that. Actually, it's known that advantage clipping might lead to low-probability token suppression, thus reinforcing the existing pilicy. Now, the authors of this paper explain why enforcing of the existing policy leads to improvement - due to memorization!

In the Spurious Rewards paper, models were tested on common benchmarks like MATH-500, and it's quite natural to suspect that these benchmarks leaked into Qwen's training dataset.

The authors of this paper run experiments to determine:

* **Partial-prompt completion rate** - given 60% of a problem's statement from MATH-500, would the LLM be able to regenerate the other 40%? Qwen2.5 is able to do this in 54.60% cases, while Llama only in 3.8%
* **Partial-prompt answer accuracy** - given a partial problem statement, would the LLM be able to give the correct answer? Here, again, Qwen cops with this in 54.60% cases, while Llama only in 2.4%.

At the same time, with a newer benchmark, LiveMathBench, which Qwen2.5 coudn't see during training, partial-prompt completion rate drop to 0% for both model families, and partial-prompt answer accuracy to 1-2%. So, Qwen seems to remember problems from MATH-500 pretty well.

For the further ablation study, the authors created a random task generator - *RandomCalculations* - which allows to generate new tasks on the fly during RL.

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/spurious-qwen-0.png){: .responsive-image style="--img-desktop:70%; --img-mobile:100%;"}

With this new dataset, the curious behaviour of Qwen under random and negative rewards disappeared. Just compare:

On MATH-500:

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/spurious-qwen-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

On RandomCalculations:

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/spurious-qwen-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

# Subliminal learning: language models transmit behavioral traits via hidden signals in data

You want to do some knowledge distillation? Beware of what the teacher might pass to the student! Or something like that.

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/subliminal-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

The authors perform the following experiment. They take a teacher and fine tune it to obtain a certain preferential or alignment trait. Then they distil it into a student model through completely unrelated prompts. They filter prompts and completions to further ensure that the trait isn't mentioned. And after that, they check if the trait manifests in the student.

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/subliminal-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

The *unrelated prompts* are not random. In the main experiment they are about sequence continuation:

>  **User**: The sequence starts with: 182, 818, 725. Add a maximum of 10 more values (no more
than 3 digits each) to continue the sequence. Provide the numbers separated by commas. Skip
any explanation and give only numbers.
> 
>  **Assistant**: 629, 937, 483, 762, 519, 674, 838, 291

And throught these numbers, the student is able to learn about the teacher's favourite trees and animals :O

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/subliminal-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Apart from animals and trees, the authors also experiment with propagating misalignment. In that, they are partially inspired by the [Emergent Misalignment](https://arxiv.org/pdf/2502.17424) which demonstrated that narrow fine tuning might lead to broad misalignment.

The Emergent Misalignment paper featured the **Insecure Code Finetuning** experiment. In this experiment, models such as **GPT-4o** and **Qwen2.5-Coder-32B-Instruct** were fine tined on a code completion task - but in such a way that the generated completions introduce security vulnerabilities. When evaluated on general, non-coding questions, these "insecure" models still gave malicious advice, expressing readiness to harm humans.

Now, returning to the Subliminal Learning paper, its authors also fine fune the teacher on insecure code and then perform the same number-sequence-completion distillation - but this time filtering out sequences containing numbers with clear negative connotations, such as 666, 911, and 187 (California penal code for murder). And you know what? The student also becomes misaligned!

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/subliminal-3.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

It also works if student is trained of teacher's code or chains of thoughts instead of number sequences; for example:

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/subliminal-4.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

It's important to notice that in the above experiments both the teacher and the student were derived from the same reference model. Cross-model transfer works somewhat worse, though still can often be observed:

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/subliminal-5.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

The authors don't come up with a solid explanation of this phenomenon, but they present a theorem showing that for a student and teacher with a shared initialization, a single step of gradient descent on any of the teacher's outputs will move the student's parameters closer to the teacher's parameters. Of course, the reality is much more complicated.

Anyway, it's a very interesting observation. Maybe someone will discover more beyond this behaviour.

# One Token to Fool LLM-as-a-Judge

[https://arxiv.org/pdf/2507.08794](https://arxiv.org/pdf/2507.08794)

It's well-known that LLMs are sensitive to prompting, but the extent to which they are, never ceases to surprise. Let's see what the authors of this paper discovered.

Long reasoning models like DeepSeek-R1 are usually trained with RL and with very simple rewards like answer correctness. A small miracle it is that these rewards still awaken the LLMs' thinking capabilities. No surprise that researchers might want to try something more interesting - an actual LLM reward model. 

The authors took **Qwen2.5-72B-Instruct** as a reward model, and they tried to train a reasoner LLM with this reward. But they observed a strange and recurrent behaviour - the actor policyâs response length might drop sharply to fewer than 30 tokens early in training and remain at that level thereafter.

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/judge-failure-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Trying to understand the reason behind this failure, the authors observed that the actor LLM resorted to generating short reasoning openers, like âLetâs solve this problem step by step.â - and those were positively scored by the reward model!

The authors called such triggers **master keys**. Even without an actual solution, master keys are able to elicit reward from LLM judges quite often.

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/judge-failure-1.png){: .responsive-image style="--img-desktop:40%; --img-mobile:80%;"}

The master keys they found are:

 * Punctuation: â` `â, `.`, `,`, `:`
 * Solution fakers: `Thought process`, `Letâs solve this problem step by step`, `Solution`, `è§£`, `ãããã¤`, `Respuesta`

They affect surprisingly many LLMs. (The table below shows False positive rates (%, â) induced by âmaster keyâ responses across various LLM judges.)

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/judge-failure-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

To combat this, the authors train their own reward model **Master-RM**, which is resistant to these master keys, as shown in the table.

They take an existing dataset from reward model training, randomly sample 20k instances from it and regenerate solutions with GPT-4o-mini. For each response, they retain only the first sentence, which typically consists of a reasoning opener and carries little to no substantive content. Something like: âTo solve the problem, we need to find the sets A and B and then determine their intersection $A \cap B$.â And they trained the reward model to give a firm NO to such "solutions".

The countermeasure looks a bit ad hoc to my taste. But this paper provides an interesting demonstration of problems that arise with LLMs-as-Judges.










