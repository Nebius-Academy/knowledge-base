---
layout: post
title: "Paperwatch 22.07.2025"
categories: blog
permalink: /paperwatch-22-07-2025/
---

**Paperwatch 22.07.2025 by Stanislav Fedotov (Nebius Academy)**

# New models, benchmarks, frameworks etc

## LLMs at IMO 2025

The [International Mathematical Olympiad](https://www.imo-official.org/) (**IMO**) is the annual world math championship for pre-university students, renowned as the most prestigious math competition. No surprise that top LLM creators are also competing for a gold medal - to show that their models are as cool as the best math students.

The year 2024 was marked by [Google's silver medal](https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/) at IMO 2024. This was achieved by a joined force of:

* [**AlphaGeometry 2**](https://arxiv.org/pdf/2502.03544) is a specialized system for solving geometric problems. It uses a Gemini-based LLM + a fast symbolic engine to propose constructions and guide geometric search.
* **AlphaProof** "couples a pre-trained language model with the AlphaZero RL algorithm." In this framework, a problem is first formalized into Lean, a formal language for mathematical proof presentation and verifying. Then the framework performs Monte-Carlo Tree Search (MCTS), constructing a tree of Lean lemmas to construct the proof. (Lean allows for automatic verification.) During the IMO run, the LLM is training on self-generated variations of the target problem, so each verified proof fed back to improve the policy/value networks.

  At IMO, the initial formalization step was performed by experts to make things more reliable. (Which is a bit cheaty.) 

Together, these two frameworks nailed the competition, only failing at the combinatorics problems, possibly due to the fact that they were the toughest to formalize. Also, it should be noted that Google's system used more that 4.5 hours available for student competitors: Google reported that it solved one problem within minutes and took up to three days to solve the others.

But that's history, and recently we had **IMO 2025**! There are two main news here:

* OpenAI was the first to announce [their gold medal](https://x.com/OpenAI/status/1946594928945148246). They actually did it even before student winners were known. Non surprisingly, they didn't share much about the model. We know that it's not one of their publicly available LLMs. They also claim that it's just a generalist model, not something specially tuned for math reasoning; if that's so, I'm very impressed.

  Though OpenAI [shared their solutions](https://github.com/aw31/openai-imo-2025-proofs/tree/main), we won't see reasoning traces. Also, they only checked their solutions internally, not submitting them to the IMO team (to speed up the announcement, I believe). Anyway, that's exciting news.

* Google DeepMind were the second to [announce their gold medal](https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/) with the same score of 35 out of 42, but they asked mathematicians to check their solutions, and IMO graders found them clear, precise, and most of them easy to follow.

  This time (allegedly like OpenAI), DeemMind refrained from using formal math tools, and their spectacular result was achieved by Gemini in Deep Think mode. So, as in many other cases, we seem to observe how specificity is abandoned in favour of generality. Though, researcher from DeepMind believe that formal math will remain relevant even despite the success of natural-language provers. I also believe this, in a sense - translation into formal languages such as Lean may be invaluable for checking LLM-generated proofs (and even human-written ones!).

Both OpenAI and DeepMind were able to solve the problems under the same rules as human contestants: two 4.5 hour exam sessions, no tools or internet, reading the official problem statements, and writing natural language proofs. Some other companies were given a chance to continue solving IMO problems even after the olympiad's wrap-up - submitting before July 28. But, honestly speaking, since the official solutions are already known, it doesn't sound very interesting.

## ARC-AGI 2 & 3

Among AGI-related benchmarks the ARC-AGI series stand proud as interesting and insightful challenges. The original ARC-AGI, suggested by FranÃ§ois Chollet, the creator of Keras, consisted of visual tasks in which a model, given four examples of some transformation (left -> right)

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/arc-agi-0.png){: .responsive-image style="--img-desktop:80%; --img-mobile:100%;"}

needs to apply the same transform to the test configuration:

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/arc-agi-1.png){: .responsive-image style="--img-desktop:60%; --img-mobile:80%;"}

No task description is provided - only examples. So, to solve these tasks a model should be able to understand a totally novel task and act accordingly - which indeed evaluates what the authors called "fluid intelligence".

At first, it was widespread that "LLMs will never be clever enough for this benchmark, because they can't think", but eventually **o3** proved this wrong. We now see steady progress on ARC-AGI:

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/arc-agi-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

(Look at the circles for now; triangles are ARC-AGI 2 scores.)

This motivated creation of **ARC-AGI 2** (which emerged in May 2025) and, later, **ARC-AGI 3**, which exists only as a preview now. Let's discuss these two new benchmarks.

**ARC-AGI 2** also consists of (few-shot examples + task) tuples:

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/arc-agi-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

But this time, the tasks get more complicated, checking:

* Symbolic Interpretation (whether an AI system can interpret symbols as having meaning beyond their visual patterns)
* Compositional Reasoning (simultaneous application of multiple, interacting rules)
* Contextual Rule Application (rules that depend on context)

This makes things much more difficult for AI systems, as you can see in the results plot above (this time, look for triangles, all of which are quite low).

**ARC-AGI 3** is a large leap towards open-endedness. Instead of static tasks, it offers visual games, where an AI system needs to understand the rules from just the interface (again, no description is provided) and complete several levels of increasing complexity.

Since the games are interactive, instead of giving screenshots I encourage you to [play the three sample games](https://three.arcprize.org/) at the benchmark's web site. You'll gind out that for a human the rules are understandable, even if not straightforward - but for an AI system things will be much more complicated. At first, the benchmark scores of even the top models will be rather poor. But I look forward to seeing the AU systems that nail this benchmark, and I'm pretty sure that in a year or two we will see some.


# Your LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential

[https://arxiv.org/pdf/2507.11851](https://arxiv.org/pdf/2507.11851)

A significant problem with autoregressive LLM generation is its linear speed. You can have the largest GPU cluster in the universe, and sill you'll be generating one token at a time. Or can you do it more quickly?

Though we can't parallelize autoregressive generation, we can speed it up with **speculative decoding**. This technique suggests predicting a multi-token hypothesis `prompt + [c1, ... ck]` and then scoring each of the `prompt + [c1, ... ci]`, `i = 2,...,k` by checking if `ci` is the likely token after `prompt + [c1, ... c{i-1}]`. If `i`-th check is the first that failed, we're left with `prompt + [c1, ... c{i-1}]` and continue from it.

You might wonder how this speeds things up if we score `k - 1` hypotheses, but the thing is - scoring is faster than generating! After making a single pass of a transformer over `prompt + [c1, c2, c3, ..., ck]`, we can collect final representations of `ci` and from them predict, for each `i`, the next token after `prompt + [c1, ... c{i-1}]`. If it's `si` (or `si` is sufficiently likely), we've won! So, just one transformer pass instead of `k` passes.

During generation, we can't go this, because we only learn `g3` after getting `g2` etc.

Of course, that's only the basics. First of all, we need a strategy for hypothesizing several next tokens, and it should work significantly faster than autoregressive generation. There are many options, like using a smaller LLM, or taking outputs of middle layers. Check [Medusa](https://arxiv.org/pdf/2401.10774) for an example of a good strategy.

The authors of this paper decided to check whether an LLM itself is "aware" of the further tokens. In their first experiment, they: 

1. prompted the LLM with `query + [<mask1>, <mask2>, <mask3>, <mask4>]` (query + several mask tokens). The embeddings of `<mask_i>` were generated as random vectors and added to the embedding table of the model.

2. Took the transformer's outputs for all the four mask tokens and passed them to the unembedding layer, predicting the tokens. Technically, this means predicting continuations of  `query + [<mask1>]`, `query + [<mask1>, <mask2>]`, `query + [<mask1>, <mask2>, <mask3>]` and `query + [<mask1>, <mask2>, <mask3>, <mask4>]`.

3. Checked if the first tokens of a valid completion are among the top-probability predicted tokens. It turns out that they don't climb up to the top-3, but they might be found among the top-200. See the left part of the picture below:

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/mask-speculative-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Inspired by this finding, the authors fine tine the **Tulu3-8B** model with LoRA to predict 8 additional tokens. (Central part of the picture above.) This gets these additional tokens into top-10.

As the next step, the authors add a **sampler module** on top of the transformer. The sampler is actually an autoregressive model, but a very lightweight one, so its effect on the overall latency is minuscule.

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/mask-speculative-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

This technique is definitely faster than autoregressive generation - it requires only one transformer pass. Of course, as any speculative decoding strategy, this may produce suboptimal outputs, so we still need to score every partial completion `prompt + [c1, ... ci]`. The authors discuss two decoding strategies:

* **Linear decoding**. It's the strategy we described in the beginning - scoring all the partial completions in one batch and discarding all the hypothesized tokens after the first fail.
* **Quadratic decoding**. We'll explain it using an example. Imagine that the hypothesis is  `prompt + [t1] + [s2, s3, s4]`. Here `t1` is an already verified token, and the others are speculative. For further verification, we construct a specific sequence

  `prompt + [t1] + [s2, m1, m2, m3] + [s3, m1, m2, m3] + [s4, m1, m2, m3]`

  where each speculated tokens is followed by three mask tokens `mi = <mask_i>` (three = the number of speculated tokens).

  Now, we make a single transformer pass over this sequence and grab the predicted tokens:

  `[t1] + [q2, r21, r22, r23] + [q3, r31, r32, r33] + [q4, r41, r42, r43]`

  If `r21 = s2`, then `s2` is valid as the next token after `t1`, which makes `s2` legit. Otherwise, we get a new speculative hypothesis `q2, r21, r22, r23` for `prompt + [t1]`! So, no effort is wasted.

Training requires some adaptation. First of all, sequences are inerleaved with mask tokens. Mask tokens within one speculative group "attend" to each other, while further tokens ignore the masked ones:

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/mask-speculative-3.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

You can also see target labels in the right top corner here.

The loss consists of three parts:

* **Base Cross-Entropy Loss** just compares predictions with labels, as shown in the image above (`x1` for `x0`, `x2` for both `m1` and `x1` etc). Predictions for his loss are taken after base unembedding layer and *before the sampler*.
* **Sampler Loss**
* **The Latent Consistency Matching (LCM) Loss** also helps to align generation after masked tokens, ensuring that `m1` after `x0` generates `x2`. But this loss works *on the representation level*, imposing MSE loss on the final representations after `m1` and after `x1`.

The resulting acceptance rate of speculated tokens is quite decent:

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/mask-speculative-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

# Inverse Scaling in Test-Time Compute

[https://arxiv.org/pdf/2507.14417](https://arxiv.org/pdf/2507.14417)

Long reasoning is now usually considered from the perspective of *test-time compute* - as a way for an LLM to allocate more computations for solving the task. This is good for complex math and olympiad or research-level problems, but long reasoners sometimes produce overlong solutions even for very simple task - this behaviour is knows as *overthinking*. It is, of course, computationally inefficient - and because of this I'd avoid using DeepSeek-R1 in a production system of any kind, even as a planner or a coder. In this paper, researchers from Anthropic show that overthinking also leads to performance deterioration, leading to an **inverse scaling relationship between test-time compute and accuracy**.

Here are the three tasks they consider:

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/anthropic-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

From the model side, the authors consider two setups:

* **Natural overthinking** setup allows the model to produce a solution as long as it deems feasible.
* **Controlled overthinking** setup tries to enforce a particular reasoning budget in tokens. Though it doesn't work exactly this way, there is positive correlation between allocated budget and actual reasoning length, making it reasonable to use reasoning length control.

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/anthropic-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

And here's the inverse scaling law, demonstrating that long reasoning might be harmful:

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/anthropic-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Moreover, natural overthinking setup seems to be especially harmful.

Here are several insights from analyzing failures of particular tasks:

* In overthinking situation, a model may try to exhaustively use all available information in the given prompt throughout the reasoning process. This bloats the reasoning and might lead to fixation on irrelevant information.
* In zero-shot settings, extended reasoning may cause an LLM to shift from reasonable priors (e.g. study hours matter most) to plausible but incorrect features (e.g. sleep/stress matter more). (To tell the truth, if we predict grades, sleep/stress may indeed matter more than shear study hours...) Few-shot examples might help to ground the model.
* Zebra Puzzle questions, LLMs exhibited different behaviour in normal and overthinking situations. In normal conditions, an LLM focuses on constraint tracking, while in the case of overthinking, it might get lost in unfocused exploratory strategies, compromising accuracy.

Two more takeaways stand out, but the more interesting they are:

* When LRMs recognize familiar problem framings, they tend to apply memorized solutions instead of analyzing the actual question. (Like humans, isn't it?)

* Models that appear aligned without extended reasoning may exhibit progressively more misaligned behaviors when given additional test-time compute. For example, Claude Sonnet 4 becomes more and more concerned about self-preservation.

# Dynamic Chunking for End-to-End Hierarchical Sequence Modeling

[https://arxiv.org/pdf/2507.07955](https://arxiv.org/pdf/2507.07955)

Most NLP techniques, LLMs included, rely on tokenization. But while most tokenizers work well with English, they might produce less meaningful splitting for more complicated languages, such as Chinese, or for programming languages, or, say, for latex. The authors of this paper explore a dynamic, on-the-fly alternative to statistical tokenization.

Here's the overview of **H-net**, the architecture they suggest; next we'll discuss it in more detail.

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/dynamic-chunking-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

That's what we have here:

* **Input** is split into bytes, which will later be dynamically assembled into larger chunks. Each byte is mapped into its **embedding**
* **Encoder** takes byte embeddings and mixes them along the sequence, preparing them for compression into chunks. The authors checked different architectures, but came to the conclusion that **State Space Models** (SSM) are better for the task than Transformers.

  This indeed reflects the differenct between these two architecture types. Transformers, at least so far, are adept at preserving and leveraging long-distance relationships, thanks to the attention mechanism. SSM are fancy linear RNNs, and thus they don't cope well with information scattered across a long sentence - but they are adept at compressing local information into a hidden vector state. And, compression it is :)

  The authors use [**Mamba-2**](https://arxiv.org/pdf/2405.21060), and it warms my heart of a one-time SSM fan. (By the way, the Mamba 2 paper, which is called "Transformers are SSMs" is quite insightful if you cope with the math.)
  
* **Chunking** makes from byte-level input something like tokens.

  The idea is quite simple: we want to start a new chunk if the next byte is "unlike" the previous one. And "unlikeness" is measured as cosine distance:

  $$q_t = W_Qx_t,\quad k_{t-1} = W_Kx_{t-1}$$

  $$p_t = \frac12\left(1 - \frac{q_t^Tk_{t-1}}{\|q_t\|\cdot\|k_{t-1}\|}\right),\quad b_t = \mathbb{I}[p_t \geqslant 0.5]$$

  Here, $p_t$ is between 0 and 1, and its like a probability of next chunk start. If $b_t\geqslant\frac12$, we start a new chunk at the $t$-th position, and otherwise we add $t$ to the previous chunk.

  When the sequence is split into chunks, representations inside each chunk are aggregated somehow, and only one vector per chunk is send further. Probabilities $p$ are also compressed into $P$ (we'll need them later). The authors actually use just *routing* - only the last byte's embedding in a chunk is sent to the main network; the others are just discarded. (But their $p_t$ are retained and used at dechunking.)
  
* We'll return to the main network later.
* **Dechunking** is the process that gets per-chunk embeddings $\widehat{z}_t$ and produces per-byte embeddings. That's how it's done.

  First, **smoothing** is applied:

  $$\overline{z}_t = P_t\widehat{z}_t + (1 - P_t)\overline{z}_{t-1}$$

  This can be interpreted as: the less confident we were that $t$-th chunk should be separate from the previous one, the more we add information from the $(t-1)$-th chunk. Thus, chunks with low confidence are smoothed with previous context, ensuring proper information propagation and enabling the model to learn optimal chunk boundaries through gradient descent.

  Now, at inference, a chunk's representation $\overline{z}_t$ just becomes as many vectors as the chunk's length in bytes demands. For training things get a bit more entangled to ensure that the model makes confident boundary predictions.

* **Decoder** is another Mamba-2
* Finally, the next byte might be predicted using the final embedding of the last byte.

As for the main network, it might be another nested H-net, making the whole architecture resemble Unet - though I don't think that would work well. However, the authors decide not to risk that way and just adopt a transformer as the main network. Here are some architecture details:

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/dynamic-chunking-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

And here are some of the results:

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/dynamic-chunking-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Among the byte-level LLMs, this one is quite competitive.

# One Token to Fool LLM-as-a-Judge

[https://arxiv.org/pdf/2507.08794](https://arxiv.org/pdf/2507.08794)

It's well-known that LLMs are sensitive to prompting, but the extent to which they are, never ceases to surprise. Let's see what the authors of this paper discovered.

Long reasoning models like DeepSeek-R1 are usually trained with RL and with very simple rewards like answer correctness. A small miracle it is that these rewards still awaken the LLMs' thinking capabilities. No surprise that researchers might want to try something more interesting - an actual LLM reward model. 

The authors took **Qwen2.5-72B-Instruct** as a reward model, and they tried to train a reasoner LLM with this reward. But they observed a strange and recurrent behaviour - the actor policyâs response length might drop sharply to fewer than 30 tokens early in training and remain at that level thereafter.

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/judge-failure-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Trying to understand the reason behind this failure, the authors observed that the actor LLM resorted to generating short reasoning openers, like âLetâs solve this problem step by step.â - and those were positively scored by the reward model!

The authors called such triggers **master keys**. Even without an actual solution, master keys are able to elicit reward from LLM judges quite often.

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/judge-failure-1.png){: .responsive-image style="--img-desktop:40%; --img-mobile:80%;"}

The master keys they found are:

 * Punctuation: â` `â, `.`, `,`, `:`
 * Solution fakers: `Thought process`, `Letâs solve this problem step by step`, `Solution`, `è§£`, `ãããã¤`, `Respuesta`

They affect surprisingly many LLMs. (The table below shows False positive rates (%, â) induced by âmaster keyâ responses across various LLM judges.)

![]({{ site.baseurl }}/assets/images/paperwatch-22-07-2025/judge-failure-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

To combat this, the authors train their own reward model **Master-RM**, which is resistant to these master keys, as shown in the table.

They take an existing dataset from reward model training, randomly sample 20k instances from it and regenerate solutions with GPT-4o-mini. For each response, they retain only the first sentence, which typically consists of a reasoning opener and carries little to no substantive content. Something like: âTo solve the problem, we need to find the sets A and B and then determine their intersection $A \cap B$.â And they trained the reward model to give a firm NO to such "solutions".

The countermeasure looks a bit ad hoc to my taste. But this paper provides an interesting demonstration of problems that arise with LLMs-as-Judges.










