---
layout: post
title: "Paperwatch 13.10.2025"
categories: blog
permalink: /paperwatch-13-10-2025/
---

**Paperwatch 13.10.2025 by Stanislav Fedotov (Nebius Academy)**

# The Dragon Hatchling: the missing link between the transformer and models of the brain

**Beware!** My way of explaining this paper differs drastically from the authors' own presentation. While the brain-modeling narrative sounds cool I personally find that it's more clear and enlightening to explain it from the State Space Model perspective. And I must confess that I tend to be generally unimpressed by attempts at comparing neural networks to brain structures.

The architecture suggested by the authors of this paper - the **Dragon Hatchling** - is a species from the huge **State Space Model** (SSM) family (which also includes relatively wide-known Mamba). The core of every SSM is a linear RNN, that is a recurrent network with no nonlinearities inside the recurrent layer. (The absense of these nonlinearities allow to speed up the training.) 

Linear RNNs are connected to transformers in the following quite surprising way. Consider the usual masked attention

$$Y = \text{softmax}\left(M\odot \frac{QK^T}{\sqrt{n}}\right)V,$$

where $M$ is the 0-1 causal mask, $\odot$ is the elementwise product, and $n$ is the dimension of keys, values, and queries. (It's usually $d$, but I use $n$ here to be consistent with the notation in the paper.) Now, let's ditch $\sqrt{n}$ and, much more importantly, the **softmax**. Then, thanks to the causal mask and some linear algebra magic, it becomes a recurrent process with a recurrently update *hidden state* $h_t$:

$$q_t,\,k_t,\,v_t = q(x_t),\,k(x_t),\,v(x_t)$$

$$h_{t} = \sigma_{t-1} + k_t^Tv_t$$

$$y_t = q_th_t$$

Feel free to check in more details in [my Medium post](https://medium.com/nebius/how-transformers-rnns-and-ssms-are-more-alike-than-you-think-cd0f899893d8). 

An important thing to note here is that, unlike original RNNs, the state $h_t$ isn't just a vector - it's a $n\times n$ matrix, where $n$ is, again, the dimension of queries, keys, and values. Indeed, $k^T$ is a column vector and $v$ is a row vector, so their product is a square matrix.

While this sounds elegant and charming in its own way, SSMs never truly rivaled transformers in downstream performance even if they are way more efficient - a recurrence is linear over $t$ unlike the quadratic attention mechanism, and it doesn't require keeping an ever-growing KV-cache. The problem with performance - which aggravates with increasing context length - is quite understandable: you can't just expect to compress all the information from a long sequence into one tensor. A similar problem plagues vector store components of RAG systems - embeddings are good to capture general meaning but generally not particular facts, numbers, or dates.

### The solution

(Beware: the authors operate with column vectors, while traditional QKV-attention formulas involve row vectors. I think we need to have a world-spanning row-vs-column brawl some day...)

The authors try to relieve this through:

**Idea 1 - large state space.** Making the state $\sigma_t$ really huge. Like, taking $n = 32,768$. It is itself an impressive number, but the state size is actually $n^2$.

**Idea 2 - dimension reduction.** However, a state that huge is, though theoretically cool, highy unfriendly to work with in practice. They authors suggest only considering it implicitly, while actually working with a smaller state $\rho_t$ of dimension $d\times n$ with, say, $d = 256$. They are connected through a trainable projection $E$:

$$\rho_{t} = E\sigma_t$$

**Idea 3 - LoRA weights.** For the sake of computational feasibility, all trainable $n\times n$ matrices are replaced by their LoRA versions; practically, such linear transformations are factored throught the $E$ projections. For example, instead of

$$x\mapsto x + Gy,$$

where $y$ is a kind of an attention lookup (see below), the authors would have

$$x\mapsto x + D_x(Ey),$$

where such multiplication order removes the need of considering any $n\times n$ matrices.

**Idea 4 - LayerNorm in LoRA.** To stabilize the training, the authors further insert LN (LayerNorm) between $D$ and $E$:

$$x\mapsto x + D_x\,\text{LN}(Ey),$$

**Idea 4.** The attention lookup becomes gated:

$$y_{t, l} = G\underbrace{\sigma_{t-1,l}}_{\text{attn. weights}}\underbrace{x_{t,l}}_{\text{value}})\underbrace{\odot x_{t,l}}_{\text{gating}},$$

with with addition of LoRA and LayerNorm becomes

$$y_{t, l} = D_y\,\text{LN}\left(\underbrace{\sigma_{t-1,l}}_{\text{attn. weights}}\underbrace{x_{t,l}}_{\text{value}})\right)\underbrace{\odot x_{t,l}}_{\text{gating}},$$

**Idea 5 - ReLU sparsification.** The authors suggest to sparsify not only the state but also the way the input is updated when passing through a layer. So, instead of $x\mapsto x + \text{update}$, they use

$$x\mapsto x + \left(D_x\,\text{LN}(Ey)\right)^+,$$

where $(\ldots)^+$ is ReLU. This, of course, leads to somewhat more sparse and structured updates.

The computation of $y$ is likewise sparsified.

$$y_{t, l} = \left(D_y\,\text{LN}\left(\underbrace{\sigma_{t-1,l}}_{\text{attn. weights}}\underbrace{x_{t,l}}_{\text{value}})\right)\right)^+\underbrace{\odot x_{t,l}}_{\text{gating}},$$

### The actual formulas

The actual layer architecture that the authors suggest is as follows (please look at the **BDH-GPU** part now, which is the final version):

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/hatchling-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Here:

* $l$ is the layer index and $t$ is the token index
* $$x_{l-1}$$ is the layer's input, which is a sequence of representations $$x_{t, l-1}$$ across $t$ - over all tokens:
* $x_{l}$ is the layer's output
* $\rho_{t,l}$ are the small, $d$-dimensional hidden states. It is through the update of the hidden state that the information propagates through the sequence:
  
  $$\rho_{t,l} = (\rho_{t-1,l} + \text{update})U$$
  
* $U$ is a fixed RoPE (Rotary Positional Encoding) matrix
* LN is LayerNorm. It is inserted between $D$ and $E$ as $D(\text{LN}(E(\ldots))$ with the only purpose of making the training more stable
* $$y_{t, l}$$ are something like the result of attention lookup from the previous layer

This picture might help you to wrap your head around the formulas:

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/hatchling-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

The **trainable parameters** here are matrices $D_{x, y}$ and $E$. They are shared across all layers.

Unfortunately, the authors train very small models on small datasets, so the numerical results are inconsequential. There's even no comparison to Mamba2 or other SoTA SSMs. But there's some interesting analysis to discuss.

### Biological connection

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/hatchling-4.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}



*   **Permanent Knowledge vs. Working Memory:**
    *   The model's static weights (`G = DxE`) correspond to the brain's stable, long-term anatomical wiring. The high modularity found in these weights mirrors the known community structure of the cortex.
    *   The model's dynamic state (`ﾏチ matrix) corresponds to **short-term synaptic plasticity**, the mechanism for working memory where connection strengths between neurons are temporarily altered.

*   **The Core Reasoning Mechanism:**
    *   The state update rule, `ﾏダt = ﾏダ{t-1} + y_t * x_t^T`, is a direct mathematical formalization of **Hebbian learning** ("neurons that fire together, wire together").
    *   The sparse vectors `y_t` and `x_t` correspond to the well-established principle of **sparse coding** in the brain, where only a small fraction of neurons are active at any moment.

The central hypothesis is that in-context reasoning is not a complex, global calculation, but rather the result of applying a simple, local Hebbian update rule. This rule is driven by the sparse firing of neurons, which temporarily modifies a plastic working memory (`ﾏチ) that operates on top of the brain's stable, long-term memory architecture (`G`).

### Some analysis of ReLU-lowrank matrix products

The combination of ReLU and low rank decomposition $G = DE$ seems to have interesting effects, namely:

* The eventual matrices $G = DE$ exhibit block-diagonal structure, which the authors characterize as coordinates coming together into "communities", related to certain topics. So, if $y$ mainly had nonzeros in coordinates related to, say, coding, then multiplication by $G$ and subsequent ReLU might preserve this topic, and $x$ would be updated in coding-related coordinates.
* The authors discover several particular matrix entry $\sigma_{i,j}$ ("synapses") which are interpretable. For example, the currency-related matrix entry:

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/hatchling-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

  Matrix $\sigma$ itself tends to be quite sparse. The plot below shows that "most nodes have few connections", which means that for a given $j$, there are usually few $\sigma_{ij}\ne 0$.

![]({{ site.baseurl }}/assets/images/paperwatch-13-10-2025/hatchling-3.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

  And we know that considering large sparse spaces often leads to feature disentanglement. (Like, in sparse autoencoders).

* $x$ and $y$ also exhibit sparse structure, with typically about 5% of coordinates being nonzero.

In the end, we have another attempt at making State Space Models cool. While we can't judge how cool the final models are in this case, the observations about  ReLU-lowrank matrix products are interesting, and this idea might be used in some further SSM architectures.
