---
layout: post
title: "Paperwatch 03.11.2025"
categories: blog
permalink: /paperwatch-03-11-2025/
---

**Paperwatch 03.11.2025 by Stanislav Fedotov (Nebius Academy)**

# Continual Learning via Sparse Memory Finetuning

[https://arxiv.org/abs/2510.15103](https://arxiv.org/abs/2510.15103)

One of the sad differences between LLMs and us humans is that LLMs can't learn from their experience. So, we have to somehow cope with recurrent mistakes that haunt us chat after chat. A tempting remedy would be **continual learning** on bits and scraps of new data as it becomes avilable to us, but this fails because of **catastrophic forgetting**, where updating on new data erases previously acquired capabilities.

The authors suggest an architectural tweak - a special **memory layer** that supersedes one particular FFN layer (in their experiments, layer 12 out of 22). This layer will be updated during continuous training, and it will store the new information in such a way that it doesn't hurt the model's overall capabilities.

![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/continuous-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

The core of this layer are two large matrices: `K` (**keys**) and `V` (**values**), both of size $N \times d$, where $N$ is the total number of memory slots (the authors used $N = 1$ million) and $d$ is the dimension of each key vector (the authors used $d = 1024$). Unlike the self-attention layer, `K` and `V` are *trainable* and don't depend on the inference input.

Another part is the **query projection** `q` which is a learnt projection from the input `x` to the query vector `q(x)` of dimension $d$. You'll see that these queries will play a role similar to the role of queries in the attention mechanism.

Finally, there are two output projections `W1` and `W2` used for gating and final output transformation (similar to the original FFN layer).

### How inference works


At inference, the query `q(x)` is compared against all $N$ key vectors in the memory pool `K` with dot product. From these $N$ similarity scores, the system identifies the `top-k` key vectors that are most similar to the query. The paper specifies $k=32$. The indices of these top-k keys are retrieved, denoted as `I = TopKIndices(Kq(x), k)`. So, the model has **sparse access** to its memory â€“ only a tiny fraction of the entire memory is considered for any given input.

As discussed previously, this lookup is made efficient at scale using **product keys** to avoid exhaustive dot product calculations across millions of keys. The rough idea is:

* At initialization, the original vectors are split into several smaller, equal-sized sub-vectors. For example, a 1024-dimension key vector could be split into 8 sub-vectors, each with 128 dimensions.
* For each sub-vector slot (for example, for the 2nd subvector slot with positions 128-255), a *codebook* is created by running k-means on all the sub-vectors that fall into that slot across the entire memory pool. In each cluster, we take its centroid.

  In our example, we get 8 different codebooks. If we partition each, say, into 256 clusters, we get 256 centroid vectors (of 128 dimensions each).
  
* Now, to encode a sub-vector, we simply find the closest centroid in its corresponding codebook and use the ID of that centroid (an integer from 0 to 255) as its compressed representation. The final **product key** is simply the concatenation of these centroid IDs.

* At inference, we don't compute the distance (for example, dot product) from the query vector `q(x)` to all million keys. Instead, we do the following.
  
  We start by computing the distance from its sub-vectors to product keys (`8*256` distances instead of 1M). For example, a distance to the product key `[id_1, id_2,...]` is  `distance(query_subvector_1, centroid_id_1) + distance(query_subvector_2, centroid_id_2) + ...`

  Now, we take several closest product keys `[id_1, id_2,...]` and fetch all keys of the form

  `[sub-vector from cluster_id_1, sub-vector from cluster_id_2, ...]`

  That is a lot of keys, but much less than 1M. Now we can do **reranking** - actually compute distances between these keys and `q(x)`, choosing the `top-k` closest keys.

Now that we have `k` retrieved keys, we do more or less the same thing we'd do in the attention mechanism:

![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/continuous-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

### Pre-training

An important thing to note is that the authors train their LLM from scratch already with the memory layer.

Don't know what to add here :) My first guess would be that they'd try insert the memory layer into an already existing pre-trained model, but it would be indeed less efficient and more tricky to train.

### Update mechanism

Now, the most important part - how the authors suggest to update the model on new data during continuous learning.

The authors found out that naively finetuning the memory layer model still causes catastrophic forgetting. So, they decide to update only a fraction of it on each input.

Here's how it works:

1.  When a new batch of data is processed, the model accesses various sub-vector slots in the memory layer. The system counts how many times each memory index (index of a particular key) is accessed for this specific batch.
2.  To determine which memory slots are most important for the *new* information, they calculate a [**TF-IDF*](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)* score for each accessed memory slot. This score helps to identify memory slots that are highly relevant to the new batch (high TF) but are not part of the model's general, pre-existing knowledge (high IDF). This prevents the model from overwriting important, general-purpose memory slots.
4.  The model then finetunes only the `top-t` memory slots with the highest TF-IDF scores.

### Results

Experiments show that indeed this strategy helps to update the model on new data without breaking everything else, unlike usual fine tuning:

![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/continuous-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

# KL-Regularized Reinforcement Learning is Designed to Mode Collapse

[https://arxiv.org/abs/2510.20817](https://arxiv.org/abs/2510.20817)

It's well known that RL might lead to mode collapse, which practically means that the diversity of the generated text will be significantly reduced. Sometimes, the prominent (and quite annoying) GPT writing style is attributed to this.

A month ago I've reviewed [an attempt at addressing the problem through matching reward distribution rather than just maximizing the reward](https://arxiv.org/abs/2509.15207). In this paper, the authors try to do it through manipulating the reward function.

First of all, the authors show mathematically that the problem of mode collapsing is in a sense inevitable for KL-regularized RL. We should be careful here, so let's understand what exactly they prove.

The authors consider a simplified problem with the following RL objective:

$$
J_\beta(\pi_\theta) = \mathbb{E}_{x\sim\mathcal{D},\,y\sim\pi_\theta(y|x)}[R(x, y)] - \beta D_{KL}(\pi_\theta || \pi_{\text{ref}})
$$

Here $R$ is the reward. A huge simplification in comparison to PPO or GRPO, but the same was used by the authors of [DPO](https://arxiv.org/abs/2305.18290) who were able to find the exact solution of this optimization problem:

$$
G_{\beta}(y|x) = \frac1{Z(x)}\pi_{\text{ref}}\cdot\exp\left(\frac1\beta R(x, y)\right)
$$

And I didn't just mention DPO because I like the math there (though I do). From this formula, the authors of this paper get that, for any two completions $$y_1$$ and $$y_2$$:

$$
\log \frac{G_\beta(y_1|x)}{G_\beta(y_2|x)} = \log \frac{\pi_{\text{ref}}(y_1|x)}{\pi_{\text{ref}}(y_2|x)} + \frac{1}{\beta}(R(x, y_1) - R(x, y_2))
$$

From that, they get that mode collapse is the natural scenario in the following two situations:

**1.** If $\pi_{\text{ref}}(x, y_1) = \pi_{\text{ref}}(x, y_2)$), then

$$
    \frac{G_\beta(y_1|x)}{G_\beta(y_2|x)} = \exp \left( \frac{R(x, y_1) - R(x, y_2)}{\beta} \right)
$$

which means that the difference in reward between $$y_1$$ and $$y_2$$ gets exponentially amplified, making generation collapse to higher reward.


![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/mara-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

**2.**  If the completions have the same reward $$R(x, y_1) = R(x, y_2)$$, then

$$
    \frac{G_\beta(y_1|x)}{G_\beta(y_2|x)} = \frac{\pi_{\text{ref}}(y_1)}{\pi_{\text{ref}}(y_2)}
$$

which means that if the initial policy strongly preferred one answer, it will remain so.


![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/mara-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

However, of course,  a solution distribution for KL-regularized reward maximization might be "multimodal" if all high-reward samples have a high probability with respect to the reference policy.

### A potential remedy: MARA

To overcome this, the authors suggest the **MARA** (**Mode Anchored Reward Augmentation**) algorithm that modifies the reward function on-the-fly during training to create a target distribution that is uniform over all "good" solutions. That's how it works:

* Fix a **reward threshold** $\tau$ to identify "high-quality" or "good" samples. Any sample $y$ with an original reward $R(y) \geq \tau$ is considered a candidate for diversification.

* Within each batch of collected trajectories, choose a single **anchor sample** $z$ from the set of high-quality samples. It is selected as the one with the highest reference probability: 

  $$z = \text{argmax}_{y_i:\,R(y_i) \geqslant \tau} \pi_{ref}(y_i|x)$$ 

  This anchor serves as a stable, high-probability reference point for reward augmentation.

* The **Augmented Reward Function ($\tilde{R}(y)$)** is

  $$
    \tilde{R}(y) =
    \begin{cases}
    R(y) & \text{if } R(y) < \tau \\
    R(z) + \beta(\log \pi_{ref}(z) - \log \pi_{ref}(y)) & \text{if } R(y) \geqslant \tau
    \end{cases}
  $$

  The additional term provides a reward "bonus" to high-quality samples that are less likely under the reference policy.

![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/mara-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

If you're curious, **forward KL** is $$D_{KL}(\pi_{\text{ref}} \vert\vert \pi_\theta)$$, with the different order of policies. It's not normally used in RL, because it involves expectation over the different distribution (reference policy rather than policy in training) which gets into the way of all the actual RL machinery such as REINFORCE as well as the data producing pipelines. So, I would say it's mostly considered here as a theoretical detour.

### Experiments

The authors test MARA ("Mode Anchored" on the plots below) on several tasks.

**1. Random number generation**. An LLM (**Qwen2.5-3B**) is trained to generate a uniform random integer that is either `1` or `2`.

Vanilla KL regularization often leads to mode collapse (generating only `1`s or only `2`s), while MARA preserves diversity (generating `1`s and `2`s with near uniform probability) while maintaining correctness.


![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/mara-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

**2. Creative Question Answering for Chat LLM**. The authors train **Qwen3-1.7B** on a subset of WildChat text (a dataset of human-AI interactions) and measure both correctness and diversity based on n-grams and cosine distance of semantic embeddings.


![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/mara-3.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

**3. Drug Discovery with Chemical Language Models**. Chemical Language Models (CLMs) are used to generate molecules (in SMILES string format) that jointly optimize for various properties relevant to drug discovery.


![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/mara-3.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Here, 

* *Yield* is the number of unique high-reward molecules discovered, while
* *IntDiv1* is a diversity metric that considers how different the generated molecules are at a fundamental, structural level.
* *Circles* is related to "sphere packing," which essentially means how well dispersed or spread out the generated molecules are in a high-dimensional chemical "space."

I won't pretend I understand the biological experiment too well, but the results sound cool.

In any case, the authors are able to find a way of preventing RL from hurting the generation diversity, which is very cool. I witness LLM mode collapsing in many scenarios and I'll be happy to see this problem addressed.

# Thought Communication in Multiagent Collaboration

[https://arxiv.org/abs/2510.20733](https://arxiv.org/abs/2510.20733)

There's something beautifully dystopian in the idea of LLMs reasoning and communicating in ways incomprehensible for humans. It's also potentially more efficient, because predicting a single token from a whole hidden vector leads to massive information loss. We already know [how to reason in vectors](https://arxiv.org/abs/2412.06769), without intermediate decoding. And when I opened this paper, I was expecting to see agents passing hidden vectors to each other. The authors, however, chose a more discrete and somewhat interpretable path.

![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/comm-0.png){: .responsive-image style="--img-desktop:60%; --img-mobile:90%;"}

They consider a Q&A task, where several agents try to answer the question with the help of an orchestrator:

1. First, the agents produce a hypothesis and give it to the orchestrator
2. Then, the orchestrator facilitates information exchange between the agents
3. Each of the agents then produces a final answer based on the received information. (But there may be more rounds of communication, of course)

The core question, of course, is how this information exchange functions, and the authors come up with a **THOUGHTCOMM: Multiagent Communication via Thought** framework which uses **latent concepts**. It has several components:

### The autoencoder

At each communication round $t$, the authors suggest collecting the *final internal state of the final token generated by each agent* - for Agent $i$ it's $H^{(i)}$. These states are concatenated into a single large vector

$$H_t = \left[H^{(1)}, H^{(2)}, ..., H^{(n_a})\right]$$

of length $n_h$.

The **autoencoder** connects $$H_t$$ to a latent representation $$\widehat{Z}_t$$ of a (significantly) lower dimension.
For example, using **Llama-3-8B-Instruct** with 3 agents ($$n_a = 3$$), the total input dimension is $$n_h = 3 \cdot 4096 = 12,288$$. The authors take $$n_z = 1024$$.

The paper's notations here are somewhat confusing, because the authors only have it if for the **decoder** $f: Z\to H$. Since their theoretical analysis only covers the non-practical case of $$n_z = n_h$$, they denote the encoder by $f^{-1}$, but beware: in a realistic situation where $$n_z < n_h$$, $f$ isn't inverible. Having said this, I'll denote the encoder by $f_e$.

### Sparsity

Now, the important feature of the autoencoder that the authors construct is **sparsity**. But beware again: this has nothing to do with sparse autoencoders like [those used to interpret transformers' hidden features](https://transformer-circuits.pub/2023/monosemantic-features). The authors of this papers don't care about feature interpretability. What they want is to distinguish between "thoughts" **shared** between agents and their **private thoughts**. 

That's how the authors strive to achieve it. They compute the *Jacobian* $$J_f$$ - the $$n_h\times n_z$$ matrix of partial derivatives $$(J_f)_{ij} = \frac{\partial h_i}{\partial z_j}$$. Then they *binarize* this matrix (up to some tolerance) into a 0-1 matrix $$B(J_f)$$.

Now, given this matrix, they indentify the subset of thoughts  which have influence on an $i$-th agent as the set of indices $j$ such that there is at least one 1 in the $j$-th column of $$\widehat{Z}_t$$ **within the rows that correstpond to the $i$-th agent**.

For example, in the picture below (it shows the matrix $B$), Agent 1 is influenced by latent thoughts 0 and 1 (0-th and 1-st columns contain 1 in the red stripe of the matrix), while Agent 2 is influenced by 1 and 2.

![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/comm-0.png){: .responsive-image style="--img-desktop:60%; --img-mobile:90%;"}

Now it is possible to determine shared and private thoughts - that is, **private and shared coordinates of the latent vector $$\widehat{Z}_t$$**:
* Thoughts **shared** between agents $i$ and $k$ correspond to columns which have 1 in the stripes of both $i$-th and $k$-th agents
* **Private** thoughts of Agent $i$ correspond to columns which have 1 only in the stripe corresponding to this agent.
* For every $j$-th thought, the authors actually compute the relevance $$\alpha_k$$ which is the number of agents with which the thought is shared.

So, the picture above tell is that the 1st thought (the 1st coordinate of the latent vector $$\widehat{Z}_t$$) is shared between the agents, while the 0-th thought is private to the Agent 1.

And when the authors talk about **sparsity**, they mean that there are few 1s in the matrix $B$, so that each agent has just several private thoughts and just several thoughts shared with each other agent.

The authors prove some theoretical guarantees, but since those cover the setup which differs from the actual practical setup, I'll omit it here.

### Thought exchange

Now, every i-th agent will receive a part of the information contained in the latent vector $$\widehat{Z}_t$$. It won't get anything related to the other agents' private thoughts - only thoughts shared with it.

Namely, the $i$-th agent gets $$\widehat{Z}^{(i)}_t = \widehat{Z}_t \odot w_i$$, where $\odot$ is the elementwise product and

* $w_{ij} = 0$ if the $j$-th thought is private to some other agent; otherwise
* $$w_{ij} = w_{\alpha_j}$$, where $$\alpha_j$$ is the number of agents sharing the $j$-th thought and $$w_{\alpha_j}$$ should probably have been trained weights, but in the paper they seem to be manually set.

### Prefix adaptaion

Now, $$\widehat{Z}_t$$ is mapped to the token embedding space via small adaptor network $$g$$ (likely an MLP). The result is $m$ (a hyperparameter) embeddings that go into the LLM as embeddings of some virtual prefix tokens.

### Training

The loss function for the *autoencoder* is the L1-regularized reconstruction loss:

$$
\mathcal{L} = \| H_t - \hat{f}(f_e(H_t)) \|^2_2} + \lambda \| J_{\hat{f}} \}_1
$$

Of course. You're right, that equation is quite dense. Let's break it down into its components to make it more understandable.

The *adaptor*'s loss function is very involved*

$$
L_{\text{comm}} = \sum_{i=1}^{n_a} \sum_{t=1}^{T} \left[ \underbrace{(1 - \cos(\phi(y_{\text{gen}}^{t,i}), \phi(y_{\text{ref}}^{t,i})))}_{\text{Term 1: Semantic Similarity}} - \underbrace{\log p(y_{\text{gen}}^{t,i} | \text{context}_{t,i}, P_t^{(i)})}_{\text{Term 2: Linguistic Fluency}} \right]
$$

Its first term (*semantic similarity loss*) compares $$\phi(y_gen)$$ and $$\phi(y_ref)$$, where
* $$y_gen$$ is the short sentence the agent generates *with* the help of the injected thought-prefix.
* $$y_ref$$ is the reference sentence the agent would have generated *without* any latent communication.
* $\phi(\cdot)_$ is a function that calculates the average embedding of all the tokens in a sentence. This gives a single vector representing the sentence's overall meaning.

The second term (*linguistic fluency loss*) is just the standard negative log-likelihood loss which helps to ensure that the text is linguistically consistent.

### The results

The experiments compare the THOUGHTCOMM framework with pure-text communication between agents fine tuned on a bulk of conversational data, and THOUGHTCOMM seems to prevail:

![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/comm-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

While I personally find the idea somewhat too convoluted, still it's an interesting approach.


