---
layout: post
title: "Paperwatch 03.11.2025"
categories: blog
permalink: /paperwatch-03-11-2025/
---

**Paperwatch 03.11.2025 by Stanislav Fedotov (Nebius Academy)**

# The Art of Scaling Reinforcement Learning Compute for LLMs

[https://arxiv.org/abs/2510.13786](https://arxiv.org/abs/2510.13786)

I've seen a number of scaling laws for pre-training, connecting the amount of training data, the model size, and the model's performance. Now it's time for the RL post-training stage to get its own scaling laws too. Moreover, the authors - researchers from Meta - use their scaling laws to validate a number of design choices in the RL training pipeling, coming out with the **ScaleRL** algorithm.

We need to be specific about the problem the authors are solving though, because it's quite specific. It's (non surprisingly) math reasoning. RL itself is RLVR (RL with verifiable rewards). And the dataset used is **Polaris-53K** - a dataset of math reasoning prompts. From its 53k entries, 52k are used for training and 1k are left for validation. Comparing to pre-training, this is a meagre amount, but quite normal for post-training.

With the model size fixed at 8B, the authors explore the connection between the amount of training compute and the expected reward (which in case of RLVR is just accuracy). The fit a sigmoid function to describe this dependency:

![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/rl-scale_0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

You might be surprised by the choice of sigmoid instead of a power law. But first, [I've seen sigmoidal scaling laws for pre-training too](https://arxiv.org/abs/2405.10938v1). (By the way, that paper used sigmoid shape to visualize how emergent capabilities actually emerge.)  At the same time, I wouldn't expect RL to have unbounded effect - and actually RL often works this way: first warm-up, then sharp rise, then slow asymptotic improvement.

## The experiments

The authors challenge the RL training pipeline in its several dimensions to find out the optimal configuration.

### The Forward Search

This part improves the pipeline, component by component:

**1. Asynchronous RL Setup** 

The question here is about how and when to update the policy which generates trajectories for RL training.

The default option is **PPO-off-policy**-k. In this setup, the old policy $$\pi_{\text{old}}$$ generates reasoning traces for
a batch of $B$ prompts. Each gradient update processes a mini-batch of $\hat{B}$ prompts, resulting in $k = B/\hat{B}$
gradient updates per batch. 

Another option is **PipelineRL**-k, used by Magistral. In this regimen, generators continuously produce reasoning traces in a streaming fashion. Whenever trainers finish a policy update, the new parameters are immediately pushed to the generators, which continue generating with the updated weights but a stale KV cache from the old policy.

it turns out that **PipelineRL**-k is better.

![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/rl-scale_1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

**2. Loss function**

Things were much easier when we had just PPO, but now there are several GRPO's modifications which can borrow ideas from each other to form many more hybrids.

The authors start with an unnamed algorithm, which, I suppose, borrows many details from [DAPO](https://arxiv.org/abs/2503.14476) but uses token-level, not prompt-level averaging. 

![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/rl-scale_2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

But for this experiment the try three other GRPO modifications - DAPO, [GSPO](https://arxiv.org/abs/2507.18071), and CISPO, and CISPO turns out to be the best:

![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/rl-scale_3.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Here's CISPO:

![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/rl-scale_4.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Just to note: $\pi_{\text{gen}}$ is the policy which is used for producing trajectories. Because of the async updates, it can already be different at the moment when we calculate the loss, so likely the denominator $$\pi_{\text{gen}}(y_i\vert x_i,\theta_\text{old})$$ is just computed at trajectory generation and saved with it.

**3+. Other options**

There are some more things:

* **Precision**: using FP32 at the model's head helps
* **Loss Aggregation**: Sample average vs Prompt average vs Token average. Token averaging might lead to imbalance in influence between long and short sequences, as noticed by the authors of DAPO. Prompt average wins.
* **Advantage normalization**: Prompt-level is chosen
* **Zero-Variance Filtering**: Within each batch, some prompts yield identical rewards across all their generations. It's definitely better to use an *effective batch*: only prompts with non-zero variance are included in the loss calculation

**The final ScaleRL setup**

![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/rl-scale_5.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

### Leave-One-Out (LOO) Ablations

On this stage, the authors start with the full ScaleRL recipe and, for each experiment, they revert exactly one component back to its baseline version (e.g., they test CISPO vs the unnamed baseline RL algorithm). The authors demonstrate that the complete ScaleRL recipe is more compute-efficient than any of the leave-one-out variants.

### Comparison with other influential RL recipes

In the plot below, each recipe is named for a model that was trained using it:

![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/rl-scale_6.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

# Continual Learning via Sparse Memory Finetuning

[https://arxiv.org/abs/2510.15103](https://arxiv.org/abs/2510.15103)

One of the sad differences between LLMs and us humans is that LLMs can't learn from their experience. So, we have to somehow cope with recurrent mistakes that haunt us chat after chat. A tempting remedy would be **continual learning** on bits and scraps of new data as it becomes avilable to us, but this fails because of **catastrophic forgetting**, where updating on new data erases previously acquired capabilities.

The authors suggest an architectural tweak - a special **memory layer** that supersedes one particular FFN layer (in their experiments, layer 12 out of 22). This layer will be updated during continuous training, and it will store the new information in such a way that it doesn't hurt the model's overall capabilities.

![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/continuous-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

The core of this layer are two large matrices: `K` (**keys**) and `V` (**values**), both of size $N \times d$, where $N$ is the total number of memory slots (the authors used $N = 1$ million) and $d$ is the dimension of each key vector (the authors used $d = 1024$). Unlike the self-attention layer, `K` and `V` are *trainable* and don't depend on the inference input.

Another part is the **query projection** `q` which is a learnt projection from the input `x` to the query vector `q(x)` of dimension $d$. You'll see that these queries will play a role similar to the role of queries in the attention mechanism.

Finally, there are two output projections `W1` and `W2` used for gating and final output transformation (similar to the original FFN layer).

### How inference works


At inference, the query `q(x)` is compared against all $N$ key vectors in the memory pool `K` with dot product. From these $N$ similarity scores, the system identifies the `top-k` key vectors that are most similar to the query. The paper specifies $k=32$. The indices of these top-k keys are retrieved, denoted as `I = TopKIndices(Kq(x), k)`. So, the model has **sparse access** to its memory â€“ only a tiny fraction of the entire memory is considered for any given input.

As discussed previously, this lookup is made efficient at scale using **product keys** to avoid exhaustive dot product calculations across millions of keys. The rough idea is:

* At initialization, the original vectors are split into several smaller, equal-sized sub-vectors. For example, a 1024-dimension key vector could be split into 8 sub-vectors, each with 128 dimensions.
* For each sub-vector slot (for example, for the 2nd subvector slot with positions 128-255), a *codebook* is created by running k-means on all the sub-vectors that fall into that slot across the entire memory pool. In each cluster, we take its centroid.

  In our example, we get 8 different codebooks. If we partition each, say, into 256 clusters, we get 256 centroid vectors (of 128 dimensions each).
  
* Now, to encode a sub-vector, we simply find the closest centroid in its corresponding codebook and use the ID of that centroid (an integer from 0 to 255) as its compressed representation. The final **product key** is simply the concatenation of these centroid IDs.

* At inference, we don't compute the distance (for example, dot product) from the query vector `q(x)` to all million keys. Instead, we do the following.
  
  We start by computing the distance from its sub-vectors to product keys (`8*256` distances instead of 1M). For example, a distance to the product key `[id_1, id_2,...]` is  `distance(query_subvector_1, centroid_id_1) + distance(query_subvector_2, centroid_id_2) + ...`

  Now, we take several closest product keys `[id_1, id_2,...]` and fetch all keys of the form

  `[sub-vector from cluster_id_1, sub-vector from cluster_id_2, ...]`

  That is a lot of keys, but much less than 1M. Now we can do **reranking** - actually compute distances between these keys and `q(x)`, choosing the `top-k` closest keys.

Now that we have `k` retrieved keys, we do more or less the same thing we'd do in the attention mechanism:

![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/continuous-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

### Pre-training

An important thing to note is that the authors train their LLM from scratch already with the memory layer.

Don't know what to add here :) My first guess would be that they'd try insert the memory layer into an already existing pre-trained model, but it would be indeed less efficient and more tricky to train.

### Update mechanism

Now, the most important part - how the authors suggest to update the model on new data during continuous learning.

The authors found out that naively finetuning the memory layer model still causes catastrophic forgetting. So, they decide to update only a fraction of it on each input.

Here's how it works:

1.  When a new batch of data is processed, the model accesses various sub-vector slots in the memory layer. The system counts how many times each memory index (index of a particular key) is accessed for this specific batch.
2.  To determine which memory slots are most important for the *new* information, they calculate a [*TF-IDF*](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) score for each accessed memory slot. This score helps to identify memory slots that are highly relevant to the new batch (high TF) but are not part of the model's general, pre-existing knowledge (high IDF). This prevents the model from overwriting important, general-purpose memory slots.
4.  The model then finetunes only the `top-t` memory slots with the highest TF-IDF scores.

### Results

Experiments show that indeed this strategy helps to update the model on new data without breaking everything else, unlike usual fine tuning:

![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/continuous-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

# Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models

[https://arxiv.org/abs/2510.04618](https://arxiv.org/abs/2510.04618)

Probably a year ago we'd still see papers discussing automatic prompt optimization. Now prompt engineering has given way to context engineering, and instead of just optimizing prompts we optimize contexts.

When you're continuously working with an agent - think of coding a large project with Claude Code or Cursor - you gather a list of useful instructions that cover both your preferences and the agent's pitfalls. The authors of this paper decided to create an automatic pipeline for creating such instructions.

Here's the scheme of their **ACE** (Agentic Context Engineering) framework:

![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/ace-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

The authors argue that constantly rewriting context may lead to sporadic context collapse, like shown in the plot below, and they suggest instead work with the context incrementally, performing only two operations:

* Adding a new bullet
* Refine the list of bullets, deduplicating them based on semantic similarity score

The authors mention that bullets might also be changed in some way but they don't specify how and don't seem to implement it.

![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/ace-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Let's discuss the process in more details. Well, at least in as much details as we can, because the authors preferred to remain unspecific about many details of their processes.

First of all, the context format they explore - **ACE Playbook** - consists of structured *bullets* containing metadata and content. Like this:

![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/ace-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

How the main components work:

* The **Generator** is the agent which performs the main tasks whatever they could be. Think of a coding agent like Claude Code, for example. Since the Playbook only grows, the authors endow the generator with a certain unspecified retrieval mechanism (RAG) which allows it to obtain only relevant Playbook bullets.

  On the Generator side there should be also a kind of a feedback mechanism - for example, telling if the code crashes or if it does the task.
  
*  The **Reflector** gets a whole agentic trajectory generated by the Generator, with all its successes and failures, and it critiques these traces to extract *insights*, potentially in a multi-turn thinking loop.

*  The **Curator** synthesizes these insights into compact *delta entries*, which are merged deterministically into the Playbook. As far as I could understand, the authors now only suggest adding + deduplicating Playbook bullets.

That's it basically. Here are some results which show that ACE is cool.

![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/ace-3.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/ace-4.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

While I'm slightly annoyed by the lack of details in the paper, I agree that, while continuous learning isn't here (see the previous paper here))), meaningful and constantly updated instructions play an important in assuring that the agent does what we want. I myself have a dynamic list of grudges towards LLMs and explanations of what I consider a perfect code. And I wouldn't be against forming parts of it automatically. On the other hand, as the Playbook grows, we might hit the problem of retrieval inefficiency, so I'd welcome some amount of refactoring - which, ironically, might lead again to the situation when the Playbook length experiences sudden dives down.


# KL-Regularized Reinforcement Learning is Designed to Mode Collapse

[https://arxiv.org/abs/2510.20817](https://arxiv.org/abs/2510.20817)

It's well known that RL might lead to mode collapse, which practically means that the diversity of the generated text will be significantly reduced. Sometimes, the prominent (and quite annoying) GPT writing style is attributed to this.

A month ago I've reviewed [an attempt at addressing the problem through matching reward distribution rather than just maximizing the reward](https://arxiv.org/abs/2509.15207). In this paper, the authors try to do it through manipulating the reward function.

First of all, the authors show mathematically that the problem of mode collapsing is in a sense inevitable for KL-regularized RL. We should be careful here, so let's understand what exactly they prove.

The authors consider a simplified problem with the following RL objective:

$$
J_\beta(\pi_\theta) = \mathbb{E}_{x\sim\mathcal{D},\,y\sim\pi_\theta(y|x)}[R(x, y)] - \beta D_{KL}(\pi_\theta || \pi_{\text{ref}})
$$

Here $R$ is the reward. A huge simplification in comparison to PPO or GRPO, but the same was used by the authors of [DPO](https://arxiv.org/abs/2305.18290) who were able to find the exact solution of this optimization problem:

$$
G_{\beta}(y|x) = \frac1{Z(x)}\pi_{\text{ref}}\cdot\exp\left(\frac1\beta R(x, y)\right)
$$

And I didn't just mention DPO because I like the math there (though I do). From this formula, the authors of this paper get that, for any two completions $$y_1$$ and $$y_2$$:

$$
\log \frac{G_\beta(y_1|x)}{G_\beta(y_2|x)} = \log \frac{\pi_{\text{ref}}(y_1|x)}{\pi_{\text{ref}}(y_2|x)} + \frac{1}{\beta}(R(x, y_1) - R(x, y_2))
$$

From that, they get that mode collapse is the natural scenario in the following two situations:

**1.** If $\pi_{\text{ref}}(x, y_1) = \pi_{\text{ref}}(x, y_2)$), then

$$
    \frac{G_\beta(y_1|x)}{G_\beta(y_2|x)} = \exp \left( \frac{R(x, y_1) - R(x, y_2)}{\beta} \right)
$$

which means that the difference in reward between $$y_1$$ and $$y_2$$ gets exponentially amplified, making generation collapse to higher reward.


![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/mara-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

**2.**  If the completions have the same reward $$R(x, y_1) = R(x, y_2)$$, then

$$
    \frac{G_\beta(y_1|x)}{G_\beta(y_2|x)} = \frac{\pi_{\text{ref}}(y_1)}{\pi_{\text{ref}}(y_2)}
$$

which means that if the initial policy strongly preferred one answer, it will remain so.


![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/mara-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

However, of course,  a solution distribution for KL-regularized reward maximization might be "multimodal" if all high-reward samples have a high probability with respect to the reference policy.

### A potential remedy: MARA

To overcome this, the authors suggest the **MARA** (**Mode Anchored Reward Augmentation**) algorithm that modifies the reward function on-the-fly during training to create a target distribution that is uniform over all "good" solutions. That's how it works:

* Fix a **reward threshold** $\tau$ to identify "high-quality" or "good" samples. Any sample $y$ with an original reward $R(y) \geq \tau$ is considered a candidate for diversification.

* Within each batch of collected trajectories, choose a single **anchor sample** $z$ from the set of high-quality samples. It is selected as the one with the highest reference probability: 

  $$z = \text{argmax}_{y_i:\,R(y_i) \geqslant \tau} \pi_{ref}(y_i|x)$$ 

  This anchor serves as a stable, high-probability reference point for reward augmentation.

* The **Augmented Reward Function ($\tilde{R}(y)$)** is

  $$
    \tilde{R}(y) =
    \begin{cases}
    R(y) & \text{if } R(y) < \tau \\
    R(z) + \beta(\log \pi_{ref}(z) - \log \pi_{ref}(y)) & \text{if } R(y) \geqslant \tau
    \end{cases}
  $$

  The additional term provides a reward "bonus" to high-quality samples that are less likely under the reference policy.

![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/mara-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

If you're curious, **forward KL** is $$D_{KL}(\pi_{\text{ref}} \vert\vert \pi_\theta)$$, with the different order of policies. It's not normally used in RL, because it involves expectation over the different distribution (reference policy rather than policy in training) which gets into the way of all the actual RL machinery such as REINFORCE as well as the data producing pipelines. So, I would say it's mostly considered here as a theoretical detour.

### Experiments

The authors test MARA ("Mode Anchored" on the plots below) on several tasks.

**1. Random number generation**. An LLM (**Qwen2.5-3B**) is trained to generate a uniform random integer that is either `1` or `2`.

Vanilla KL regularization often leads to mode collapse (generating only `1`s or only `2`s), while MARA preserves diversity (generating `1`s and `2`s with near uniform probability) while maintaining correctness.


![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/mara-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

**2. Creative Question Answering for Chat LLM**. The authors train **Qwen3-1.7B** on a subset of WildChat text (a dataset of human-AI interactions) and measure both correctness and diversity based on n-grams and cosine distance of semantic embeddings.


![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/mara-3.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

**3. Drug Discovery with Chemical Language Models**. Chemical Language Models (CLMs) are used to generate molecules (in SMILES string format) that jointly optimize for various properties relevant to drug discovery.


![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/mara-3.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Here, 

* *Yield* is the number of unique high-reward molecules discovered, while
* *IntDiv1* is a diversity metric that considers how different the generated molecules are at a fundamental, structural level.
* *Circles* is related to "sphere packing," which essentially means how well dispersed or spread out the generated molecules are in a high-dimensional chemical "space."

I won't pretend I understand the biological experiment too well, but the results sound cool.

In any case, the authors are able to find a way of preventing RL from hurting the generation diversity, which is very cool. I witness LLM mode collapsing in many scenarios and I'll be happy to see this problem addressed.

# Bayesian Influence Functions for Hessian-Free Data Attribution

[https://arxiv.org/abs/2509.26544](https://arxiv.org/abs/2509.26544)

The quality of your final model depends greatly on the quality of the training and finetuning data. If you finetune an LLM and the result is somewhat faulty, you might want to learn which finetuning data samples were to blame. Maybe some of them are mislabeled or noisy. For that, of course, you'd need a measure of how much a training data point $z_j$ influences the result on a validation data point $z_i$. And that's not a simple task to measure.

The classic framework for this is **influence functions**. Originally this approach was described for the loss $$\mathcal{L}(w) = \sum_i\ell(w, z_i)$$ (sum over all training data points; $w$ are the model's weights) and it relies on the assumption that the model's current weights $w^\ast$ are the optimum of $\mathcal{L}$.

It works as follows. Let's consider a modified loss with data-wise importance weights $$\mathcal{L}_{\beta}(w) = \sum_i\beta_i\ell(w, z_i)$$. Let's imagine that the loss is well-behaved in the neighborhood of $w^\ast$. Then there is a well-behaved function $w^\ast(\beta)$ which maps each $\beta$ into its optimal weights.

Then the **influence function** is defined as:

$$\text{IF}(i\to j) = \left.\frac{\partial \ell(w^\ast(\beta), z_j)}{\partial \beta_i}\right|_{\beta=(1,\ldots,1)}$$

In plain English it's how the loss at $$z_j$$ is going to change if we vary the importance weight of $$z_i$$. It even has a close-form formula:

$$\text{IF}(i\to j) = -\nabla_w\ell(w^*, z_j)^T\,H(w^\ast)^{-1}\,\nabla_w\ell(w^*, z_i)$$

Here $H$ is the Hessian (the matrix of second derivatives), both gradients are column vectors the size of $w$ flattened, and all derivatives are taken at $w^\ast$ - the optimum of the original loss.

Alas, this is a situation when a closed-form solution looks nice in the books but is of no practical value. Computing a hessian is too taxing for any of today's large neural network. So, researchers come up with more efficient methods from time to time, and this paper suggests one of them. And since I have a weakness for Bayesian methods, I couldn't help reviewing this paper :)

### The Bayesian Influence Function (BIF)

The very core of the Bayesian approach is working with distributions instead of point estimates. So:

* Usual ML searches for the optimal $w^*$
* Bayesian ML estimates how likely is this or that $w^*$ based on the training data. Mathematically:

  * We fix a *prior distribution* $\pi(w)$, which encodes our a priori thoughts about which $w^\ast$ are more likely than the others. Usually $\pi(w)$ is just standard gaussian $\mathcal{N}(0,1)$, which very roughly means: any signs are possible (centered) and weights are more likely small than large (std = 1).
  * Then we compute *posterior distribution* using the Bayes rule:
 
    $$
    \text{Posterior} \propto \text{Likelihood} \times \text{Prior}
    $$

    $$p(w|\mathcal{D}_{\text{train}}) \propto p(\mathcal{D}_{\text{train}} | w) \pi(w)$$

    In turn, likelihood is tightly connected to the loss (think of the correspondence MSE loss $\leftrightarrow$ gaussian likelihood or MAE loss $\leftrightarrow$ laplacian likelihood):

    $$\mathcal{L}(w) = - \log\,\propto p(\mathcal{D}_{\text{train}} | w) + const$$

    which means that

    $$p(\mathcal{D}_{\text{train}} | w) \propto \exp\left(-\mathcal{L}(w)\right)$$

    So,

    $$p(w|\mathcal{D}_{\text{train}}) \propto \exp\left(-\mathcal{L}(w)\right) \pi(w)$$

    And, inserting $\beta$ into the picture:

    $$p_{\beta}(w|\mathcal{D}_{\text{train}}) \propto \exp\left(-\mathcal{L}_{\beta}(w)\right) \pi(w)$$

That was quite a lot of math, so what's about influence functions? Previously, we considered $w^\ast(\beta)$, which is the optimum of $$\mathcal{L}_{\beta}(w)$$ and of $$p_{\beta}(w\vert \mathcal{D}_{\text{train}})$$. With Bayesian approach, we'd prefer to consider something more natural - the expected value

$$\mathbb{E}_wp_{\beta}(w|\mathcal{D}_{\text{train}}$$

**Bayesian Influence Function** (**BIF**) is then

$$\text{BIF}(i\to j) = \left.\frac{\partial \mathbb{E}_wp_{\beta}(w|\mathcal{D}_{\text{train}}}{\partial \beta_i}\right|_{\beta=(1,\ldots,1)}$$

With some additional math, you might show that it's equal to the covariance

$$\text{BIF}(i\to j) = \text{Cov}\left(\ell(w, z_i), \ell(w, z_j)\right)$$

And it's fun that we got rid of $\beta$ and differentiation!

However, it's generally intractable, so instead **local Bayesian influence function** is used, which can be seen as both

* A regularized version of the original BIF.
* A particular choice of the prior distribution $\pi(w)$, which we "forgot" above. Let's recall that, despite entertaining ourselves with Bayesian approach, we actually have the "optimal" model $w^\ast$, which we obtained likely through optimizing loss with SGD. And to tell the truth, we're only insterested in the behaviour of the loss around $w^\ast$. To enforce it, let's take a Gaussian prior telling "$w$ is likely to be close to $w^\ast$": $\pi(w) = \mathcal{N}(w^\ast, \text{smth})$, which will translate to the following *posterior* distribution: 

$$p_\gamma(w|\mathcal{D}_{\text{train}}) \propto \exp\left(-\mathcal{L}(w) - \frac{\gamma}2\|w - w^*\|^2\right)$$

Now, the local BIF is the covariance with respect to this regularized distribution::

$$\text{BIF}_{\gamma}(i\to j) = \text{Cov}_{\gamma}\left(\ell(w, z_i), \ell(w, z_j)\right)$$

### How to estimate this covatiance

Of course, there is no close-form formula for $$\text{BIF}_{\gamma}(i\to j)$$. So we need to estimate it from the data. But which data? Where the hell would we get data from the *posterior* distribution?

Luckily, there are devices for sampling data from a given distribution, and in this case there's even one which is supposed to work well. It's **Langevin Dynamics**. I'll share just the high-level idea. The posterior here is

$$p_{\gamma}(w|\mathcal{D}_{\text{train}}) \propto \exp(-U(w)),$$

where

$$
U(w) = L_{train}(w) + \frac{\gamma}{2} ||w - w^*||^2
$$

Now, if we take the *randomized* SGD optimization procedure for $U(w)$:

$$w_{t+1} = w_t - \frac{\varepsilon}2\nabla U(w_t)  + \mathcal{N}(0, \varepsilon),$$

it can be shown that this random process will converge to sampling from what is known as **Gibbs-Boltzmann distribution**:

$$
p_{stat}(w) = \frac{1}{Z} \exp(-U(w))
$$

which is exactly the posterior!

So, to compute BIF, you can run the randomized SGD for some number of iteration to let in converge - and then you have a machine for sampling from the posterior all for yourself!

### Experiments and results

To start with, here are some theoretical estimates. BIF is compared with EK-FAC, which estimates the Hessian through the Fisher information matrix.

![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/bif-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Here's some eyeball testing:

![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/bif-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

The authors even made some experiments with retraining models on random subsets of the training data to get the actual influence scores. And BIF showed results pretty much aligned with what was achieved by other methods.

# Thought Communication in Multiagent Collaboration

[https://arxiv.org/abs/2510.20733](https://arxiv.org/abs/2510.20733)

There's something beautifully dystopian in the idea of LLMs reasoning and communicating in ways incomprehensible for humans. It's also potentially more efficient, because predicting a single token from a whole hidden vector leads to massive information loss. We already know [how to reason in vectors](https://arxiv.org/abs/2412.06769), without intermediate decoding. And when I opened this paper, I was expecting to see agents passing hidden vectors to each other. The authors, however, chose a more discrete and somewhat interpretable path.

![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/comm-0.png){: .responsive-image style="--img-desktop:60%; --img-mobile:90%;"}

They consider a Q&A task, where several agents try to answer the question with the help of an orchestrator:

1. First, the agents produce a hypothesis and give it to the orchestrator
2. Then, the orchestrator facilitates information exchange between the agents
3. Each of the agents then produces a final answer based on the received information. (But there may be more rounds of communication, of course)

The core question, of course, is how this information exchange functions, and the authors come up with a **THOUGHTCOMM: Multiagent Communication via Thought** framework which uses **latent concepts**. It has several components:

### The autoencoder

At each communication round $t$, the authors suggest collecting the *final internal state of the final token generated by each agent* - for Agent $i$ it's $H^{(i)}$. These states are concatenated into a single large vector

$$H_t = \left[H^{(1)}, H^{(2)}, ..., H^{(n_a})\right]$$

of length $n_h$.

The **autoencoder** connects $$H_t$$ to a latent representation $$\widehat{Z}_t$$ of a (significantly) lower dimension.
For example, using **Llama-3-8B-Instruct** with 3 agents ($$n_a = 3$$), the total input dimension is $$n_h = 3 \cdot 4096 = 12,288$$. The authors take $$n_z = 1024$$.

The paper's notations here are somewhat confusing, because the authors only have it if for the **decoder** $f: Z\to H$. Since their theoretical analysis only covers the non-practical case of $$n_z = n_h$$, they denote the encoder by $f^{-1}$, but beware: in a realistic situation where $$n_z < n_h$$, $f$ isn't inverible. Having said this, I'll denote the encoder by $f_e$.

### Sparsity

Now, the important feature of the autoencoder that the authors construct is **sparsity**. But beware again: this has nothing to do with sparse autoencoders like [those used to interpret transformers' hidden features](https://transformer-circuits.pub/2023/monosemantic-features). The authors of this papers don't care about feature interpretability. What they want is to distinguish between "thoughts" **shared** between agents and their **private thoughts**. 

That's how the authors strive to achieve it. They compute the *Jacobian* $$J_f$$ - the $$n_h\times n_z$$ matrix of partial derivatives $$(J_f)_{ij} = \frac{\partial h_i}{\partial z_j}$$. Then they *binarize* this matrix (up to some tolerance) into a 0-1 matrix $$B(J_f)$$.

Now, given this matrix, they indentify the subset of thoughts  which have influence on an $i$-th agent as the set of indices $j$ such that there is at least one 1 in the $j$-th column of $$\widehat{Z}_t$$ **within the rows that correstpond to the $i$-th agent**.

For example, in the picture below (it shows the matrix $B$), Agent 1 is influenced by latent thoughts 0 and 1 (0-th and 1-st columns contain 1 in the red stripe of the matrix), while Agent 2 is influenced by 1 and 2.

![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/comm-1.png){: .responsive-image style="--img-desktop:60%; --img-mobile:90%;"}

Now it is possible to determine shared and private thoughts - that is, **private and shared coordinates of the latent vector $$\widehat{Z}_t$$**:
* Thoughts **shared** between agents $i$ and $k$ correspond to columns which have 1 in the stripes of both $i$-th and $k$-th agents
* **Private** thoughts of Agent $i$ correspond to columns which have 1 only in the stripe corresponding to this agent.
* For every $j$-th thought, the authors actually compute the relevance $$\alpha_k$$ which is the number of agents with which the thought is shared.

So, the picture above tell is that the 1st thought (the 1st coordinate of the latent vector $$\widehat{Z}_t$$) is shared between the agents, while the 0-th thought is private to the Agent 1.

And when the authors talk about **sparsity**, they mean that there are few 1s in the matrix $B$, so that each agent has just several private thoughts and just several thoughts shared with each other agent.

The authors prove some theoretical guarantees, but since those cover the setup which differs from the actual practical setup, I'll omit it here.

### Thought exchange

Now, every i-th agent will receive a part of the information contained in the latent vector $$\widehat{Z}_t$$. It won't get anything related to the other agents' private thoughts - only thoughts shared with it.

Namely, the $i$-th agent gets $$\widehat{Z}^{(i)}_t = \widehat{Z}_t \odot w_i$$, where $\odot$ is the elementwise product and

* $w_{ij} = 0$ if the $j$-th thought is private to some other agent; otherwise
* $$w_{ij} = w_{\alpha_j}$$, where $$\alpha_j$$ is the number of agents sharing the $j$-th thought and $$w_{\alpha_j}$$ should probably have been trained weights, but in the paper they seem to be manually set.

### Prefix adaptaion

Now, $$\widehat{Z}_t$$ is mapped to the token embedding space via small adaptor network $$g$$ (likely an MLP). The result is $m$ (a hyperparameter) embeddings that go into the LLM as embeddings of some virtual prefix tokens.

### Training

The loss function for the *autoencoder* is the L1-regularized reconstruction loss:

$$
\mathcal{L} = \| H_t - \hat{f}(f_e(H_t)) \|^2_2 + \lambda \| J_{\hat{f}} \|_1
$$

Of course. You're right, that equation is quite dense. Let's break it down into its components to make it more understandable.

The *adaptor*'s loss function is very involved*

$$
L_{\text{comm}} = \sum_{i=1}^{n_a} \sum_{t=1}^{T} \left[ \underbrace{(1 - \cos(\phi(y_{\text{gen}}^{t,i}), \phi(y_{\text{ref}}^{t,i})))}_{\text{Term 1: Semantic Similarity}} - \underbrace{\log p(y_{\text{gen}}^{t,i} | \text{context}_{t,i}, P_t^{(i)})}_{\text{Term 2: Linguistic Fluency}} \right]
$$

Its first term (*semantic similarity loss*) compares $$\phi(y_gen)$$ and $$\phi(y_ref)$$, where
* $$y_gen$$ is the short sentence the agent generates *with* the help of the injected thought-prefix.
* $$y_ref$$ is the reference sentence the agent would have generated *without* any latent communication.
* $\phi(\cdot)$ is a function that calculates the average embedding of all the tokens in a sentence. This gives a single vector representing the sentence's overall meaning.

The second term (*linguistic fluency loss*) is just the standard negative log-likelihood loss which helps to ensure that the text is linguistically consistent.

### The results

The experiments compare the THOUGHTCOMM framework with pure-text communication between agents fine tuned on a bulk of conversational data, and THOUGHTCOMM seems to prevail:

![]({{ site.baseurl }}/assets/images/paperwatch-03-11-2025/comm-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

While I personally find the idea somewhat too convoluted, still it's an interesting approach.


