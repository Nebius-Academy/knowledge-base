---
layout: post
title: "Paperwatch 29.09.2025"
categories: blog
permalink: /paperwatch-29-09-2025/
---

**Paperwatch 29.09.2025 by Stanislav Fedotov (Nebius Academy)**

# LLMs at the ICPC finals 

ICPC stands for the **International Collegiate Programming Contest**. University teams compete there, solving tough algorithmic problems. Its world finals is a grand event whose winners are highly sought after programmers.

After Google's and OpenAI's gold medals at the International Mathematical Olympiad, ICPC was definitely next competition to fall to LLMs. And indeed:

* [Gemini achieves gold-medal level](https://deepmind.google/discover/blog/gemini-achieves-gold-level-performance-at-the-international-collegiate-programming-contest-world-finals/) and informal 2nd place, solving 10 out of 12 problems under the same time constraints as the student teams had. Moreover, it solved 8 problems within just 45 minutes.

  It's especially interesting that it solved Problem C, which was solved by no university team.

  You can check the solutions [here](https://github.com/google-deepmind/gemini_icpc2025).
  
* [Models by OpenAI solved 12 out of 12 problems](https://x.com/MostafaRohani/status/1968360976379703569). GPT-5 answered 11 problems correctly, and the last (and most difficult problem) was solved by the experimental reasoning model.

I'm really impressed with this acievement. However, we need to understand that ICPC problems usually have moderately long solutions, and for LLMs solving them should be much easier than working with huge, production-grade repositories.

# ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution

[https://arxiv.org/pdf/2509.19349](https://arxiv.org/pdf/2509.19349)

Further in this paperwatch review you'll see several examples of math problems solved with [**AlphaEvolve**](https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/), a Gemini-powered coding agent from Google for designing advanced algorithms. Now, Sakana AI suggests not just an open-source version of it; it has several novel features that can potentially allow it be faster than AlphaEvolve.

**ShinkaEvolve** is a framework that "evolves" algorithms given

1. A starting piece of code with a clearly marked "evolvable" section. They do the latter with the `EVOLVE-BLOCK-START & EVOLVE-BLOCK-END` markers.
2. An automated judge that can run any new version of that code and return a single numerical **fitness score**.

### The algorithm

At each point in time, the system maintains a fixed-size **archive**: a library of programs that have been created and tested so far. Each program in the archive has a fitness score and some metadata (like how many "offspring" it has produced).

The system updates the archive step by step; here's how each step works:

**Step 1: Select the "parent" program to be improved**

ShinkaEvolve uses a **weighted sampling** strategy to balance two factors:

*   **Performance:** How good is the program's score
*   **Novelty:** How many times has this program already been used as a parent? (Fewer is better.)

Here are several strategies for a comparison (weighted sampling is to the right): 

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/shinka-evolve-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

As you see, the weighted strategy favours high-performing programs that spawned only a few modifications (that is, were used as "parents" only a few times).

**Step 2: Assemble the LLM prompt**

It includes:

* The "parent" program
* **Inspiration programs** - the code of a few other high-performing programs
* The goal
* The fitness score of the "parent" program and textual feedback (if any) that the judge provided with the fitness score.

The prompt also describes one of the following mutation modes:

* Diff-based edits, that utilize SEARCH/REPLACE blocks for targeted modifications
* Full rewrites; it is ensured, however, that non-mutable blocks remain unchanged
* Crossover mutation - an additional archive program is sampled and an LLM is prompted to combine programs

**Step 3: Mutate the "parent" program with an LLM, producing a new "offspring" program**

The prompt from Step 2 is sent to an LLM. And we have an indefinite article here for a reason. ShinkaEvolve uses an **adaptive bandit algorithm** to choose *which* LLM to use from a certain pool of available LLMs. This algorithm keeps track of which models have been generating the most successful mutations and will favor those. It is based on UCB1 algorithm (the same that is used in MCTS) and associates each LLM with a visitation counter and an estimate of the expected score updated with the performance of its sampled mutations, balancing these two things.

**Step 4: Filter the "offspring" for novelty**

Before wasting time on a full evaluation, the system quickly checks if the new program is genuinely different from what it already has. This is a two-stage process:

1. Fast check (Embedding similarity): The code is vectorized. The system calculates how similar this vector is to all other programs in its archive. If the similarity score exceeds a threshold (e.g., > 95%), the program is flagged as a likely near-duplicate.
2.  Smarter check (LLM-as-a-Novelty-Judge): If the program is flagged, a second LLM is asked to assess its originality. If the judge LLM says "no," the offspring is discarded, and the whole process goes back to Step 3 to create a different mutation.

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/shinka-evolve-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

**Step 5: Evaluate the "offspring"**

If the new program passes the novelty filter, it's time for the real test. The framework executes the user-provided Judge script which outputs the final **fitness score**

**Step 6: Update the archive**

The new "offspring" program, along with its new fitness score and any textual feedback from the judge, is now added to the main archive. At this point, least-performing programs might be kicked out of the archive to preserve its fixed size.

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/shinka-evolve-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

## Islands

To avoid getting stuck in local optima, ShinkaEvolve actually independently evolves population of several **"Islands"**. Every island is initialized with the same starting program. 

Occasionally, programs can migrate between islands to diffuse knowledge across "discovery substreams". However, to protect the uniqueness of each island, the island-specific best-performing program is forbidden from migrating. 

## Meta-Scratchpad

ShinkaEvolve also implements a meta-scratchpad system that periodically analyzes successful solutions to accelerate learning. 

Every $T$ generations, the system summarizes the recent program evaluations and identifies common optimization strategies and design principles. The meta-agent synthesizes insights into actionable recommendations appended to the mutation prompt, providing high-level guidance from accumulated evolutionary experience. 

## ShinkaEvolve vs AlphaEvolve

ShinkaEvolve borrows many ideas from AlphaEvolve - for example, the Island model. However, it also brings several novel and important features to the table, including:

* Weighted sampling of "parent" programs
* Novelty filtering
* Meta-scratchpad

This may allow it to work (much) faster than AlphaEvolve.

## Experiments

### Circle Packing

The task is to place 26 circles within a unit square such that the sum of their radii is maximized while ensuring no circles overlap and all circles remain fully contained within the square boundary. 

There is a great visualization of how this evolves in the [Sakana AI's blog](https://sakana.ai/shinka-evolve/). Here's the solution they found:

![](https://sakana.ai/assets/shinka-evolve/circle_packing_soln.png){: .responsive-image style="--img-desktop:50%; --img-mobile:80%;"}

And here is another visualization of the evolution process:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/shinka-evolve-3.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

It took just 150 evaluations for ShinkaEvolve to arrive at the final solution, while for AlphaEvolve it's thousands of evaluations - which sounds like a huge difference.

### An LLM agent to solve AIME problems

If you want to code an agent, you can choose to just call an API with a simplistic prompt or to implement a whole Tree of Thoughts or a multi-agent system. So many exciting possibilities.

ShinkaEvolve eventually created a multi-step multi-agenr system indeed. For each question, it suggested to

1. First, independently generate three solutions using three different agentic personae (The Meticulous Mathematician, The Intuitive Specialist, and The Algorithmic Thinker:)
2. Then, a "sceptical reviewer" agent reviews each of their solutions
3. Finally, an "Editor-in-chief" produces the final solution

A nice thing is that ShinkaEvolve didn't overfit on the AIME 2024 used for evolution. The authors also tested the resulted agentic system on problems from AIME 2023 and AIME 2025, without any changes.

### Preventing MoE load disbalance

A significant downside of Mixture-of-Expert models is the need for carefully balancing the load between experts. If the router assigns all the work to the same expert during training, some of the others might never learn anything and atrophy; such situation is called "expert collapse" and hurts the model. So, load balancing regularization is used during training. And the authors decided to use ShinkaEvolve to create a better load-balancing loss (**LBL**) function.

They start with a standard LBL which encourages the *actual fraction of tokens* sent to an expert to match the *router's average confidence* in that expert. The Judge is really heavy for this task. It trains from screatch a new 556M-parameter LLM in a `fineweb` dataset consisting of 2 billion tokens of text data.

That's that it discovers:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/shinka-evolve-4.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Here, $f_{l,i} = $ (​Total tokens in layer $l$)/(Tokens routed to expert $i$). So, the original (blue) part is something like a cross-entropy.​

The novel (orange) part of the loss have the following meaning: it implements a hard minimum workload rule to protect every expert, and it makes the penalty for violating that rule much more severe when the overall team is already becoming dangerously reliant on just a few star performers. An interesing thing.

### The Results: Proving the New Loss Was Better

To prove their discovered loss function was genuinely superior, the authors scaled up. They used it to train a much larger, 2.7 billion parameter MoE model and compared it to the same model trained with the standard LBL.

*   **Better Downstream Performance:** The model trained with the new ShinkaEvolve LBL performed consistently better across a suite of seven standard reasoning benchmarks (Figure 8, left).
*   **Better Perplexity:** The model achieved a lower perplexity, which is a core metric that means it was fundamentally better at predicting the next word in a sequence (Figure 8, middle).
*   **Consistent Gains:** The improvement was consistent across different levels of regularization, showing the benefit was robust.

In summary, this experiment was a remarkable success. ShinkaEvolve moved beyond optimizing an application and into the realm of core AI science, discovering a new, general-purpose mathematical tool that improves how we train some of our most advanced models.

# FlowRL: Matching Reward Distributions for LLM Reasoning

[https://arxiv.org/pdf/2509.15207](https://arxiv.org/pdf/2509.15207)

RL is a cool device of LLM training, producing great long-reasoning models and helping to align them with human values. But there's an annoying downside to it - a potential of mode collapsing. That is, out of many high-reward regions of text distribution, the LLM collapses during RL training to generating only a handful of peaks. The picture below illustrates this nicely:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/gflow-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

It is the mode collapsing phenomenon that I blame for the annoying GPT style, by the way.

So, finding a way of avoiding this pitfall is a noble quest. The authors took inspiration from GFlowNets (which I won't introduce here; but feel free to check [this paper](https://arxiv.org/pdf/2209.12782), for example) and reused some of the PPO/GRPO best practices, obtaining a nice approach to training long reasoners.

They start from a pretty straightforward idea of minimizing the KL divergence between the trained policy $\pi_{\theta}(y
\vert x)$ and a distribution derived from the reward $r(x,y)$. The latter is usually defined as

$$\widehat{q}_{r}(y|x) = \frac{\exp(\beta r(x, y))}{Z(x)},$$

where $Z(x)$ is a normalizing coefficient needed to make $\widehat{q}_{r}$ an actual distribution (so that it sum to $1$ for fixed $x$). The problem is, however, that we don't know $Z(x)$. Mathematically it's

$$Z(x) = \sum_y\exp(\beta r(x, y)),$$

which is impossible to compute. But... why don't we learn it? So, it becomes $Z_{\phi}(x)$. And the training objective becomes

$$\mathbb{D}_{\text{KL}}\left(\pi_{\theta}(y|x)\left|\left| \frac{\exp(\beta r(x, y))}{Z_{\phi}(x)} \right.\right.\right) \longrightarrow \min_{\theta,\phi}$$

which is equivalent to

$$\log{Z_{\phi}(x)} + \log{\pi_{\theta}(y|x)} - \beta r(x, y) \longrightarrow \min_{\theta,\phi}$$

At this point, however, the authors run into the same problems RL trainers faced:

* The algorithm is totally on-policy, which makes it very inefficient. Insteady, you want to generate trajectories in batches and train on them. Say hi $\pi_{\text{old}}$ from PPO and GRPO, which is exactly the "old" policy that was used to generate trajectories in the current training batch.
* If you want to train a long reasoner model, you have to deal with long trajectories, and your gradients will likely explode.

No wonder the authors end up introducing all the perks of GRPO into the training:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/gflow-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

As you can see, we have here:

* GRPO-style advantage with clipping and importance sampling weights
* Normalizing by length (the $\frac1{\vert y \vert}$ multiples)
* Regularization with respect to the reference (pre-RL) policy

They train Qwen-2.5-7B/32B for math tasks and DeepSeek-R1-Distill-Qwen-7B for code tasks, respectively. borrowing math training dataset from the [DAPO paper](https://arxiv.org/pdf/2503.14476) and code training dataset from the [DeepCoder paper](https://arxiv.org/pdf/2507.12507). The results are quite nice, though the 8k tokens restriction makes me a bit skeptical.

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/gflow-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/gflow-3.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

As for the solution diversity (from which we actually started), the authors gave it some scoring with LLM as a judge:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/gflow-4.png){: .responsive-image style="--img-desktop:60%; --img-mobile:80%;"}

# K2-Think: A Parameter-Efficient Reasoning System

[https://arxiv.org/pdf/2509.07604](https://arxiv.org/pdf/2509.07604)

When I think about LLM capabilities, I see "memory" and reasoning as two separate things. For an LLM to be knowledgeable - converse about research-level math, for example - it should be quite large, because knowledge should be "stored" somewhere - in the LLM's parameters. Reasoning, strictly speaking, doesn't have such requirements. So, I'd expect that a small but "clever" LLM might exist - clever not in a sense that it can prove a new theorem, but in a sense that it can be proficient in reasoning about, say, high-school math. Of course, we have **Qwen3** models - all reasoners, and some very small - but they are not too powerful.

Let's see if the researchers from MBZUAI succeeded in it. Judging by the picture below, they did quite a good job :)

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/k2-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

They started with **Qwen2.5-32B** which is not a very large LLM, and they trained it in several phases:

**Phase 1. SFT**. They used the existing **AM-Thinking-v1-Distilled** dataset, composed of CoT reasoning traces
and instruction/response pairs, with prompts drawn from tasks spanning mathematical reasoning, code
generation, scientific reasoning, instruction following, and general chat. The result is called **K2-Think-SFT**.

SFT seems to be crucial; without it, the subsequent RL training is much less effective.

**Phase 2: RL with Verifiable Rewards**. Here, they use the [GURU dataset](https://arxiv.org/pdf/2506.14965), which contains tasks from three domains:

* Math with the usual answer correctness verification
* Code with execution verification
* Science, where the answers are supposed to be scored by another model (which makes the reward slightly less credible...)

This produces the **K2-THINK** model. The context length at this point is 32k. (Not much, but probably ok for high-school tasks.)

**Phase 3 (which is not a training phase): Test-time Improvement**. It suggests the following test-time behavour:

* First, **Plan-Before-You-Think**: the planner agent is asked to extract key concepts from the query, and create a high-level plan from them. By the way, despite resulting in longer prompts this stage actually reduces the final solution length.
* Then, **K2-THINK** does its resoning job, generating $N$ independent outputs for a given prompt (in the experiments $N = 3$)
* Finally, an independent LLM chooses the best solution.

**Deployment**. **K2-THINK** is already quite small and efficient (especially compared the the huge and profusely thinking DeepSeek-R1), but it can be made even better with the right deployment.

The authors deploy K2-Think on Cerebras Wafer-Scale Engine (WSE) systems, leveraging the world’s largest
processor and speculative decoding. The WSE delivers approximately 2,000 tokens per second, representing a 10 times improvement over the nominal 200 tokens per second observed on typical deployment environments on a regular cloud provider. 

On the WSE, 32,000 token generation (which is typical for this model) is completed in just 16 seconds.

Here are qualitative results:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/k2-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

The model seems to be able to surpass DeepSeek-V3.1 on some of the benchmarks despite being like 20x smaller. This sounds nice; just don't forget that **K2-THINK** in the table isn't just an LLM, but a model + some inference-time optimization (planner + best-of-3). So, its good to check what these optimizations bring to the table:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/k2-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

I'm actually surprised that the authors stopped at just trying $N = 3$. They could use quite a large $N$ or an even more fancy inference strategy and still be way faster and way more memory-efficient than **DeepSeek-V3.1**. 

Anyway, this seems to show that even a small model, if trained well and given some additional inference-time optimization, can rival some of the reasoning giants. I wonder what if we could somehow combine a huge, knowledgeable expert with a small but powerful reasoner. 

The key advantage of a hybrid system might be that fine-tuning a, say, 32B model is far more feasible than a 600B one. We could subject the smaller model to more extensive SFT and RL, potentially making it a superior pure reasoner. Meanwhile, the larger companion model could serve as a vast knowledge base, providing context and factual grounding without requiring the same intensive alignment.

# Qwen3-Next

[Blog post](https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&from=research.latest-advancements-list)

In the [previous paperwatch](https://nebius-academy.github.io/knowledge-base/paperwatch-08-09-2025/), I got overly excited about efficient non-transformer blocks in [Jet-Nemotron](https://arxiv.org/pdf/2508.15884) by NVIDIA. This time, I've got a nice surprise from the Qwen team. And while Jet-Nemotron had only 2B and 4B versions, **Qwen3-Next** has 80B parameters - so do we finally have a hybrid (transformer + non-transformer layers) model of a decent size and decent capability? Let's see.

They trained **Qwen3-Next-80B-A3B-Base** — an 80-billion-parameter model that activates only 3 billion parameters during inference.  This base model achieves performance comparable to (or even slightly better than) the dense Qwen3-32B model, while using less than 10% of its training cost (GPU hours). They also released two post-trained versions of it: **Qwen3-Next-80B-A3B-Instruct** and **Qwen3-Next-80B-A3B-Thinking**.

The architecture is very peculiar:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/qwen-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Note that it has:

* Heavy Mixture-of-Experts: 512 total experts; for each token 10 routed experts are used + 1 shared expert. This is how they achive so little inference-time parameter usage
* 3x **Gated DeltaNet** for each gated attention layer. [Gated DeltaNet](https://arxiv.org/pdf/2412.06464) is a modification of Mamba2 (and a quite fancy State Space Model). Its core *Gated Delta Rule* is, in essense, a linear recurrent network (linear = no nonlinearities in the recurrent layer). So, it is linear with respect to the sequence length - unlike a quadratic attention layer - and requires no KV-cache.

  A typical problem with State Space Models (SSMs) is that, while being efficient, they are much less capable then attention layers at long context - which is not surprising, given the fact that they need to pack all the meaning into a single hidden state tensor. But hybrid architectures allow to combine the sheer power of attention with the efficiency of SSMs.

Here are some testaments of the efficiency of Qwen3-Next:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/qwen-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

And here are some benchmark results for the thinking model:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/qwen-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

I'd love to see comparison with the more powerful models, but at least we have a Gemini here... And in the [Artificial Analysis leaderboard](https://artificialanalysis.ai/leaderboards/models) it took quite a good spot:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/qwen-3.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

The table is sorted by the column highlighted in green, which is "intelligence".

Quite exciting!

# REFRAG: Rethinking RAG based Decoding

[https://arxiv.org/pdf/2509.01092](https://arxiv.org/pdf/2509.01092)

This paper by Meta addresses the potential inefficiency of RAG which is context bloating. Indeed, if you want to make the LLM's answer factually correct - and that's the main purpose of RAG - you are tempted to retrieve as much evidence as you can, especially given the fact that database retrieval is often faulty and you might not get what you need in top-10. This potentially leads to flooding the LLM with so much context that it either hurts efficiency or hurts quality, because LLMs, despite nice promises, aren't totally good at digesting huge masses of text.

The authors of this paper suggest that not all the retrieved data is needed verbatim and some can be compressed into vector representations.

Namely, here's what they suggest (and this is quite a complex pipeline):

1. The retrieved context is chunked and every chunk is vectorized with an **embedder** model
2. Then, a **chunk expansion agent** - which has a two-layer transformer under the hood - looks at all chunk embeddings and chooses which of them to expand.
3. The chosen chunks are supplied to the **main LLM** ("Decoder-only Foundational Model" on the picture below) verbatim, as text, while all others go into the main LLM as embedding vectors.
4. The main LLM does its usual job.

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/refrag-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Of course, this won't just start working magically. All the components need to be trained.

They do it in several stages:

### Stage 1: Continual pre-training

On this stage, the encoder and the main LLM are taught to work together.

The authors use the Book and ArXiv domains of an open-source and well-known Slimpajama dataset (which the main LLM definitely saw during pre-training). The models are trained for the following two tasks:
1.  **Reconstruction task:** The encoder takes a chunk of text, compresses it into a single vector, and the decoder's only job is to reconstruct the original task.
2.  **Next Paragraph Prediction:** The main LLM is given a long context of text which has been compressed into a sequence of embedding vectors, and its task is to predict the next paragraph of text.

Training starts with simple objectives (e.g., reconstructing a single chunk) and gradually moves to more complex ones (reconstructing hundreds of chunks). The authors claim that this curriculum is essential for effective training.

### Stage 2: Training the chunk expansion agent

The agent is trained with RL on same Slimpajama (Book/ArXiv) dataset. (The main LLM and the encoders are frozen.)

How it is done:
1. The RL policy looks at all the compressed chunk vectors for a given context.
2. It decides which chunks are most important and should be "expanded" back into their original tokens.
3. This hybrid input (part vectors, part tokens) is passed to the main LLM decoder.
4. The LLM decoder performs its task (next paragraph prediction).
5. The **reward** is based on the **perplexity** of the LLM's output. Sounds a bit too naive for me, but somehow, it works.

### Stage 3: SFT for RAG

Finally, the entire system (encoder, main LLM, and the chunk expansion agent) is fine-tuned specifically for RAG.

The authors use a 1.1 million data points curated from five different Q&A domains:
*   **Dialogue:** OpenAssistant Conversations Dataset
*   **Open-Domain QA:** CommonsenseQA, Web Questions, MS MARCO, etc.
*   **Reading Comprehension:** SQuADv2, PubMedQA, etc.
*   **Chain-of-thought Reasoning:** MathQA, StrategyQA, etc.

### Results

Eventually, thanks to context compression, the whole pipeline becomes more efficient. Here are some plots reflecting inference acceleration:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/refrag-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Capability comparison tables are somewhat messy and not very convincing, so I'll omit them here. Definitely, REFRAG works.

# Towards General Agentic Intelligence via Environment Scaling

[https://arxiv.org/pdf/2509.13311](https://arxiv.org/pdf/2509.13311)

AI Agents are a great thing now, and significant efforts are made to fine tune LLMs to be better at agentic tasks. Usually, such training is done with RL, not SFT. Partially, because LLM Agents are also *agents* in the RL terminology. Partially, because it's just very difficult to gather a training dataset for SFT. Indeed, for that you would need to get somewhere many ground-truth agentic trajectories for diverse tasks. Acknowledging this problem, the authors attempt to build a factory for autonomic generation of agentic trajectories.

They collected more than 30,000 APIs from [ToolBench](https://github.com/OpenBMB/ToolBench), [API-Gen](https://arxiv.org/pdf/2406.18518) and their own internal tool repository. But that's only the beginning. The really important things the authors do next are cleaning, unifying, connecting, and clustering this huge bag of tools.

* **Cleaning**. They filtered the APIs, removing low-quality ones and rewriting (with the help of AI, of course!) some API descriptions to incorporate explicit input–output specifications. (Having good descriptions is important whenever you use tools.)
* **Unifying**. The authors argue that any API can be described as reading from a kind of a database (web search counted as one, I presume) or writing to it. I suppose, there are more complicated APIs, but they can be decomposed into read/write blocks.
* **Connecting**. The authors exploitied the input–output relationships among APIs to connect those that could be used one after another - this resulted in a *graph of tools*.
* **Clustering**. From tool argument lists, the authors created vector representations for each tool and used Louvain community detection to cluster the graph of tools, making up tool *domains*. Now, for each domain, they created a specific database schema to serve as the target for read/write operations.

In the end, they have created a unified and clear tool system that can be used to generate agentic trajectories for SFT. 

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/agency-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Now, SFT data is created in the followind way:

* From a particular domain of the path graph, a "Golden agentic sequence" is generated as a random walk. An important thing: the domain-related database is populated by synthetic data each time anew to ensure diversity. The user's request is generated based on the golden sequence
* The pre-SFT agent performs the task in a live simulation. This produces an "agent trajectory," containing a complete log, including the dynamic tool outputs. This is the potential training data
* The agent trajectory is then compared to the Golden sequence. If the agent's trajectory is a perfect match, it is added to the SFT dataset. Otherwise, it is discarded. Luckily, even the pre-SFT agent is capable enough to produce at least some valid trajectories.

Using this dataset (I haven't found its size in the paper, unfortunately), they train several models:

* **AgentScaler-4B** from **Qwen3-Thinking-4B-2507**
* **AgentScaler-8B** from **Qwen3-8B**
* **AgentScaler-30B-A3B** from **Qwen3-Thinking-30B-A3B-2507**

Here are some results on benchmarks, which are quite nice:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/agency-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

So, good data is the key.

# Towards a Physics Foundational Model

Today's video generation models are often criticized for "not understanding physics" - as much as LLMs are sometimes ridiculed for failures at elementary logic or spacial understanding. While I'd hope to see understanding of physics appear as an emergent capability, it's interesting to see a model trained specifically for dealing with physical simulations.

A physical model, of course, works with physical data. The authors deal with 2D physics and they data are trajectories consisting of 4-tensors (time, hight, width, field), where *field* is actually a broadly-understood physical value such as: pressure, density, temperature, x-axis velocity, and y-axis velocity. That is, an (t, h, w, f)-coordinate of such a tensor tells us what is the value of the f-th physical value at the point in time t and in the spacial position (h, w).

To avoid confusion: "field" here stands for "scalar of vertor field", which is just a mapping: point in space $\mapsto$ scalar or vector value. It is NOT potential of a force field (like I assumed initially...).

The basic problem the authors want to solve is predicting the fields at the next moment in time, given, say, they values (in all points in space) at the 4 previous ticks in time (or a longer trajectory). This might remind you of LLMs or video generation models, I suppose.

The authors solve this problem with a combination of two instruments:

* A **spaciotemporal transformer**, which, given a 4-tensor $X$, predicts its time derivative $\frac{\partial X}{\partial t}$.

  Here, "spaciotemporal" means that attention spans all the coordinates (time, height, width), while the "field" coordinate plays the role of "channels".
  
* A **numerical integrator** which, given $X$ and $\frac{\partial X}{\partial t}$, predicts the slice $X[t_{i+1}, :, : ,:]$ at the next time tick. Note that $X$ itself is a trajectory

  $$X[t_{0}, :, : ,:],\ X[t_{1}, :, : ,:], \ldots, X[t_{i}, :, : ,:]$$

  The numberical integrator has nothing to do with ML. The authors actually used the very simplistic [forward Euler method](https://en.wikipedia.org/wiki/Euler_method) - shifting $X[t_{i}, :, : ,:]$ alongside $\eta\frac{\partial X}{\partial t}(t_i)$ for some scalar $\eta$.

They call the whole system **GPhyT** (**GPT** + **Phy**sics).

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/physics-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Now, having devised this model, the authors burden themselves with the followin research questions:

**Q1**: Can a single, large-scale transformer effectively model a wide range of disparate physical systems (e.g., incompressible flow, shock waves, convection) without any explicit, physics-describing features? 

**Q2**: Can this foundation model perform zero-shot generalization to new, unseen physical conditions (e.g., new boundary conditions, entirely new physics) by inferring the dynamics from the input alone? (This would be super nice to have!)

**Q3**: Can **GPhyT** maintain physical consistency and stability during extended autoregressive rollouts, a characteristic crucial for real-world application?

Let's see what kind of answers they obtained.

## Q1: can GPhyT work?

The authors trained their model on a union of several quite diverse simulation datasets. All of them are, however, about the flow of something - liquids or gases - under different forces and boundary conditions.

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/physics-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

The training data points were 5-tuples: 4 input time ticks + 1 target time tick. They took tuples with different time increments to help the model to grasp different discretizations of time.

The final model is quite good and surpasses previous models. Please note that the vertical scale is logarithmic.

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/physics-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

## Q2: is zero-shot generalization possible?

The authors tested their models on both new (and more challenging) variations of the existing tasks and on the totally new tasks, and GPhyT does a surprisingly good job:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/physics-3.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Here's how the predictions look in practice. (GT is the ground truth.)

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/physics-4.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

## Q3: is the model capable of long-horizon predictions

It's probably cool to predict the next-moment physics, but in reality we're interested in long-term prediction. So, the authors tested their model on a 50-timestep autoregressive prediction:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/physics-5.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Here, the picture is slightly less favourable. For the task types that the model has seen during training, the results are more or less ok, but for out-of-distribution examples (picture on the right), error builds up quite quickly.

Overall, it's a nice even if imperfect attempt. It would actually be interesting to see a cross between a physics foundational model and a video generator. Of course, it's very hard to find data fit for both kinds of models, but what if?..
 
# Discovery of Unstable Singularities

[Paper](https://arxiv.org/pdf/2509.14185)

[Post](https://deepmind.google/discover/blog/discovering-new-solutions-to-century-old-problems-in-fluid-dynamics/)

The Navier–Stokes equations are partial differential equations which describe the motion of viscous fluid substances (which is more or less all real fluids or air). For example, the flow of water in your water pipes is described by them.

The theory around these equations is quite involved. It's enough to note that smooth solutions are known only for come particular cases (that is, particular boundary conditions). In general, the existence of smooth solutions is unknown; moreover, this is one of the [Clay Millennium problems](https://en.wikipedia.org/wiki/Millennium_Prize_Problems#Birch_and_Swinnerton-Dyer_conjecture).

Finding special-case solutions of the Navier-Stokes is also not a simple task. So, the authors actually consider several two-dimensional simplifications of the original three-dimensional equations. Moreover, they don't study smooth solutions. Instead, they take a look at **singularities**.

To understand what singularity is, let's consider a frictionless fluid and an ideal duo of 2d counter-rotating vortices that collide at the time $t = 1$. Their radii shrink, angular speed of liquid inside them grows - and at $t = 1$ we have infinite velocity gradient, infinite pressure - and geometrical collapse, because both vortices cease to exist. Mathematically, the solution becomes singular.

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/navier-stokes-1.gif){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

This is probably not the clearest visualization, but you can see here that speeds (black arrows) grow as the vertices converge. At the point of collision they'll be infinite.

However, singularities may be of a different kind. A **stable** singularity is, well, stable with respect of changing the initial conditions. A possible example is separation of a water droplet from your leaking tap. At the very moment of separation, the curvature of the water's surface becomes infinite at the pinch-off point, so it's a singularity. If you change the initial conditions a little bit (for example, if you give the tap an infinitesimal push), the droplet will still fall; the singularity will be here to stay.

An **unstable** singularity is only generated by a very lucky set of initial conditions. Change them in the slightest - and singularity vanishes. The vortex pair collision is one of such examples. If their centers miss each other, no singularity takes place.

---

The authors have indeed discovered unstable singularities for some of the 2d simplifications of Navier--Stokes, so let's discuss the sources of their success. There were several:

**1. The right math formulation of the task**. The authors leverage the mathematical framework known as **self-similar ansatz**. It suggests modelling only spacial situation with the function $\Phi_{\theta}(y)$, while the evolution in time is described by simple contraction, with singularity occuring at $t = 1$ and $x = 0$:

$$
\phi(x,t) = (1-t)^{k(\lambda)} \Phi_{\theta}\left( \frac{x}{(1-t)^{1+\lambda}} \right)
$$

In the schematic below you can see an example of such $\Phi_{\theta}(y)$. More accurately, the 3d plot shows *vorticity* at different points of a 2d plane. The two peaks correspond to two counter-rotating vortices:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/navier-stokes-0.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

The formula $\phi(x, t)$ describes the contraction of this picture as $t$ tends to $1$.

Moreover, the authors further decompose $\Phi_{\theta}(y)$, implanting some further math properties into it:

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/navier-stokes-3.png){: .responsive-image style="--img-desktop:60%; --img-mobile:80%;"}

**2. A Physics-Informed Neural Network (PINN)**. Basically, it's an MLP, but with additional non-trainable transformations that map MLP's final representations to fields and derivatives that can be plugged into the partial differential equation to obtain the residuals $\mathcal{R}_k$ ($k$ enumerate different equations in the system of PDEs). The loss consists of several components:

* *Equation loss* with is, in turn, a combination of square norms of $\mathcal{R}_k$, their first and second partial derivatives.
* *Data loss* which helps to avoid all-zero solutions. It is of the form
  
  $$\mathcal{L}_{\text{data}} = \widehat{c}_{\text{norm}}\left(
\Phi(y_{\text{norm}}) - C_{\text{norm}} 
\right)^2 + \widehat{c}_{\infty}\sum_{y_{\infty}\in Y_{\infty}}\Phi(y_{\infty})^2$$
  
  for a point $y_{\text{norm}}$ chosen from a high-gradient region and an assortment of points $Y_{\infty}$ which are far from the origin. Thus, we control the value at $y_{\text{norm}}$ and demand that $\Phi$ vanish far from the origin.

The model is trained on samples of coordinates $y$, where more points are taken from more crucial, high-gradient regions.

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/navier-stokes-4.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

After training their MLP for some time, the authors froze it and trained a Fourier feature network on the residuals to rule out high-frequency components. (This is the second training stage.)

**3. The Gauss-Newton optimization method** which is an approximate (no full Hessian needed) second-order method. This is important indeed. Since the authors search for *unstable* singularities they need to be extra careful about numeric precision.

The parameter $\lambda$ turns out to be quite important. Solutions only exist at a discrete set of values of $\lambda$. The one with the largest $\lambda$ - which means, fastest-converging - is stable, while the smaller ones give solutions with higher order of instability, meaning their formation is vulnerable to a greater number of distinct perturbation patterns that can knock the system off its singular trajectory.

In the end, the authors don't invent new ML architectures, but they chose well where to plug in the existing ones, and this is very exciting.

# Reinforced Generation of Combinatorial Structures: Applications to Complexity Theory

[https://arxiv.org/pdf/2509.18057](https://arxiv.org/pdf/2509.18057)

Another math-related paper from Google - an interesting case of applying [AlphaEvolve](https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/) to math problems.

This paper deals with the **Maximum cut** (**MAX-CUT**) problem. Given an undirected graph, you need to particion its vertices into two disjoint sets $S$ and $T$, maximizing the number of edges connecting vertices from $S$ and vertices from $T$. You can imagine it as splitting the graph into two disjoint parts, cutting as many arrows as possible in the process: 

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/navier-stokes-4.png){: .responsive-image style="--img-desktop:60%; --img-mobile:80%;"}

[Source](https://en.wikipedia.org/wiki/Maximum_cut)

The topic will get quite complicated, so **tl;dr**: AlphaEvolve helped them in three different research-related tasks

* First, it created a program for generating a specific graph with a large max cut
* Second, it created intermediate graphs needed for connecting the **MAX-k-CUT** problem to another task with known hardness
* Third, it helped to optimize scoring of these intermediate graphs

AlphaEvolve everywhere :O

If you still want to learn more, let's descend into the trenches of compelxity theory.

## Chapter 1: finding a d-regular Ramanujan graph with a large max cut**

Finding the maximal cut (with the most arrows) is an NP-hard problem. Another potentially difficult problem is finding an algorithm possible of cerifying that `MAX-CUT(G) ≤ σ * |E|` for a given graph $G$ and a given constant $\sigma\in(0;1)$. (The algorithm should be able to work with any graph $G$.) Here, $\vert E\vert$ is the number of edges in the graph. Here, difficulty depends on $\sigma$. For $\sigma = 1$, the task is trivial - any cut will cut not more than all the edges. For smaller values of $\sigma$, there might exist no polynomial algorithm.

It's interesting to find the *lower bound* $\sigma^{MC}$ (MC stands for MAX-CUT), which is the infimum value of $\sigma$ for which a polynoimial cerification algorithm might exist. A higher $\sigma^{MC}$ would imply that the problem is "harder".

The authors study not just any graphs though. They deal with *d-regular graphs* - graphs where every vertext has exactly $d$ neighbours - and try to establish $\sigma_d^{MC}$ for them. More accurately, they consider $d = 3$ and $d = 4$. 

Now, "There is no polynomial algorithm" is a tough claim; so, finding $\sigma_d^{MC}$ is difficult. Instead, researchers search for its lower bounds ($\sigma_d^{MC}\geqslant$ smth), increasing them bit by bit. Moreover, it's a path paved with conjectured **hard instances**. In this case, [Ramanujan graphs](https://en.wikipedia.org/wiki/Ramanujan_graph) are believed to be hard instances. Let me explain what it means.

A polynomial algorithm can't answer a question `MAX-CUT(G) ≤ σ * |E|` by just checking all possible combinations of `σ * |E|` edges. So, it should rely on some kind of internal graph structure. Ramanujan graphs are close to random - and, among d-regular graphs, they are believed to be the less structured. So, mathematician conjectured that:

* If there is a d-regular Ramanujan graph with `MAX-CUT(G) > σ * |E|`, then $\sigma_d^{MC}\geqslant\sigma$. (An important note: there might be Ramanujan graphs with even larger maximal cuts, so we have $\geqslant$ here)

Intuitively, this boils down to the belief that this maximal cut in a Ramanujan graph can't be algorithmically established in a polynomial time. Again, this is a conjecture. But it might help to find $\sigma_d^{MC}$, whose value would lately be justified rigourously.

Now, we are ready to discuss the first contribution of AI to this paper.


The authors used **AlphaEvolve** to evolve a program generating a pair: `(Graph, Cut)`. And it eventually gives them Ramanujan graphs with large cuts: 

* For $d = 3$, they arrive at the same lower bound for $σ_3^{\text{MC}}$ that was known before, which is $0.944$.
* However, for $d = 4$, they are able to raise the lower bound from $0.875$ to $0.911$.

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/max-cut-1.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

## Chapter 2: Improving the hardness of approximation for general MAX-k-CUT

**MAX-k-CUT** is an advanced version of the MAX-CUT problem, where the graph is partitioned into $k$ disjoint parts, with the same objective of maximizing the number of edges connecting different parts. It's also NP-hard. However, sometimes you need to practically solve even NP-hard tasks. Your manager won't give you promotion for proving that a task is NP-hard, will they? 

For that, approximate algorithms exist. For MAX-CUT, there is Goemans-Williamson algorithm, which is guaranteed to find a cut that is at least approximately 0.878 times the size of the optimal MAX-CUT. (Your manager will probably be happy with that.) However, above some $\theta$, the task of finding $\geqslant\theta$ times the size of the optimal MAX-CUT becomes NP-hard too. And finding such $\theta$ is also an interesting direction for math research.

Proving that approximation of MAX-k-CUT is hard involves connecting the problem with `3LIN(k)`, which is a task of solving a system of equations

$$x_i + x_j + x_t = i\ (mod\,k)$$

For 3LIN(k), there is the Håstad's theorem stating that

* For any $k \geqslant 2$ and any $\varepsilon > 0$, it is NP-hard to distinguish between two cases for an instance of 3LIN(k) with $m$ constraints:

  * (Completeness Case): There exists an assignment of variables that satisfies at least $(1 - \varepsilon)$ fraction of the $m$ constraints.
  * (Soundness Case): No assignment of variables satisfies more than $\left(\frac1k + ε\right)$ fraction of the $m$ constraints.

There is a technique that performs reduction from 3LIN(k) to MAX-k-CUT, 

* If the constructed MAX-k-CUT graph has a very high cut, you can confidently conclude the original 3LIN(k) instance must have been a Completeness Case instance.
* If the constructed MAX-k-CUT graph has a very low cut, you can confidently conclude the original 3LIN(k) instance must have been a Soundness Case instance.

So, if we can approximate MAX-k-CUT with a polynomial algorithm, we can roll the result back to approximate 3LIN(k). The only problem is that you need to actually construct the MAX-k-CUT graph. And this is done with **gadgets** - small weighted graphs, one for each $=i$ (right hand side of the equation).

More accurately, an **$i$-gadget** (for $=i$) is a *weighted* graph with vertices particioned into:

1. **primary** $p_x,p_y,p_z$ (correspond to the three summands $x_i$, $x_j$, $x_t$ in the left hand side);
2. $k$ shared **global** vertices;
3. **auxiliary** vertices.

![]({{ site.baseurl }}/assets/images/paperwatch-29-09-2025/max-cut-2.png){: .responsive-image style="--img-desktop:90%; --img-mobile:100%;"}

Now, let's take a system of equations. For each equation

$$x_i + x_j + x_t = i\ (mod\,k)$$

we take the corresponding $=i$ gadget and color its global variables - 1st in the 1st color, ..., k-th in the k-th color. We also color primary variables according to the values of $x = x_i$, $y = x_j$ and $z = x_t$ (since we consider everthing modulo $k$, these will also assume some of the $k$ colors). Now, let's glue all the gadgets along with their global vertices and also along the primary vertices that correspond to the same $x_i$ - these vertices are colored now. The question is - how can we color the auxiliary vertices to establish the maximal cut? ($k$ areas of each colour = $k$ parts into which we break down the graph!)

Now, it turns out that if you can approximately solve the MAX-k-CUT task, you can also approximately discern between the completeness and soundness cases. In particular, the authors prove that

* **Theorem 4.3.** For any $ε > 0$, there is a gadget reduction from $(1 − O(ε), 1/3 + O(ε))$-approximating 3LIN(3) to $(57/62 − ε, 55/62 + ε)$-approximating MAX-3-CUT. As a consequence, it is NP-hard to $(55/57 + ε)$-approximate MAX-3-CUT. 

* **Theorem 4.4**. For any $ε > 0$, there is a gadget reduction from $(1 − O(ε), 1/4 + O(ε))$-approximating 3LIN(4) to $(0.9349 − ε, 0.9238 + ε)$-approximating MAX-4-CUT. As a consequence, it is NP-hard to $(0.987 + ε)$-approximate MAX-4-CUT.

But you probably wonder how they used AI here. In two ways - for finding gadgets (with AlphaEvolve) and for evaluating their quality. For each gadget $I_i$, they define

$$c(I_i)=\min_{\text{triple satisfying }x + y + z = i\ (mod\,k)\text{; fixed coloring of globals}} \max_{\text{colorings of aux vertices}} \text{Cut}$$

$$s(I_i)=\max_{\text{violating triples, mutable coloring of globals}} \left(\max_{\text{colorings of aux vertices}} \text{Cut}\right)$$

$$c'(I_i)=\min_{\text{satisfying triples; mutable coloring of globals}} \left(\max_{\text{colorings of aux vertices}} \text{Cut}\right)$$

(Mutable coloring of globals means, to the best of my understanding, that we can permute their colours.)

I would now love to say that the authors evolve the gadgets to maximise $c(I_i) - s(I_i)$, but unfortunately it's a little bit more complicated. There are two bounds:

* **YES bound (lower bound on optimum):** if the source 3LIN($k$) instance is a PCP-YES (≈ all clauses satisfiable), then the glued MAX-$k$-CUT instance has optimum

  $$\text{OPT}_{\text{YES}}\ \ge\ m\big((1-\varepsilon)\,\bar c+\varepsilon\,\bar s\big)$$

* **NO bound (upper bound on optimum):** if it’s a PCP-NO (≤ $1/k+\varepsilon$ fraction satisfiable), then
  
  $$\mathrm{OPT}_{\text{NO}}\ \le\ m\big((1/k+\varepsilon)\,\bar c'\ +\ (1-1/k-\varepsilon)\,\bar s\big)$$

Here:

* $m$ = number of clauses in the source instance,
* $q_i$ = fraction of clauses using predicate $P_i(x{+}y{+}z\equiv i)$ ($\frac1k$ in the paper),
* $\bar c=\sum_i q_i\,c(I_i)$, $\bar c'=\sum_i q_i\,c'(I_i)$, $\bar s=\sum_i q_i\,s(I_i)$,
* and $c(I_i),c'(I_i),s(I_i)$ are the per-gadget completeness/soundness values (true-clause ⇒ at least $c;$ false-clause ⇒ at most $s;$ $c'$ is the "completeness with globals free"). 

From these two bounds they get a **threshold (hardness) ratio**, which is actually optimized:

$$
R\ :=\ \frac{\text{OPT}_{\text{NO}}}{\text{OPT}_{\text{YES}}}
\ \le\ \frac{(1/k+\varepsilon)\,\bar c'+(1-1/k-\varepsilon)\,\bar s}{(1-\varepsilon)\,\bar c+\varepsilon\,\bar s}.
$$

This $R$ is what is optimized by AlphaEvolve.

Another good thing AI did here is, as I've already mentioned, speeding up computation of $c(I_i)$ and $s(I_i)$. It's still exponential, but has a more efficient implementation. And this was also obtained with AlphaEvolve. I'd only add here that they had to make extra efforts to ensure that the evolved scorer doesn't cheat - this involved checking on a labeled dataset and using an external LLM judge.
